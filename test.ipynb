{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2) \n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2)\n",
    "\n",
    "        self.mu = nn.Linear(256*2*2, z_dim)\n",
    "        self.logvar = nn.Linear(256*2*2, z_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, 3, 64, 64]\n",
    "        c1 = self.conv1(x) # (64x64x3) -> (31x31x32)\n",
    "        h1 = self.relu(c1) # (31x31x32) -> (31x31x32)\n",
    "        c2 = self.conv2(h1) # (31x31x32) -> (14x14x64)\n",
    "        h2 = self.relu(c2) # (14x14x64) -> (14x14x64)\n",
    "        c3 = self.conv3(h2) # (14x14x64) -> (6x6x128)\n",
    "        h3 = self.relu(c3) # (6x6x128) -> (6x6x128)\n",
    "        c4 = self.conv4(h3) # (6x6x128) -> (2x2x256)\n",
    "        h4 = self.relu(c4) # (2x2x256) -> (2x2x256)\n",
    "\n",
    "        d1 = h4.view(-1, 256*2*2) # (2x2x256) -> (1024)\n",
    "\n",
    "        mu = self.mu(d1) # (1024) -> (32)\n",
    "        logvar = self.logvar(d1) # (1024) -> (32)\n",
    "        var = torch.exp(logvar)\n",
    "        std = torch.sqrt(var)\n",
    "\n",
    "        ep = torch.randn_like(std)\n",
    "\n",
    "        z = mu + ep * std # (32) -> (32)\n",
    "\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder\"\"\"\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.l1 = nn.Linear(z_dim, 1024*1*1)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(1024, 128, 5, stride=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, 5, stride=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, 6, stride=2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, 6, stride=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # z: [batch_size, 32]\n",
    "        d1 = self.l1(z) # (32) -> (1024)\n",
    "        d1 = d1.view(-1, 1024, 1, 1) # (1024) -> (1x1x1024)\n",
    "\n",
    "        dc1 = self.deconv1(d1) # (1x1x1024) -> (5x5x128)\n",
    "        h1 = self.relu(dc1) # (5x5x128) -> (5x5x128)\n",
    "        dc2 = self.deconv2(h1) # (5x5x128) -> (13x13x64)\n",
    "        h2 = self.relu(dc2) # (13x13x64) -> (13x13x64)\n",
    "        dc3 = self.deconv3(h2) # (13x13x64) -> (30x30x32)\n",
    "        h3 = self.relu(dc3) # (30x30x32) -> (30x30x32)\n",
    "        dc4 = self.deconv4(h3) # (30x30x32) -> (64x64x3)\n",
    "        x = self.sigmoid(dc4) # (64x64x3) -> (64x64x3)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    '''VAE'''\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        x = self.decoder(z)\n",
    "\n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [1, 32]                   --\n",
       "├─Conv2d: 1-1                            [32, 31, 31]              1,568\n",
       "├─ReLU: 1-2                              [32, 31, 31]              --\n",
       "├─Conv2d: 1-3                            [64, 14, 14]              32,832\n",
       "├─ReLU: 1-4                              [64, 14, 14]              --\n",
       "├─Conv2d: 1-5                            [128, 6, 6]               131,200\n",
       "├─ReLU: 1-6                              [128, 6, 6]               --\n",
       "├─Conv2d: 1-7                            [256, 2, 2]               524,544\n",
       "├─ReLU: 1-8                              [256, 2, 2]               --\n",
       "├─Linear: 1-9                            [1, 32]                   32,800\n",
       "├─Linear: 1-10                           [1, 32]                   32,800\n",
       "==========================================================================================\n",
       "Total params: 755,744\n",
       "Trainable params: 755,744\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 400.37\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.39\n",
       "Params size (MB): 3.02\n",
       "Estimated Total Size (MB): 3.46\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(encoder, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [1, 3, 64, 64]            --\n",
       "├─Linear: 1-1                            [1024]                    33,792\n",
       "├─ConvTranspose2d: 1-2                   [1, 128, 5, 5]            3,276,928\n",
       "├─ReLU: 1-3                              [1, 128, 5, 5]            --\n",
       "├─ConvTranspose2d: 1-4                   [1, 64, 13, 13]           204,864\n",
       "├─ReLU: 1-5                              [1, 64, 13, 13]           --\n",
       "├─ConvTranspose2d: 1-6                   [1, 32, 30, 30]           73,760\n",
       "├─ReLU: 1-7                              [1, 32, 30, 30]           --\n",
       "├─ConvTranspose2d: 1-8                   [1, 3, 64, 64]            3,459\n",
       "├─Sigmoid: 1-9                           [1, 3, 64, 64]            --\n",
       "==========================================================================================\n",
       "Total params: 3,592,803\n",
       "Trainable params: 3,592,803\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 231.70\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.45\n",
       "Params size (MB): 14.37\n",
       "Estimated Total Size (MB): 14.82\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(decoder, (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vae = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VAE                                      [1, 3, 64, 64]            --\n",
       "├─Encoder: 1-1                           [1, 32]                   --\n",
       "│    └─Conv2d: 2-1                       [32, 31, 31]              1,568\n",
       "│    └─ReLU: 2-2                         [32, 31, 31]              --\n",
       "│    └─Conv2d: 2-3                       [64, 14, 14]              32,832\n",
       "│    └─ReLU: 2-4                         [64, 14, 14]              --\n",
       "│    └─Conv2d: 2-5                       [128, 6, 6]               131,200\n",
       "│    └─ReLU: 2-6                         [128, 6, 6]               --\n",
       "│    └─Conv2d: 2-7                       [256, 2, 2]               524,544\n",
       "│    └─ReLU: 2-8                         [256, 2, 2]               --\n",
       "│    └─Linear: 2-9                       [1, 32]                   32,800\n",
       "│    └─Linear: 2-10                      [1, 32]                   32,800\n",
       "├─Decoder: 1-2                           [1, 3, 64, 64]            --\n",
       "│    └─Linear: 2-11                      [1, 1024]                 33,792\n",
       "│    └─ConvTranspose2d: 2-12             [1, 128, 5, 5]            3,276,928\n",
       "│    └─ReLU: 2-13                        [1, 128, 5, 5]            --\n",
       "│    └─ConvTranspose2d: 2-14             [1, 64, 13, 13]           204,864\n",
       "│    └─ReLU: 2-15                        [1, 64, 13, 13]           --\n",
       "│    └─ConvTranspose2d: 2-16             [1, 32, 30, 30]           73,760\n",
       "│    └─ReLU: 2-17                        [1, 32, 30, 30]           --\n",
       "│    └─ConvTranspose2d: 2-18             [1, 3, 64, 64]            3,459\n",
       "│    └─Sigmoid: 2-19                     [1, 3, 64, 64]            --\n",
       "==========================================================================================\n",
       "Total params: 4,348,547\n",
       "Trainable params: 4,348,547\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 597.50\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.84\n",
       "Params size (MB): 17.39\n",
       "Estimated Total Size (MB): 18.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vae, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_data)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 1090621.375\n",
      "epoch: 0, step: 100, loss: 989722.9375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m history[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_torch_world_models\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_torch_world_models\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def criterion(predict, target, ave, log_dev):\n",
    "  bce_loss = F.binary_cross_entropy(predict, target, reduction='sum')\n",
    "  kl_loss = -0.5 * torch.sum(1 + log_dev - ave**2 - log_dev.exp())\n",
    "  loss = bce_loss + kl_loss\n",
    "  return loss\n",
    "\n",
    "net = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "history = {'train_loss': []}\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (img, _) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        output, mu, logvar = net(img)\n",
    "        loss = criterion(output, img, mu, logvar)\n",
    "        history['train_loss'].append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = make_dot(output, params=dict(net.named_parameters()))\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormBasicLSTM(nn.Module):\n",
    "    \"\"\"LSTM\"\"\"\n",
    "    def __init__(self, z_size, a_size, hidden_size=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.z_size = z_size\n",
    "        self.a_size = a_size\n",
    "        self.input_size = z_size + a_size\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, num_layers, batch_first=False) # LSTM：入力サイズ、隠れ層サイズ、層数、バイアス、バッチファースト\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch_size, z_size + a_size]\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        # output: [seq_len, batch_size, hidden_size]\n",
    "        # h_n: [num_layers, batch_size, hidden_size]\n",
    "        # c_n: [num_layers, batch_size, hidden_size]\n",
    "        return output, (h_n, c_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    \"\"\"Mixture Density Network\"\"\"\n",
    "    def __init__(self, hidden_size=256, z_size=32, r_size=0, num_gaussians=5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.z_size = z_size\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.output_size = num_gaussians * 3 *  z_size + r_size # 3: mu, sigma, pi, r_size: reward\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch_size, hidden_size]\n",
    "        output = self.fc(x)\n",
    "        # output: [seq_len, batch_size, num_gaussians * 3 * z_size + r_size]\n",
    "        p, mu, sigma, r = self.split_mdn_params(output)\n",
    "\n",
    "        return p, mu, sigma, r\n",
    "    \n",
    "    def split_mdn_params(self, output):\n",
    "        # output: [seq_len, batch_size, num_gaussians * 3 * z_size + r_size]\n",
    "        p = output[:, :, :self.num_gaussians]\n",
    "        mu = output[:, :, self.num_gaussians:self.num_gaussians * (self.z_size + 1)]\n",
    "        sigma = output[:, :, self.num_gaussians * (self.z_size + 1):self.num_gaussians * (self.z_size * 2 + 1)]\n",
    "        r = output[:, :, -1]\n",
    "\n",
    "        # p: [seq_len, batch_size, num_gaussians]\n",
    "        # mu: [seq_len, batch_size, num_gaussians * z_size]\n",
    "        # sigma: [seq_len, batch_size, num_gaussians * z_size]\n",
    "        # r: [seq_len, batch_size, r_size]\n",
    "\n",
    "        p = F.softmax(p, dim=2)\n",
    "        sigma = torch.exp(sigma)\n",
    "        return p, mu, sigma, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDRNN(nn.Module):\n",
    "    \"\"\"MDN-RNN\"\"\"\n",
    "    def __init__(self, z_size, a_size, r_size, hidden_size=256, num_layers=1, num_gaussians=5):\n",
    "        super().__init__()\n",
    "        self.z_size = z_size\n",
    "        self.a_size = a_size\n",
    "        self.r_size = r_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_gaussians = num_gaussians\n",
    "\n",
    "        self.lstm = LayerNormBasicLSTM(z_size, a_size, hidden_size, num_layers)\n",
    "        self.mdn = MDN(hidden_size, z_size, r_size, num_gaussians)\n",
    "\n",
    "    def forward(self, z, a):\n",
    "        # z: [seq_len, batch_size, z_size]\n",
    "        # a: [seq_len, batch_size, a_size]\n",
    "        za = torch.cat([z, a], dim=2)\n",
    "        # za: [seq_len, batch_size, z_size + a_size]\n",
    "        output, (h_n, c_n) = self.lstm(za)\n",
    "        # output: [seq_len, batch_size, hidden_size]\n",
    "        # h_n: [num_layers, batch_size, hidden_size]\n",
    "        # c_n: [num_layers, batch_size, hidden_size]\n",
    "        p, mu, sigma, r = self.mdn(output)\n",
    "        # p: [seq_len, batch_size, num_gaussians]\n",
    "        # mu: [seq_len, batch_size, num_gaussians * z_size]\n",
    "        # sigma: [seq_len, batch_size, num_gaussians * z_size]\n",
    "        # r: [seq_len, batch_size, r_size]\n",
    "        return output, p, mu, sigma, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LayerNormBasicLSTM(32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LayerNormBasicLSTM                       [1, 1, 256]               --\n",
       "├─LSTM: 1-1                              [1, 1, 256]               300,032\n",
       "==========================================================================================\n",
       "Total params: 300,032\n",
       "Trainable params: 300,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.30\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 1.20\n",
       "Estimated Total Size (MB): 1.20\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(lstm, (1, 1, 35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdn = MDN(256, 32, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MDN                                      [1, 1, 5]                 --\n",
       "├─Linear: 1-1                            [1, 1, 480]               123,360\n",
       "==========================================================================================\n",
       "Total params: 123,360\n",
       "Trainable params: 123,360\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.49\n",
       "Estimated Total Size (MB): 0.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mdn, (1, 1, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdnrnn = MDRNN(32, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MDRNN                                    [1, 1, 256]               --\n",
       "├─LayerNormBasicLSTM: 1-1                [1, 1, 256]               --\n",
       "│    └─LSTM: 2-1                         [1, 1, 256]               300,032\n",
       "├─MDN: 1-2                               [1, 1, 5]                 --\n",
       "│    └─Linear: 2-2                       [1, 1, 480]               123,360\n",
       "==========================================================================================\n",
       "Total params: 423,392\n",
       "Trainable params: 423,392\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 1.69\n",
       "Estimated Total Size (MB): 1.70\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mdnrnn, [(1, 1, 32), (1, 1, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch_world_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
