{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2) \n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2)\n",
    "\n",
    "        self.mu = nn.Linear(256*2*2, z_dim)\n",
    "        self.logvar = nn.Linear(256*2*2, z_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, 3, 64, 64]\n",
    "        c1 = self.conv1(x) # (64x64x3) -> (31x31x32)\n",
    "        h1 = self.relu(c1) # (31x31x32) -> (31x31x32)\n",
    "        c2 = self.conv2(h1) # (31x31x32) -> (14x14x64)\n",
    "        h2 = self.relu(c2) # (14x14x64) -> (14x14x64)\n",
    "        c3 = self.conv3(h2) # (14x14x64) -> (6x6x128)\n",
    "        h3 = self.relu(c3) # (6x6x128) -> (6x6x128)\n",
    "        c4 = self.conv4(h3) # (6x6x128) -> (2x2x256)\n",
    "        h4 = self.relu(c4) # (2x2x256) -> (2x2x256)\n",
    "\n",
    "        d1 = h4.view(-1, 256*2*2) # (2x2x256) -> (1024)\n",
    "\n",
    "        mu = self.mu(d1) # (1024) -> (32)\n",
    "        logvar = self.logvar(d1) # (1024) -> (32)\n",
    "        var = torch.exp(logvar)\n",
    "        std = torch.sqrt(var)\n",
    "\n",
    "        ep = torch.randn_like(std)\n",
    "\n",
    "        z = mu + ep * std # (32) -> (32)\n",
    "\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder\"\"\"\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.l1 = nn.Linear(z_dim, 1024*1*1)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(1024, 128, 5, stride=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, 5, stride=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, 6, stride=2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, 6, stride=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # z: [batch_size, 32]\n",
    "        d1 = self.l1(z) # (32) -> (1024)\n",
    "        d1 = d1.view(-1, 1024, 1, 1) # (1024) -> (1x1x1024)\n",
    "\n",
    "        dc1 = self.deconv1(d1) # (1x1x1024) -> (5x5x128)\n",
    "        h1 = self.relu(dc1) # (5x5x128) -> (5x5x128)\n",
    "        dc2 = self.deconv2(h1) # (5x5x128) -> (13x13x64)\n",
    "        h2 = self.relu(dc2) # (13x13x64) -> (13x13x64)\n",
    "        dc3 = self.deconv3(h2) # (13x13x64) -> (30x30x32)\n",
    "        h3 = self.relu(dc3) # (30x30x32) -> (30x30x32)\n",
    "        dc4 = self.deconv4(h3) # (30x30x32) -> (64x64x3)\n",
    "        x = self.sigmoid(dc4) # (64x64x3) -> (64x64x3)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    '''VAE'''\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        x = self.decoder(z)\n",
    "\n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [1, 32]                   --\n",
       "├─Conv2d: 1-1                            [32, 31, 31]              1,568\n",
       "├─ReLU: 1-2                              [32, 31, 31]              --\n",
       "├─Conv2d: 1-3                            [64, 14, 14]              32,832\n",
       "├─ReLU: 1-4                              [64, 14, 14]              --\n",
       "├─Conv2d: 1-5                            [128, 6, 6]               131,200\n",
       "├─ReLU: 1-6                              [128, 6, 6]               --\n",
       "├─Conv2d: 1-7                            [256, 2, 2]               524,544\n",
       "├─ReLU: 1-8                              [256, 2, 2]               --\n",
       "├─Linear: 1-9                            [1, 32]                   32,800\n",
       "├─Linear: 1-10                           [1, 32]                   32,800\n",
       "==========================================================================================\n",
       "Total params: 755,744\n",
       "Trainable params: 755,744\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 400.37\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.39\n",
       "Params size (MB): 3.02\n",
       "Estimated Total Size (MB): 3.46\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(encoder, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [1, 3, 64, 64]            --\n",
       "├─Linear: 1-1                            [1024]                    33,792\n",
       "├─ConvTranspose2d: 1-2                   [1, 128, 5, 5]            3,276,928\n",
       "├─ReLU: 1-3                              [1, 128, 5, 5]            --\n",
       "├─ConvTranspose2d: 1-4                   [1, 64, 13, 13]           204,864\n",
       "├─ReLU: 1-5                              [1, 64, 13, 13]           --\n",
       "├─ConvTranspose2d: 1-6                   [1, 32, 30, 30]           73,760\n",
       "├─ReLU: 1-7                              [1, 32, 30, 30]           --\n",
       "├─ConvTranspose2d: 1-8                   [1, 3, 64, 64]            3,459\n",
       "├─Sigmoid: 1-9                           [1, 3, 64, 64]            --\n",
       "==========================================================================================\n",
       "Total params: 3,592,803\n",
       "Trainable params: 3,592,803\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 231.70\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.45\n",
       "Params size (MB): 14.37\n",
       "Estimated Total Size (MB): 14.82\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(decoder, (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vae = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VAE                                      [1, 3, 64, 64]            --\n",
       "├─Encoder: 1-1                           [1, 32]                   --\n",
       "│    └─Conv2d: 2-1                       [32, 31, 31]              1,568\n",
       "│    └─ReLU: 2-2                         [32, 31, 31]              --\n",
       "│    └─Conv2d: 2-3                       [64, 14, 14]              32,832\n",
       "│    └─ReLU: 2-4                         [64, 14, 14]              --\n",
       "│    └─Conv2d: 2-5                       [128, 6, 6]               131,200\n",
       "│    └─ReLU: 2-6                         [128, 6, 6]               --\n",
       "│    └─Conv2d: 2-7                       [256, 2, 2]               524,544\n",
       "│    └─ReLU: 2-8                         [256, 2, 2]               --\n",
       "│    └─Linear: 2-9                       [1, 32]                   32,800\n",
       "│    └─Linear: 2-10                      [1, 32]                   32,800\n",
       "├─Decoder: 1-2                           [1, 3, 64, 64]            --\n",
       "│    └─Linear: 2-11                      [1, 1024]                 33,792\n",
       "│    └─ConvTranspose2d: 2-12             [1, 128, 5, 5]            3,276,928\n",
       "│    └─ReLU: 2-13                        [1, 128, 5, 5]            --\n",
       "│    └─ConvTranspose2d: 2-14             [1, 64, 13, 13]           204,864\n",
       "│    └─ReLU: 2-15                        [1, 64, 13, 13]           --\n",
       "│    └─ConvTranspose2d: 2-16             [1, 32, 30, 30]           73,760\n",
       "│    └─ReLU: 2-17                        [1, 32, 30, 30]           --\n",
       "│    └─ConvTranspose2d: 2-18             [1, 3, 64, 64]            3,459\n",
       "│    └─Sigmoid: 2-19                     [1, 3, 64, 64]            --\n",
       "==========================================================================================\n",
       "Total params: 4,348,547\n",
       "Trainable params: 4,348,547\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 597.50\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.84\n",
       "Params size (MB): 17.39\n",
       "Estimated Total Size (MB): 18.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vae, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_data)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 1090587.75\n",
      "epoch: 0, step: 100, loss: 984764.5\n",
      "epoch: 0, step: 200, loss: 972798.8125\n",
      "epoch: 0, step: 300, loss: 968391.5\n",
      "epoch: 1, step: 0, loss: 962117.5625\n",
      "epoch: 1, step: 100, loss: 955147.0\n",
      "epoch: 1, step: 200, loss: 937359.75\n",
      "epoch: 1, step: 300, loss: 923683.75\n",
      "epoch: 2, step: 0, loss: 929104.25\n",
      "epoch: 2, step: 100, loss: 924408.5625\n",
      "epoch: 2, step: 200, loss: 922848.875\n",
      "epoch: 2, step: 300, loss: 936311.0625\n",
      "epoch: 3, step: 0, loss: 935280.3125\n",
      "epoch: 3, step: 100, loss: 909964.25\n",
      "epoch: 3, step: 200, loss: 921540.75\n",
      "epoch: 3, step: 300, loss: 916599.75\n",
      "epoch: 4, step: 0, loss: 916079.4375\n",
      "epoch: 4, step: 100, loss: 925119.1875\n",
      "epoch: 4, step: 200, loss: 924801.125\n",
      "epoch: 4, step: 300, loss: 924342.8125\n",
      "epoch: 5, step: 0, loss: 934412.5\n",
      "epoch: 5, step: 100, loss: 940165.1875\n",
      "epoch: 5, step: 200, loss: 922508.3125\n",
      "epoch: 5, step: 300, loss: 910573.3125\n",
      "epoch: 6, step: 0, loss: 911282.0\n",
      "epoch: 6, step: 100, loss: 925865.1875\n",
      "epoch: 6, step: 200, loss: 940942.1875\n",
      "epoch: 6, step: 300, loss: 923259.75\n",
      "epoch: 7, step: 0, loss: 894229.25\n",
      "epoch: 7, step: 100, loss: 912733.3125\n",
      "epoch: 7, step: 200, loss: 919449.5\n",
      "epoch: 7, step: 300, loss: 936662.25\n",
      "epoch: 8, step: 0, loss: 923708.75\n",
      "epoch: 8, step: 100, loss: 917466.3125\n",
      "epoch: 8, step: 200, loss: 934364.125\n",
      "epoch: 8, step: 300, loss: 920154.1875\n",
      "epoch: 9, step: 0, loss: 917714.75\n",
      "epoch: 9, step: 100, loss: 923945.5625\n",
      "epoch: 9, step: 200, loss: 926422.0625\n",
      "epoch: 9, step: 300, loss: 917001.6875\n"
     ]
    }
   ],
   "source": [
    "def criterion(predict, target, ave, log_dev):\n",
    "  bce_loss = F.binary_cross_entropy(predict, target, reduction='sum')\n",
    "  kl_loss = -0.5 * torch.sum(1 + log_dev - ave**2 - log_dev.exp())\n",
    "  loss = bce_loss + kl_loss\n",
    "  return loss\n",
    "\n",
    "net = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "history = {'train_loss': []}\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (img, _) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        output, mu, logvar = net(img)\n",
    "        loss = criterion(output, img, mu, logvar)\n",
    "        history['train_loss'].append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = make_dot(output, params=dict(net.named_parameters()))\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch_world_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
