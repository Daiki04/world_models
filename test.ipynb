{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import tqdm\n",
    "import cma\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2) \n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2)\n",
    "\n",
    "        self.mu = nn.Linear(256*2*2, z_dim)\n",
    "        self.logvar = nn.Linear(256*2*2, z_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, 3, 64, 64]\n",
    "        c1 = self.conv1(x) # (64x64x3) -> (31x31x32)\n",
    "        h1 = self.relu(c1) # (31x31x32) -> (31x31x32)\n",
    "        c2 = self.conv2(h1) # (31x31x32) -> (14x14x64)\n",
    "        h2 = self.relu(c2) # (14x14x64) -> (14x14x64)\n",
    "        c3 = self.conv3(h2) # (14x14x64) -> (6x6x128)\n",
    "        h3 = self.relu(c3) # (6x6x128) -> (6x6x128)\n",
    "        c4 = self.conv4(h3) # (6x6x128) -> (2x2x256)\n",
    "        h4 = self.relu(c4) # (2x2x256) -> (2x2x256)\n",
    "\n",
    "        d1 = h4.view(-1, 256*2*2) # (2x2x256) -> (1024)\n",
    "\n",
    "        mu = self.mu(d1) # (1024) -> (32)\n",
    "        logvar = self.logvar(d1) # (1024) -> (32)\n",
    "        var = torch.exp(logvar)\n",
    "        std = torch.sqrt(var)\n",
    "\n",
    "        ep = torch.randn_like(std)\n",
    "\n",
    "        z = mu + ep * std # (32) -> (32)\n",
    "\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder\"\"\"\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.l1 = nn.Linear(z_dim, 1024*1*1)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(1024, 128, 5, stride=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, 5, stride=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, 6, stride=2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 3, 6, stride=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # z: [batch_size, 32]\n",
    "        d1 = self.l1(z) # (32) -> (1024)\n",
    "        d1 = d1.view(-1, 1024, 1, 1) # (1024) -> (1x1x1024)\n",
    "\n",
    "        dc1 = self.deconv1(d1) # (1x1x1024) -> (5x5x128)\n",
    "        h1 = self.relu(dc1) # (5x5x128) -> (5x5x128)\n",
    "        dc2 = self.deconv2(h1) # (5x5x128) -> (13x13x64)\n",
    "        h2 = self.relu(dc2) # (13x13x64) -> (13x13x64)\n",
    "        dc3 = self.deconv3(h2) # (13x13x64) -> (30x30x32)\n",
    "        h3 = self.relu(dc3) # (30x30x32) -> (30x30x32)\n",
    "        dc4 = self.deconv4(h3) # (30x30x32) -> (64x64x3)\n",
    "        x = self.sigmoid(dc4) # (64x64x3) -> (64x64x3)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    '''VAE'''\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        x = self.decoder(z)\n",
    "\n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [1, 32]                   --\n",
       "├─Conv2d: 1-1                            [32, 31, 31]              1,568\n",
       "├─ReLU: 1-2                              [32, 31, 31]              --\n",
       "├─Conv2d: 1-3                            [64, 14, 14]              32,832\n",
       "├─ReLU: 1-4                              [64, 14, 14]              --\n",
       "├─Conv2d: 1-5                            [128, 6, 6]               131,200\n",
       "├─ReLU: 1-6                              [128, 6, 6]               --\n",
       "├─Conv2d: 1-7                            [256, 2, 2]               524,544\n",
       "├─ReLU: 1-8                              [256, 2, 2]               --\n",
       "├─Linear: 1-9                            [1, 32]                   32,800\n",
       "├─Linear: 1-10                           [1, 32]                   32,800\n",
       "==========================================================================================\n",
       "Total params: 755,744\n",
       "Trainable params: 755,744\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 400.37\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.39\n",
       "Params size (MB): 3.02\n",
       "Estimated Total Size (MB): 3.46\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(encoder, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [1, 3, 64, 64]            --\n",
       "├─Linear: 1-1                            [1024]                    33,792\n",
       "├─ConvTranspose2d: 1-2                   [1, 128, 5, 5]            3,276,928\n",
       "├─ReLU: 1-3                              [1, 128, 5, 5]            --\n",
       "├─ConvTranspose2d: 1-4                   [1, 64, 13, 13]           204,864\n",
       "├─ReLU: 1-5                              [1, 64, 13, 13]           --\n",
       "├─ConvTranspose2d: 1-6                   [1, 32, 30, 30]           73,760\n",
       "├─ReLU: 1-7                              [1, 32, 30, 30]           --\n",
       "├─ConvTranspose2d: 1-8                   [1, 3, 64, 64]            3,459\n",
       "├─Sigmoid: 1-9                           [1, 3, 64, 64]            --\n",
       "==========================================================================================\n",
       "Total params: 3,592,803\n",
       "Trainable params: 3,592,803\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 231.70\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.45\n",
       "Params size (MB): 14.37\n",
       "Estimated Total Size (MB): 14.82\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(decoder, (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vae = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VAE                                      [1, 3, 64, 64]            --\n",
       "├─Encoder: 1-1                           [1, 32]                   --\n",
       "│    └─Conv2d: 2-1                       [32, 31, 31]              1,568\n",
       "│    └─ReLU: 2-2                         [32, 31, 31]              --\n",
       "│    └─Conv2d: 2-3                       [64, 14, 14]              32,832\n",
       "│    └─ReLU: 2-4                         [64, 14, 14]              --\n",
       "│    └─Conv2d: 2-5                       [128, 6, 6]               131,200\n",
       "│    └─ReLU: 2-6                         [128, 6, 6]               --\n",
       "│    └─Conv2d: 2-7                       [256, 2, 2]               524,544\n",
       "│    └─ReLU: 2-8                         [256, 2, 2]               --\n",
       "│    └─Linear: 2-9                       [1, 32]                   32,800\n",
       "│    └─Linear: 2-10                      [1, 32]                   32,800\n",
       "├─Decoder: 1-2                           [1, 3, 64, 64]            --\n",
       "│    └─Linear: 2-11                      [1, 1024]                 33,792\n",
       "│    └─ConvTranspose2d: 2-12             [1, 128, 5, 5]            3,276,928\n",
       "│    └─ReLU: 2-13                        [1, 128, 5, 5]            --\n",
       "│    └─ConvTranspose2d: 2-14             [1, 64, 13, 13]           204,864\n",
       "│    └─ReLU: 2-15                        [1, 64, 13, 13]           --\n",
       "│    └─ConvTranspose2d: 2-16             [1, 32, 30, 30]           73,760\n",
       "│    └─ReLU: 2-17                        [1, 32, 30, 30]           --\n",
       "│    └─ConvTranspose2d: 2-18             [1, 3, 64, 64]            3,459\n",
       "│    └─Sigmoid: 2-19                     [1, 3, 64, 64]            --\n",
       "==========================================================================================\n",
       "Total params: 4,348,547\n",
       "Trainable params: 4,348,547\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 597.50\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.84\n",
       "Params size (MB): 17.39\n",
       "Estimated Total Size (MB): 18.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vae, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(root='./data3', train=True, download=True, transform=transform_data)\n",
    "test_dataset = CIFAR10(root='./data3', train=False, download=True, transform=transform_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 1090621.375\n",
      "epoch: 0, step: 100, loss: 989722.9375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m history[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_torch_world_models\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_torch_world_models\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def criterion(predict, target, ave, log_dev):\n",
    "  bce_loss = F.binary_cross_entropy(predict, target, reduction='sum')\n",
    "  kl_loss = -0.5 * torch.sum(1 + log_dev - ave**2 - log_dev.exp())\n",
    "  loss = bce_loss + kl_loss\n",
    "  return loss\n",
    "\n",
    "net = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "history = {'train_loss': []}\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (img, _) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        output, mu, logvar = net(img)\n",
    "        loss = criterion(output, img, mu, logvar)\n",
    "        history['train_loss'].append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = make_dot(output, params=dict(net.named_parameters()))\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MDMRNN(nn.Module):\n",
    "#     \"\"\"MDN-RNN\"\"\"\n",
    "#     def __init__(self, num_mixtures, hidden_size,  z_size, a_size, r_size, num_layers, batch_size, sequence_len, output_size):\n",
    "#         super().__init__()\n",
    "#         self.z_size = z_size # 潜在状態の次元\n",
    "#         self.a_size = a_size # 行動の次元\n",
    "#         self.r_size = r_size # 報酬の次元\n",
    "#         self.input_size = z_size + a_size + r_size # 入力の次元\n",
    "#         self.num_mixtures = num_mixtures # 正規分布の数\n",
    "#         self.hidden_size = hidden_size # 隠れ層の次元\n",
    "#         self.num_layers = num_layers # LSTMの層数\n",
    "#         self.batch_size = batch_size # バッチサイズ\n",
    "#         self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers) # LSTM\n",
    "#         self.sequence_len = sequence_len # シーケンスの長さ\n",
    "#         self.output_size = output_size # 出力の次元\n",
    "#         self.dense = nn.Linear(self.hidden_size, self.output_size * num_mixtures * 3) # 全結合層\n",
    "    \n",
    "#     def forward(self, x, hidden_state, cell_state):\n",
    "#         x = x.view(self.sequence_len, self.batch_size, self.input_size)\n",
    "#         z, hidden_states = self.lstm(x, (hidden_state, cell_state))\n",
    "#         hidden = hidden_states[0]\n",
    "#         cell = hidden_states[1]\n",
    "#         z = z.view(-1, self.hidden_size)\n",
    "#         z = self.dense(z)\n",
    "#         z = z.view(-1, self.num_mixtures * 3)\n",
    "#         out_logmix, out_mean, out_log_std = get_mdn_coef(z, self.num_mixtures)\n",
    "#         return out_logmix, out_mean, out_log_std, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"LSTM\"\"\"\n",
    "    def __init__(self, z_size, a_size, hidden_size=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.z_size = z_size\n",
    "        self.a_size = a_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = z_size + a_size\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, num_layers, batch_first=False) # LSTM：入力サイズ、隠れ層サイズ、層数、バイアス、バッチファースト\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        # x: [seq_len, batch_size, z_size + a_size]\n",
    "        output, hidden_cell = self.lstm(x, (hidden_state, cell_state))\n",
    "        hidden = hidden_cell[0]\n",
    "        cell = hidden_cell[1]\n",
    "        # output: [seq_len, batch_size, hidden_size]\n",
    "        # hidden: [num_layers, batch_size, hidden_size]\n",
    "        # cell: [num_layers, batch_size, hidden_size]\n",
    "        return output, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"LSTM\"\"\"\n",
    "    def __init__(self, z_size, a_size, hidden_size=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.z_size = z_size\n",
    "        self.a_size = a_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = z_size + a_size\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, num_layers, batch_first=False) # LSTM：入力サイズ、隠れ層サイズ、層数、バイアス、バッチファースト\n",
    "    \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        output, hidden_cell = self.lstm(x, (hidden_state, cell_state))\n",
    "        hidden = hidden_cell[0]\n",
    "        cell = hidden_cell[1]\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    \"\"\"Mixture Density Network\"\"\"\n",
    "    def __init__(self, hidden_size=256, z_size=32, r_size=0, num_gaussians=5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.z_size = z_size\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.output_size = num_gaussians * 3 *  z_size + r_size # 3: mu, log_sigma, pi, r_size: reward\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch_size, hidden_size]\n",
    "        output = self.fc(x)\n",
    "        # output: [seq_len, batch_size, num_gaussians * 3 * z_size + r_size]\n",
    "        p, mu, logsigma, r = self.split_mdn_params(output)\n",
    "        return p, mu, logsigma, r\n",
    "    \n",
    "    def split_mdn_params(self, output):\n",
    "        # output: [seq_len, batch_size, num_gaussians * 3 * z_size + r_size]\n",
    "        p = output[:, :, :self.num_gaussians]\n",
    "        mu = output[:, :, self.num_gaussians:self.num_gaussians * (self.z_size + 1)]\n",
    "        logsigma = output[:, :, self.num_gaussians * (self.z_size + 1):self.num_gaussians * (self.z_size * 2 + 1)]\n",
    "        r = output[:, :, -1]\n",
    "\n",
    "        # p: [seq_len, batch_size, num_gaussians]\n",
    "        # mu: [seq_len, batch_size, num_gaussians * z_size]\n",
    "        # sigma: [seq_len, batch_size, num_gaussians * z_size]\n",
    "        # r: [seq_len, batch_size, r_size]\n",
    "\n",
    "        p = F.softmax(p, dim=2)\n",
    "        sigma = torch.exp(sigma)\n",
    "        return p, mu, sigma, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDRNN(nn.Module):\n",
    "    \"\"\"MDN-RNN\"\"\"\n",
    "    def __init__(self, z_size, a_size, r_size, hidden_size=256, num_layers=1, num_gaussians=5):\n",
    "        super().__init__()\n",
    "        self.z_size = z_size\n",
    "        self.a_size = a_size\n",
    "        self.r_size = r_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_gaussians = num_gaussians\n",
    "\n",
    "        self.lstm = LayerNormBasicLSTM(z_size, a_size, hidden_size, num_layers)\n",
    "        self.mdn = MDN(hidden_size, z_size, r_size, num_gaussians)\n",
    "\n",
    "    def forward(self, z, a):\n",
    "        # z: [seq_len, batch_size, z_size]\n",
    "        # a: [seq_len, batch_size, a_size]\n",
    "        za = torch.cat([z, a], dim=2)\n",
    "        # za: [seq_len, batch_size, z_size + a_size]\n",
    "        output, (h_n, c_n) = self.lstm(za)\n",
    "        # output: [seq_len, batch_size, hidden_size]\n",
    "        # h_n: [num_layers, batch_size, hidden_size]\n",
    "        # c_n: [num_layers, batch_size, hidden_size]\n",
    "        p, mu, sigma, r = self.mdn(output)\n",
    "        # p: [seq_len, batch_size, num_gaussians]\n",
    "        # mu: [seq_len, batch_size, num_gaussians * z_size]\n",
    "        # sigma: [seq_len, batch_size, num_gaussians * z_size]\n",
    "        # r: [seq_len, batch_size, r_size]\n",
    "        return output, p, mu, sigma, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LayerNormBasicLSTM(32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LayerNormBasicLSTM                       [1, 1, 256]               --\n",
       "├─LSTM: 1-1                              [1, 1, 256]               300,032\n",
       "==========================================================================================\n",
       "Total params: 300,032\n",
       "Trainable params: 300,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.30\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 1.20\n",
       "Estimated Total Size (MB): 1.20\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(lstm, (1, 1, 35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdn = MDN(256, 32, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MDN                                      [1, 1, 5]                 --\n",
       "├─Linear: 1-1                            [1, 1, 480]               123,360\n",
       "==========================================================================================\n",
       "Total params: 123,360\n",
       "Trainable params: 123,360\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.49\n",
       "Estimated Total Size (MB): 0.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mdn, (1, 1, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdnrnn = MDRNN(32, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MDRNN                                    [1, 1, 256]               --\n",
       "├─LayerNormBasicLSTM: 1-1                [1, 1, 256]               --\n",
       "│    └─LSTM: 2-1                         [1, 1, 256]               300,032\n",
       "├─MDN: 1-2                               [1, 1, 5]                 --\n",
       "│    └─Linear: 2-2                       [1, 1, 480]               123,360\n",
       "==========================================================================================\n",
       "Total params: 423,392\n",
       "Trainable params: 423,392\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 1.69\n",
       "Estimated Total Size (MB): 1.70\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mdnrnn, [(1, 1, 32), (1, 1, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"Controller\"\"\"\n",
    "\n",
    "    def __init__(self, a_size, h_size=256, z_size=32):\n",
    "        super().__init__()\n",
    "        self.a_size = a_size\n",
    "        self.h_size = h_size\n",
    "        self.z_size = z_size\n",
    "        self.input_size = z_size + h_size\n",
    "        self.fc = nn.Linear(self.input_size, a_size)\n",
    "\n",
    "    def forward(self, z, h):\n",
    "        # z: [batch_size, z_size]\n",
    "        # h: [batch_size, h_size]\n",
    "        zh = torch.cat([z, h], dim=1)\n",
    "        # zh: [batch_size, z_size + h_size]\n",
    "        a = self.fc(zh)\n",
    "        # a: [batch_size, a_size]\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Controller                               [1, 3]                    --\n",
       "├─Linear: 1-1                            [1, 3]                    867\n",
       "==========================================================================================\n",
       "Total params: 867\n",
       "Trainable params: 867\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Controller(3)\n",
    "\n",
    "summary(c, [(1, 32), (1, 256)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces.box import Box\n",
    "from gym.envs.box2d.car_racing import CarRacing\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2')\n",
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_X = 64 # 画像の横幅\n",
    "SCREEN_Y = 64 # 画像の縦幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.action_space: Box([-1.  0.  0.], 1.0, (3,), float32)\n",
      "actions:\n",
      "[[ 0  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  1]\n",
      " [ 1  0  0]\n",
      " [-1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "#ゲームの選択\n",
    "env = gym.make('CarRacing-v2', render_mode='rgb_array')\n",
    "\n",
    "#デフォルトのactionsの確認（steer, gas, brakeの3次元連続値）\n",
    "print('env.action_space: {}'.format(env.action_space))\n",
    "\n",
    "#アクションを離散値で用意しておく\n",
    "actions = np.array([[ 0, 0, 0],  #actions[0]：何もしない（＝等速直線運動）\n",
    "                    [ 0, 1, 0],  #actions[1]：加速\n",
    "                    [ 0, 0, 1],  #actions[2]：減速\n",
    "                    [ 1, 0, 0],  #actions[3]：右旋回\n",
    "                    [-1, 0, 0]]) #actions[4]：左旋回\n",
    "print('actions:\\n{}'.format(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "\n",
    "env = gym.make('CarRacing-v2', render_mode='rgb_array')\n",
    "\n",
    "def get_rollouts(num_rollouts=10000, reflesh_rate=20, file_dir='./data/'):\n",
    "    for i in tqdm.tqdm(range(num_rollouts)):\n",
    "        state_sequence = []\n",
    "        action_sequence = []\n",
    "        reward_sequence = []\n",
    "        done_sequence = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        iter = 0\n",
    "        while not done:\n",
    "            if iter % reflesh_rate == 0:\n",
    "                if iter < 20:\n",
    "                    steering = -0.1\n",
    "                    acceleration = 1\n",
    "                    brake = 0\n",
    "                else:\n",
    "                    steering = nr.uniform(-1, 1)\n",
    "                    acceleration = nr.uniform(0, 1)\n",
    "                    brake = nr.uniform(0, 1)\n",
    "            action = np.array([steering, acceleration, brake])\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            state = reshape_state(state)\n",
    "            state_sequence.append(state)\n",
    "            action_sequence.append(action)\n",
    "            reward_sequence.append(reward)\n",
    "            done_sequence.append(done)\n",
    "            iter += 1\n",
    "        np.savez_compressed(os.path.join(file_dir, 'rollout_{}.npz'.format(i)), state=state_sequence, action=action_sequence, reward=reward_sequence, done=done_sequence)\n",
    "\n",
    "def load_rollout(idx_rolloout, file_dir='./data/'):\n",
    "    data = np.load(os.path.join(file_dir, 'rollout_{}.npz'.format(idx_rolloout)))\n",
    "    return data['state'], data['action'], data['reward'], data['done']\n",
    "\n",
    "def reshape_state(state):\n",
    "    HEIGHT = 64\n",
    "    WIDTH = 64\n",
    "    state = state[0:84, :, :]\n",
    "    state = np.array(Image.fromarray(state).resize((HEIGHT, WIDTH)))\n",
    "    state = state / 255.0\n",
    "    return state\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reshape_state(state[\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[17], line 42\u001b[0m, in \u001b[0;36mreshape_state\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     40\u001b[0m HEIGHT \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     41\u001b[0m WIDTH \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m---> 42\u001b[0m state \u001b[39m=\u001b[39m state[\u001b[39m0\u001b[39;49m:\u001b[39m84\u001b[39;49m, :, :]\n\u001b[0;32m     43\u001b[0m state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39mfromarray(state)\u001b[39m.\u001b[39mresize((HEIGHT, WIDTH)))\n\u001b[0;32m     44\u001b[0m state \u001b[39m=\u001b[39m state \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "reshape_state(state[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(state[0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(reshape_state(state[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "#初期画像を表示\n",
    "plt.figure()\n",
    "plt.imshow(env.render())\n",
    "plt.show()\n",
    "\n",
    "#ランダムにアクション（1アクションは4フレーム継続）\n",
    "for i in range(10):\n",
    "    for _ in range(4):\n",
    "        #アクションの選択\n",
    "        rand = nr.randint(5)\n",
    "\n",
    "        #状態の更新\n",
    "        state, reward, done, info, _ = env.step(actions[rand])\n",
    "        print('reward:{}'.format(reward))\n",
    "        \n",
    "    #1アクション（＝4フレーム）毎に画像を表示\n",
    "    plt.figure()\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "observation, reward, done, info, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x247dfe101f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "import gym\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import argparse\n",
    "\n",
    "class CarRacing_rollouts():\n",
    "    def __init__(self, max_episode=200):\n",
    "        self.env = gym.make('CarRacing-v2', render_mode='rgb_array', domain_randomize=True)\n",
    "        self.env.reset()\n",
    "        self.file_dir = './data/'\n",
    "        self.max_episode = 200\n",
    "\n",
    "    def get_rollout(self, file_number, reflesh_rate=20):\n",
    "        state_sequence = []\n",
    "        action_sequence = []\n",
    "        reward_sequence = []\n",
    "        done_sequence = []\n",
    "        state = self.env.reset(seed=file_number)\n",
    "        done = False\n",
    "        iter = 0\n",
    "        while (not done) and (iter < self.max_episode):\n",
    "            if iter % reflesh_rate == 0:\n",
    "                if iter < 20:\n",
    "                    steering = -0.1\n",
    "                    acceleration = 1\n",
    "                    brake = 0\n",
    "                else:\n",
    "                    steering = nr.uniform(-1, 1)\n",
    "                    acceleration = nr.uniform(0, 1)\n",
    "                    brake = nr.uniform(0, 1)\n",
    "            action = np.array([steering, acceleration, brake])\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            state = self.reshape_state(state)\n",
    "            state_sequence.append(state)\n",
    "            action_sequence.append(action)\n",
    "            reward_sequence.append(reward)\n",
    "            done_sequence.append(done)\n",
    "            iter += 1\n",
    "        np.savez_compressed(os.path.join(self.file_dir, 'rollout_{}.npz'.format(i)), state=state_sequence, action=action_sequence, reward=reward_sequence, done=done_sequence)\n",
    "        print('rollout_{}.npz is saved.'.format(i))\n",
    "\n",
    "    def get_rollouts(self, num_rollouts=10000, reflesh_rate=20):\n",
    "        start_idx = 0\n",
    "        if os.path.exists(self.file_dir):\n",
    "            start_idx = len(os.listdir(self.file_dir)) \n",
    "        for i in tqdm.tqdm(range(start_idx, num_rollouts+1)):\n",
    "            state_sequence = []\n",
    "            action_sequence = []\n",
    "            reward_sequence = []\n",
    "            done_sequence = []\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            iter = 0\n",
    "            while not done:\n",
    "                if iter % reflesh_rate == 0:\n",
    "                    if iter < 20:\n",
    "                        steering = -0.1\n",
    "                        acceleration = 1\n",
    "                        brake = 0\n",
    "                    else:\n",
    "                        steering = nr.uniform(-1, 1)\n",
    "                        acceleration = nr.uniform(0, 1)\n",
    "                        brake = nr.uniform(0, 1)\n",
    "                action = np.array([steering, acceleration, brake])\n",
    "                state, reward, done, _, _ = self.env.step(action)\n",
    "                state = self.reshape_state(state)\n",
    "                state_sequence.append(state)\n",
    "                action_sequence.append(action)\n",
    "                reward_sequence.append(reward)\n",
    "                done_sequence.append(done)\n",
    "                iter += 1\n",
    "            np.savez_compressed(os.path.join(self.file_dir, 'rollout_{}.npz'.format(i)), state=state_sequence, action=action_sequence, reward=reward_sequence, done=done_sequence)\n",
    "\n",
    "    def load_rollout(self, idx_rolloout):\n",
    "        data = np.load(os.path.join(self.file_dir, 'rollout_{}.npz'.format(idx_rolloout)))\n",
    "        return data['state'], data['action'], data['reward'], data['done']\n",
    "\n",
    "    def reshape_state(self, state):\n",
    "        HEIGHT = 64\n",
    "        WIDTH = 64\n",
    "        state = state[0:84, :, :]\n",
    "        state = np.array(Image.fromarray(state).resize((HEIGHT, WIDTH)))\n",
    "        state = state / 255.0\n",
    "        return state\n",
    "    \n",
    "    def make_gif(self, idx_rolloout):\n",
    "        state, _, _, _ = self.load_rollout(idx_rolloout, self.file_dir)\n",
    "        state = state * 255.0\n",
    "        state = state.astype(np.uint8)\n",
    "        for i in range(len(state)):\n",
    "            img = Image.fromarray(state[i])\n",
    "            img.save(os.path.join(self.file_dir, 'rollout_{}.gif'.format(idx_rolloout, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 並列処理の実行：CarRacingのロールアウトデータの取得\n",
    "Cr = CarRacing_rollouts()\n",
    "with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    executor.submit(, num_rollouts=10000, reflesh_rate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('CarRacing-v2', render_mode='rgb_array')\n",
    "s, _ = env.reset(seed=123)\n",
    "s, _ = env.reset()\n",
    "\n",
    "#初期画像を表示\n",
    "pil_im = Image.fromarray(s[0:84, :, :])\n",
    "pil_im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_state(s):\n",
    "    HEIGHT = 64\n",
    "    WIDTH = 64\n",
    "    s = state[0:84, :, :]\n",
    "    s = np.array(Image.fromarray(s).resize((HEIGHT, WIDTH)))\n",
    "    s = s / 255.0\n",
    "    return s\n",
    "\n",
    "re_state = reshape_state(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz = np.savez_compressed('./re_sate.npz', state=re_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_state = np.load('./re_sate.npz')['state'] * 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_im_re = Image.fromarray(re_state.astype(\"uint8\"))\n",
    "pil_im_re.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sequence = []\n",
    "action_sequence = []\n",
    "reward_sequence = []\n",
    "done_sequence = []\n",
    "state = env.reset()\n",
    "done = False\n",
    "iter = 0\n",
    "max_episode = 300\n",
    "reflesh_rate = 20\n",
    "while (not done) and iter < max_episode:\n",
    "    if iter % reflesh_rate == 0:\n",
    "        if iter < 20:\n",
    "            steering = -0.1\n",
    "            acceleration = 1\n",
    "            brake = 0\n",
    "        else:\n",
    "            steering = nr.uniform(-1, 1)\n",
    "            acceleration = nr.uniform(0, 1)\n",
    "            brake = nr.uniform(0, 1)\n",
    "    action = np.array([steering, acceleration, brake])\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    state = reshape_state(state)\n",
    "    state_sequence.append(state)\n",
    "    action_sequence.append(action)\n",
    "    reward_sequence.append(reward)\n",
    "    done_sequence.append(done)\n",
    "    iter += 1\n",
    "np.savez_compressed('rollout_test.npz', state=state_sequence, action=action_sequence, reward=reward_sequence, done=done_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_states = np.load('rollout_test.npz')['state']\n",
    "re_test_states = test_states[0] * 255.0\n",
    "re_test_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_re_test_states = Image.fromarray(re_test_states.astype(\"uint8\"))\n",
    "im_re_test_states.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.VAE import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rollout\n",
    "\n",
    "rollout = rollout.CarRacing_rollouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 616.3217, r_loss: 600.3217, kl_loss: 16.0000\n",
      "Epoch [200/10000], Loss: 146.1612, r_loss: 130.1612, kl_loss: 16.0000\n",
      "Epoch [300/10000], Loss: 1205.0826, r_loss: 1189.0826, kl_loss: 16.0000\n",
      "Epoch [400/10000], Loss: 671.6410, r_loss: 634.8912, kl_loss: 36.7498\n",
      "Epoch [500/10000], Loss: 545.1108, r_loss: 529.1108, kl_loss: 16.0000\n",
      "Epoch [600/10000], Loss: 575.2524, r_loss: 559.2524, kl_loss: 16.0000\n",
      "Epoch [700/10000], Loss: 653.1595, r_loss: 637.1595, kl_loss: 16.0000\n",
      "Epoch [800/10000], Loss: 570.1837, r_loss: 554.1837, kl_loss: 16.0000\n",
      "Epoch [900/10000], Loss: 377.5027, r_loss: 361.5027, kl_loss: 16.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     43\u001b[0m predict, mu, log_var \u001b[39m=\u001b[39m vae(inputs_train)\n\u001b[1;32m---> 44\u001b[0m r_loss, kl_loss, loss \u001b[39m=\u001b[39m loss_function(labels_train, predict, mu, log_var)\n\u001b[0;32m     45\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     46\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[52], line 20\u001b[0m, in \u001b[0;36mloss_function\u001b[1;34m(label, predict, mu, log_var, kl_tolerance, z_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m r_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(r_loss)\n\u001b[0;32m     19\u001b[0m kl_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m log_var \u001b[39m-\u001b[39m mu\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m log_var\u001b[39m.\u001b[39mexp(), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m kl_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(kl_loss, kl_loss\u001b[39m.\u001b[39;49mnew([kl_tolerance \u001b[39m*\u001b[39;49m z_size]))\n\u001b[0;32m     21\u001b[0m kl_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(kl_loss)\n\u001b[0;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m r_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), kl_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), r_loss \u001b[39m+\u001b[39m kl_loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "lr = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "# 学習\n",
    "num_batches = 1\n",
    "num_epochs = 10000\n",
    "\n",
    "def loss_function(label, predict, mu, log_var, kl_tolerance=0.5, z_size=32):\n",
    "    r_loss = torch.sum((predict - label).pow(2), dim=(1, 2, 3))\n",
    "    r_loss = torch.mean(r_loss)\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)\n",
    "    kl_loss = torch.max(kl_loss, kl_loss.new([kl_tolerance * z_size]))\n",
    "    kl_loss = torch.mean(kl_loss)\n",
    "    return r_loss.detach().cpu().numpy(), kl_loss.detach().cpu().numpy(), r_loss + kl_loss\n",
    "\n",
    "for batch in range(num_batches):\n",
    "    for epoch in range(num_epochs):\n",
    "        states, _, _, _ = rollout.load_rollout(epoch)\n",
    "        inputs = torch.tensor(states).permute(0, 3, 1, 2).float()\n",
    "        labels = torch.tensor(states).permute(0, 3, 1, 2).float()\n",
    "        inputs_train = inputs[0:int(len(inputs)*0.7)]\n",
    "        inputs_test = inputs[int(len(inputs)*0.7):]\n",
    "        labels_train = labels[0:int(len(inputs)*0.7)]\n",
    "        labels_test = labels[int(len(inputs)*0.7):]\n",
    "\n",
    "        #GPU\n",
    "        inputs_train = inputs_train.to(device)\n",
    "        inputs_test = inputs_test.to(device)\n",
    "        labels_train = labels_train.to(device)\n",
    "        labels_test = labels_test.to(device)\n",
    "\n",
    "        # 学習\n",
    "        vae.train()\n",
    "        optimizer.zero_grad()\n",
    "        predict, mu, log_var = vae(inputs_train)\n",
    "        r_loss, kl_loss, loss = loss_function(labels_train, predict, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # テスト\n",
    "        vae.eval()\n",
    "        predict, mu, log_var = vae(inputs_test)\n",
    "        r_loss, kl_loss, loss = loss_function(labels_test, predict, mu, log_var)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, r_loss: {:.4f}, kl_loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item(), r_loss, kl_loss))\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            train_hist = {}\n",
    "            train_hist['loss'] = []\n",
    "            train_hist['loss'].append(loss.item())\n",
    "            train_hist['r_loss'] = []\n",
    "            train_hist['r_loss'].append(r_loss.item())\n",
    "            train_hist['kl_loss'] = []\n",
    "            train_hist['kl_loss'].append(kl_loss.item())\n",
    "\n",
    "            test_hist = {}\n",
    "            test_hist['loss'] = []\n",
    "            test_hist['loss'].append(loss.item())\n",
    "            test_hist['r_loss'] = []\n",
    "            test_hist['r_loss'].append(r_loss.item())\n",
    "            test_hist['kl_loss'] = []\n",
    "            test_hist['kl_loss'].append(kl_loss.item())\n",
    "\n",
    "    # モデルの保存\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': vae.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, './vae.pth')\n",
    "\n",
    "    # historyの保存\n",
    "    hist_dir = './hist'\n",
    "    hist_date = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    if not os.path.exists(hist_dir):\n",
    "        os.makedirs(hist_dir)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(hist_dir, hist_date)):\n",
    "        os.makedirs(os.path.join(hist_dir, hist_date))\n",
    "    \n",
    "    df_hist = pd.DataFrame(train_hist)\n",
    "    df_hist.to_csv(os.path.join(hist_dir, hist_date, 'train_hist.csv'), index=False)\n",
    "    df_hist = pd.DataFrame(test_hist)\n",
    "    df_hist.to_csv(os.path.join(hist_dir, hist_date, 'test_hist.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rollout\n",
    "\n",
    "rollouts = rollout.CarRacing_rollouts()\n",
    "\n",
    "states, _, _, _ = rollouts.load_rollout(10)\n",
    "t100 = states[150]\n",
    "t100_255 = states[100] * 255.0\n",
    "pil_image = Image.fromarray(t100_255.astype(\"uint8\"))\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t100 = t100.reshape(1, 3, 64, 64)\n",
    "t100 = torch.tensor(t100).float()\n",
    "t100 = t100.to(device)\n",
    "t100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t100_vae = vae(t100)\n",
    "t100_vae[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t100_vae_255 = t100_vae[0].detach().cpu().permute(0, 2, 3, 1).numpy() * 255.0\n",
    "t100_vae_255 = t100_vae_255[0]\n",
    "t100_vae_255.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image_vae = Image.fromarray(t100_vae_255.astype(\"uint8\"))\n",
    "pil_image_vae.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.321947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.321884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.321947\n",
       "1  0.321884\n",
       "2  0.000063"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hist = pd.DataFrame([train_hist[\"loss\"][-1], train_hist[\"rmse_loss\"][-1], train_hist[\"kl_loss\"][-1]])\n",
    "df_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_hist \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(train_hist[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m      2\u001b[0m df_hist\u001b[39m.\u001b[39mto_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(hist_dir, hist_date, \u001b[39m'\u001b[39m\u001b[39mtrain_hist.csv\u001b[39m\u001b[39m'\u001b[39m), index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m df_hist \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(test_hist)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "df_hist = pd.DataFrame([train_hist[\"loss\"][-1], train_hist[\"rmse_loss\"][-1], train_hist[\"kl_loss\"][-1]])\n",
    "df_hist.to_csv(os.path.join(hist_dir, hist_date, 'train_hist.csv'), index=False)\n",
    "df_hist = pd.DataFrame(test_hist)\n",
    "df_hist.to_csv(os.path.join(hist_dir, hist_date, 'test_hist.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDN-RNN Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.MDN_RNN import MDNRNN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mdnrnn = MDNRNN(z_size=32, a_size=3, r_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MDNRNN                                   [1, 1, 256]               --\n",
       "├─LayerNormBasicLSTM: 1-1                [1, 1, 256]               --\n",
       "│    └─LSTM: 2-1                         [1, 1, 256]               300,032\n",
       "├─MDN: 1-2                               [1, 1, 5]                 --\n",
       "│    └─Linear: 2-2                       [1, 1, 480]               123,360\n",
       "==========================================================================================\n",
       "Total params: 423,392\n",
       "Trainable params: 423,392\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 1.69\n",
       "Estimated Total Size (MB): 1.70\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mdnrnn, [(1, 1, 32), (1, 1, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (mu): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    (logvar): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (l1): Linear(in_features=32, out_features=1024, bias=True)\n",
       "    (deconv1): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (deconv2): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (deconv3): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))\n",
       "    (deconv4): ConvTranspose2d(32, 3, kernel_size=(6, 6), stride=(2, 2))\n",
       "    (relu): ReLU()\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.VAE import VAE\n",
    "\n",
    "vae = VAE()\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rollout\n",
    "\n",
    "rollout = rollout.CarRacing_rollouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, _, _, _ = rollout.load_rollout(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, mu, logvar = vae.encode(torch.tensor(states).permute(0, 3, 1, 2).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m mdnrnn\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     66\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 67\u001b[0m h, pi, mu, sigma, r \u001b[39m=\u001b[39m mdnrnn(inputs_train_z, inputs_train_a)\n\u001b[0;32m     68\u001b[0m loss \u001b[39m=\u001b[39m loss_function(pi, sigma, mu, labels_train)\n\u001b[0;32m     69\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_torch_wm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\daiki\\Documents\\world_models\\models\\MDN_RNN.py:78\u001b[0m, in \u001b[0;36mMDNRNN.forward\u001b[1;34m(self, z, a)\u001b[0m\n\u001b[0;32m     76\u001b[0m za \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([z, a], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[39m# za: [seq_len, batch_size, z_size + a_size]\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m output, (h_n, c_n) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(za)\n\u001b[0;32m     79\u001b[0m \u001b[39m# output: [seq_len, batch_size, hidden_size]\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39m# h_n: [num_layers, batch_size, hidden_size]\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m# c_n: [num_layers, batch_size, hidden_size]\u001b[39;00m\n\u001b[0;32m     82\u001b[0m p, mu, sigma, r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmdn(output)\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_torch_wm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\daiki\\Documents\\world_models\\models\\MDN_RNN.py:16\u001b[0m, in \u001b[0;36mLayerNormBasicLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m     \u001b[39m# x: [seq_len, batch_size, z_size + a_size]\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     output, (h_n, c_n) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[0;32m     17\u001b[0m     \u001b[39m# output: [seq_len, batch_size, hidden_size]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[39m# h_n: [num_layers, batch_size, hidden_size]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[39m# c_n: [num_layers, batch_size, hidden_size]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m output, (h_n, c_n)\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_torch_wm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\daiki\\miniconda3\\envs\\env_torch_wm\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    778\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(mdnrnn.parameters(), lr=lr)\n",
    "\n",
    "# 学習\n",
    "num_batches = 20\n",
    "num_epochs = 100\n",
    "\n",
    "train_hist = {}\n",
    "train_hist['epoch'] = []\n",
    "train_hist['loss'] = []\n",
    "train_hist['rmse_loss'] = []\n",
    "train_hist['kl_loss'] = []\n",
    "\n",
    "test_hist = {}\n",
    "test_hist['epoch'] = []\n",
    "test_hist['loss'] = []\n",
    "test_hist['rmse_loss'] = []\n",
    "test_hist['kl_loss'] = []\n",
    "\n",
    "def loss_function(pi, sigma, mu, target):\n",
    "    # gaussian mixture loss\n",
    "    gausian_prob = (1 / (sigma * torch.sqrt(2 * np.pi))) * torch.exp(- (target - mu)**2 / (2 * sigma**2))\n",
    "    gausian_prob = torch.prod(gausian_prob, dim=2)\n",
    "    gausian_prob = torch.sum(gausian_prob * pi, dim=1)\n",
    "    gausian_prob = -torch.log(gausian_prob + 1e-5)\n",
    "    gausian_prob = torch.mean(gausian_prob)\n",
    "\n",
    "    return gausian_prob\n",
    "    \n",
    "\n",
    "for batch in range(num_batches):\n",
    "    for epoch in range(num_epochs):\n",
    "        states, actions, _, _ = rollout.load_rollout(epoch)\n",
    "        z = vae.encode(torch.tensor(states).permute(0, 3, 1, 2).float())[0].detach().numpy()\n",
    "\n",
    "        inputs_train_z = z[0:int(len(z)*0.7)-1]\n",
    "        inputs_test_z = z[int(len(z)*0.7)-1:int(len(z))-1]\n",
    "        inputs_train_a = actions[0:int(len(z)*0.7)-1]\n",
    "        inputs_test_a = actions[int(len(z)*0.7)-1:int(len(z))-1]\n",
    "\n",
    "        labels_train_z = z[1:int(len(z)*0.7)]\n",
    "        labels_test_z = z[int(len(z)*0.7):]\n",
    "\n",
    "        inputs_train_z = torch.tensor(inputs_train_z).float()\n",
    "        inputs_test_z = torch.tensor(inputs_test_z).float()\n",
    "        inputs_train_a = torch.tensor(inputs_train_a).float()\n",
    "        inputs_test_a = torch.tensor(inputs_test_a).float()\n",
    "        labels_train_z = torch.tensor(labels_train_z).float()\n",
    "        labels_test_z = torch.tensor(labels_test_z).float()\n",
    "\n",
    "        inputs_train_z = inputs_train_z.view(-1, 1, 32)\n",
    "        inputs_test_z = inputs_test_z.view(-1, 1, 32)\n",
    "        inputs_train_a = inputs_train_a.view(-1, 1, 3)\n",
    "        inputs_test_a = inputs_test_a.view(-1, 1, 3)\n",
    "        labels_train_z = labels_train_z.view(-1, 1, 32)\n",
    "        labels_test_z = labels_test_z.view(-1, 1, 32)\n",
    "\n",
    "        # 学習\n",
    "        mdnrnn.train()\n",
    "        optimizer.zero_grad()\n",
    "        h, pi, mu, sigma, r = mdnrnn(inputs_train_z, inputs_train_a)\n",
    "        loss = loss_function(pi, sigma, mu, labels_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_hist['loss'].append(loss.item())\n",
    "\n",
    "        # テスト\n",
    "        mdnrnn.eval()\n",
    "        h, pi, mu, sigma, r = mdnrnn(inputs_test)\n",
    "        loss = loss_function(pi, sigma, mu, labels_test)\n",
    "        test_hist['loss'].append(loss.item())\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print('epoch [{}/{}], loss: {:.4f}, rmse_loss: {:.4f}, kl_loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item(), rmse_loss.item(), kl_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs_train_z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs_train_z\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m)\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs_train_z' is not defined"
     ]
    }
   ],
   "source": [
    "inputs_train_z.view(-1, 1, 32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([209, 1, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([209, 1, 35])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([inputs_train_z, inputs_train_a], dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.VAE import VAE\n",
    "\n",
    "vae = VAE()\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "checkpoint = torch.load('vae.pth')\n",
    "vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rollout\n",
    "\n",
    "rollouts = rollout.CarRacing_rollouts()\n",
    "\n",
    "states, _, _, _ = rollouts.load_rollout(90)\n",
    "t100 = states[100] * 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "pil_image = Image.fromarray(t100.astype(\"uint8\"))\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t100_vae = vae(torch.tensor(t100).permute(2, 0, 1).float().unsqueeze(0) / 255.0*255.0)[0].detach().permute(0, 2, 3, 1).numpy()[0]*255.0\n",
    "t100_vae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t100_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image_vae = Image.fromarray(t100_vae.astype(\"uint8\"))\n",
    "pil_image_vae.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rollout\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\daiki\\miniconda3\\envs\\env_torch_world_models\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "100%|██████████| 3/3 [00:10<00:00,  3.40s/it]\n"
     ]
    }
   ],
   "source": [
    "cr = rollout.CarRacing_rollouts()\n",
    "seq = cr.get_rollouts(num_rollouts=2, reflesh_rate=20, max_episode=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, dones = cr.load_rollout(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s100 = states[0]\n",
    "\n",
    "pil_im = Image.fromarray((s100).astype(\"uint8\"))\n",
    "pil_im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_seq = states[0]\n",
    "pil_im_seq = Image.fromarray((s_seq).astype(\"uint8\"))\n",
    "pil_im_seq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.VAE import VAE, loss_function\n",
    "import rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_vae(n, m, idx):\n",
    "    cr = rollout.CarRacing_rollouts()\n",
    "    dataset = np.zeros((n*m, 64, 64, 3))\n",
    "    index = 0\n",
    "    for i in idx:\n",
    "        states, _, _, _ = cr.load_rollout(i)\n",
    "        dataset[index:index+m] = states\n",
    "        index += m\n",
    "    dataset = np.moveaxis(dataset, 3, 1)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "n = 10000\n",
    "m = 300\n",
    "file_list = os.listdir('./data/CarRacing/')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vae = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "checkpoint_epoch = -1\n",
    "\n",
    "# if os.path.exists('vae.pth'):\n",
    "#     checkpoint = torch.load('vae.pth')\n",
    "#     vae = VAE().to(device)\n",
    "#     vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     for state in optimizer.state.values():\n",
    "#         for k, v in state.items():\n",
    "#             if isinstance(v, torch.Tensor):\n",
    "#                 state[k] = v.to(device)\n",
    "#     checkpoint_epoch = checkpoint['epoch']\n",
    "# else:\n",
    "#     vae = VAE().to(device)\n",
    "#     optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "# del checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/10][train 10/900, test 0/100], loss: 651.6155, rmse_loss: 635.6155, kl_loss: 16.0000\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 20/900, test 0/100], loss: 737.1656, rmse_loss: 721.1656, kl_loss: 16.0000\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 30/900, test 0/100], loss: 940.7211, rmse_loss: 924.7211, kl_loss: 16.0000\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 40/900, test 0/100], loss: 821.1846, rmse_loss: 805.1846, kl_loss: 16.0000\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 50/900, test 0/100], loss: 620.1093, rmse_loss: 587.1301, kl_loss: 32.9792\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 60/900, test 0/100], loss: 460.0755, rmse_loss: 441.0674, kl_loss: 19.0080\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 70/900, test 0/100], loss: 489.7912, rmse_loss: 468.2515, kl_loss: 21.5398\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 80/900, test 0/100], loss: 657.7531, rmse_loss: 639.9333, kl_loss: 17.8197\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 90/900, test 0/100], loss: 519.6362, rmse_loss: 499.2305, kl_loss: 20.4057\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 100/900, test 0/100], loss: 562.6977, rmse_loss: 545.3950, kl_loss: 17.3027\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 110/900, test 0/100], loss: 608.2382, rmse_loss: 591.6832, kl_loss: 16.5551\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 120/900, test 0/100], loss: 623.6414, rmse_loss: 604.4860, kl_loss: 19.1554\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 130/900, test 0/100], loss: 606.2006, rmse_loss: 588.1070, kl_loss: 18.0936\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 140/900, test 0/100], loss: 543.3575, rmse_loss: 526.5933, kl_loss: 16.7642\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 150/900, test 0/100], loss: 595.6215, rmse_loss: 577.5243, kl_loss: 18.0972\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 160/900, test 0/100], loss: 623.3929, rmse_loss: 605.7291, kl_loss: 17.6639\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 170/900, test 0/100], loss: 594.8058, rmse_loss: 578.1075, kl_loss: 16.6984\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 180/900, test 0/100], loss: 492.1679, rmse_loss: 474.7514, kl_loss: 17.4165\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 190/900, test 0/100], loss: 518.3859, rmse_loss: 502.2464, kl_loss: 16.1396\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 200/900, test 0/100], loss: 523.8251, rmse_loss: 505.7315, kl_loss: 18.0936\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 210/900, test 0/100], loss: 585.1702, rmse_loss: 568.3669, kl_loss: 16.8033\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 220/900, test 0/100], loss: 443.5674, rmse_loss: 426.8875, kl_loss: 16.6799\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 230/900, test 0/100], loss: 587.1479, rmse_loss: 570.9199, kl_loss: 16.2280\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 240/900, test 0/100], loss: 388.4378, rmse_loss: 371.8647, kl_loss: 16.5731\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 250/900, test 0/100], loss: 507.6056, rmse_loss: 491.3033, kl_loss: 16.3023\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 260/900, test 0/100], loss: 514.7761, rmse_loss: 497.7828, kl_loss: 16.9932\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 270/900, test 0/100], loss: 462.6637, rmse_loss: 446.2640, kl_loss: 16.3997\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 280/900, test 0/100], loss: 673.9467, rmse_loss: 657.7086, kl_loss: 16.2381\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 290/900, test 0/100], loss: 551.8722, rmse_loss: 535.3727, kl_loss: 16.4995\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 300/900, test 0/100], loss: 592.1514, rmse_loss: 575.8452, kl_loss: 16.3062\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 310/900, test 0/100], loss: 505.7187, rmse_loss: 488.8225, kl_loss: 16.8962\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 320/900, test 0/100], loss: 563.0413, rmse_loss: 546.6682, kl_loss: 16.3732\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 330/900, test 0/100], loss: 376.8377, rmse_loss: 360.5163, kl_loss: 16.3214\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 340/900, test 0/100], loss: 500.4396, rmse_loss: 483.8713, kl_loss: 16.5683\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 350/900, test 0/100], loss: 568.1965, rmse_loss: 552.0704, kl_loss: 16.1261\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 360/900, test 0/100], loss: 434.3935, rmse_loss: 417.8304, kl_loss: 16.5631\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 370/900, test 0/100], loss: 413.8654, rmse_loss: 397.6364, kl_loss: 16.2290\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 380/900, test 0/100], loss: 637.5946, rmse_loss: 620.5731, kl_loss: 17.0216\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 390/900, test 0/100], loss: 420.2914, rmse_loss: 403.3202, kl_loss: 16.9712\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 400/900, test 0/100], loss: 545.2650, rmse_loss: 528.4960, kl_loss: 16.7690\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 410/900, test 0/100], loss: 678.2085, rmse_loss: 661.6653, kl_loss: 16.5432\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 420/900, test 0/100], loss: 473.2536, rmse_loss: 456.6409, kl_loss: 16.6127\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 430/900, test 0/100], loss: 523.9349, rmse_loss: 507.0018, kl_loss: 16.9331\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 440/900, test 0/100], loss: 491.7934, rmse_loss: 474.9192, kl_loss: 16.8742\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 450/900, test 0/100], loss: 478.3514, rmse_loss: 461.7418, kl_loss: 16.6096\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 460/900, test 0/100], loss: 551.5117, rmse_loss: 535.2856, kl_loss: 16.2261\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 470/900, test 0/100], loss: 477.8234, rmse_loss: 460.3093, kl_loss: 17.5141\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 480/900, test 0/100], loss: 647.3800, rmse_loss: 631.1755, kl_loss: 16.2045\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 490/900, test 0/100], loss: 600.4458, rmse_loss: 584.3900, kl_loss: 16.0558\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 500/900, test 0/100], loss: 629.9352, rmse_loss: 613.1645, kl_loss: 16.7707\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 510/900, test 0/100], loss: 718.3262, rmse_loss: 701.8338, kl_loss: 16.4924\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 520/900, test 0/100], loss: 444.9624, rmse_loss: 423.2733, kl_loss: 21.6891\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 530/900, test 0/100], loss: 483.3540, rmse_loss: 466.6634, kl_loss: 16.6906\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 540/900, test 0/100], loss: 426.4191, rmse_loss: 407.4748, kl_loss: 18.9443\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 550/900, test 0/100], loss: 434.3152, rmse_loss: 416.6404, kl_loss: 17.6748\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 560/900, test 0/100], loss: 429.4482, rmse_loss: 413.1443, kl_loss: 16.3040\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 570/900, test 0/100], loss: 579.8461, rmse_loss: 560.4214, kl_loss: 19.4247\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 580/900, test 0/100], loss: 400.8893, rmse_loss: 383.3312, kl_loss: 17.5580\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 590/900, test 0/100], loss: 441.8861, rmse_loss: 425.5120, kl_loss: 16.3741\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 600/900, test 0/100], loss: 415.5110, rmse_loss: 399.0516, kl_loss: 16.4595\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 610/900, test 0/100], loss: 529.6861, rmse_loss: 512.7660, kl_loss: 16.9201\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 620/900, test 0/100], loss: 490.9854, rmse_loss: 474.8116, kl_loss: 16.1737\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 630/900, test 0/100], loss: 602.4611, rmse_loss: 578.2398, kl_loss: 24.2213\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 640/900, test 0/100], loss: 494.7939, rmse_loss: 467.3557, kl_loss: 27.4382\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 650/900, test 0/100], loss: 430.6346, rmse_loss: 413.1389, kl_loss: 17.4957\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 660/900, test 0/100], loss: 437.8142, rmse_loss: 417.9516, kl_loss: 19.8626\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 670/900, test 0/100], loss: 376.2945, rmse_loss: 355.7672, kl_loss: 20.5272\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 680/900, test 0/100], loss: 397.3218, rmse_loss: 367.3847, kl_loss: 29.9370\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 690/900, test 0/100], loss: 386.8644, rmse_loss: 370.1366, kl_loss: 16.7278\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 700/900, test 0/100], loss: 423.2297, rmse_loss: 405.6811, kl_loss: 17.5486\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 710/900, test 0/100], loss: 413.0899, rmse_loss: 394.5629, kl_loss: 18.5270\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 720/900, test 0/100], loss: 362.8944, rmse_loss: 344.7127, kl_loss: 18.1817\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 730/900, test 0/100], loss: 449.7293, rmse_loss: 431.0359, kl_loss: 18.6934\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 740/900, test 0/100], loss: 410.8580, rmse_loss: 391.4348, kl_loss: 19.4233\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 750/900, test 0/100], loss: 365.2251, rmse_loss: 346.2698, kl_loss: 18.9553\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 760/900, test 0/100], loss: 395.8637, rmse_loss: 376.3134, kl_loss: 19.5503\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 770/900, test 0/100], loss: 437.8921, rmse_loss: 417.7118, kl_loss: 20.1803\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 780/900, test 0/100], loss: 377.9259, rmse_loss: 358.6281, kl_loss: 19.2979\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 790/900, test 0/100], loss: 310.2752, rmse_loss: 288.4534, kl_loss: 21.8218\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 800/900, test 0/100], loss: 322.3777, rmse_loss: 304.8755, kl_loss: 17.5021\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 810/900, test 0/100], loss: 280.2307, rmse_loss: 261.9801, kl_loss: 18.2507\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 820/900, test 0/100], loss: 377.1840, rmse_loss: 356.5912, kl_loss: 20.5927\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 830/900, test 0/100], loss: 250.1010, rmse_loss: 231.9065, kl_loss: 18.1945\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 840/900, test 0/100], loss: 380.3855, rmse_loss: 359.2647, kl_loss: 21.1208\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 850/900, test 0/100], loss: 347.7488, rmse_loss: 328.5767, kl_loss: 19.1721\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 860/900, test 0/100], loss: 313.8580, rmse_loss: 293.4737, kl_loss: 20.3843\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 870/900, test 0/100], loss: 216.0572, rmse_loss: 197.9321, kl_loss: 18.1251\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 880/900, test 0/100], loss: 346.2259, rmse_loss: 326.0072, kl_loss: 20.2187\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 890/900, test 0/100], loss: 207.0691, rmse_loss: 185.3065, kl_loss: 21.7627\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 900/900, test 0/100], loss: 303.5602, rmse_loss: 285.4037, kl_loss: 18.1565\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 1\n",
      "test: epoch 1\n",
      "[epoch 1/10][train 9000/900, test 10/100], loss: 308.8809, rmse_loss: 290.8075, kl_loss: 18.0733\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 9000/900, test 20/100], loss: 356.4467, rmse_loss: 337.1362, kl_loss: 19.3105\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 9000/900, test 30/100], loss: 310.7863, rmse_loss: 289.9043, kl_loss: 20.8820\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 9000/900, test 40/100], loss: 265.3439, rmse_loss: 247.4012, kl_loss: 17.9427\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 9000/900, test 50/100], loss: 236.9406, rmse_loss: 218.2536, kl_loss: 18.6870\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 9000/900, test 60/100], loss: 347.9532, rmse_loss: 329.1844, kl_loss: 18.7688\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 9000/900, test 70/100], loss: 301.8524, rmse_loss: 284.2073, kl_loss: 17.6451\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 9000/900, test 80/100], loss: 286.3932, rmse_loss: 266.2845, kl_loss: 20.1087\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 9000/900, test 90/100], loss: 299.4445, rmse_loss: 279.7340, kl_loss: 19.7105\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 1/10][train 9000/900, test 100/100], loss: 402.4150, rmse_loss: 381.8659, kl_loss: 20.5491\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 10/900, test 0/100], loss: 257.2354, rmse_loss: 239.2120, kl_loss: 18.0234\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 20/900, test 0/100], loss: 257.0244, rmse_loss: 236.3851, kl_loss: 20.6393\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 30/900, test 0/100], loss: 219.2823, rmse_loss: 201.6710, kl_loss: 17.6113\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 40/900, test 0/100], loss: 304.3499, rmse_loss: 284.1386, kl_loss: 20.2114\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 50/900, test 0/100], loss: 339.0680, rmse_loss: 319.1121, kl_loss: 19.9559\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 60/900, test 0/100], loss: 389.0177, rmse_loss: 366.2785, kl_loss: 22.7392\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 70/900, test 0/100], loss: 285.8757, rmse_loss: 266.2178, kl_loss: 19.6579\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 80/900, test 0/100], loss: 211.6445, rmse_loss: 190.0514, kl_loss: 21.5931\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 90/900, test 0/100], loss: 279.9898, rmse_loss: 262.3326, kl_loss: 17.6573\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 100/900, test 0/100], loss: 233.5697, rmse_loss: 214.4111, kl_loss: 19.1586\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 110/900, test 0/100], loss: 257.7923, rmse_loss: 234.2341, kl_loss: 23.5582\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 120/900, test 0/100], loss: 223.7343, rmse_loss: 204.0897, kl_loss: 19.6447\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 130/900, test 0/100], loss: 249.0094, rmse_loss: 230.8805, kl_loss: 18.1289\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 140/900, test 0/100], loss: 284.9914, rmse_loss: 262.5873, kl_loss: 22.4041\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 150/900, test 0/100], loss: 404.3804, rmse_loss: 386.1562, kl_loss: 18.2242\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 160/900, test 0/100], loss: 230.1829, rmse_loss: 213.3897, kl_loss: 16.7932\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 170/900, test 0/100], loss: 249.0829, rmse_loss: 226.7652, kl_loss: 22.3177\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 180/900, test 0/100], loss: 312.4329, rmse_loss: 290.3295, kl_loss: 22.1034\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 190/900, test 0/100], loss: 265.1060, rmse_loss: 241.5911, kl_loss: 23.5149\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 200/900, test 0/100], loss: 273.3858, rmse_loss: 254.4684, kl_loss: 18.9174\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 210/900, test 0/100], loss: 384.5031, rmse_loss: 363.4700, kl_loss: 21.0331\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 220/900, test 0/100], loss: 250.7623, rmse_loss: 223.4801, kl_loss: 27.2822\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 230/900, test 0/100], loss: 319.4369, rmse_loss: 299.8358, kl_loss: 19.6011\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 240/900, test 0/100], loss: 247.3855, rmse_loss: 223.5094, kl_loss: 23.8761\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 250/900, test 0/100], loss: 206.3815, rmse_loss: 186.5352, kl_loss: 19.8463\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 260/900, test 0/100], loss: 252.4209, rmse_loss: 229.6961, kl_loss: 22.7248\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 270/900, test 0/100], loss: 259.3108, rmse_loss: 241.1305, kl_loss: 18.1804\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 280/900, test 0/100], loss: 228.3045, rmse_loss: 206.4921, kl_loss: 21.8124\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 290/900, test 0/100], loss: 242.9810, rmse_loss: 220.4308, kl_loss: 22.5501\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 300/900, test 0/100], loss: 241.8812, rmse_loss: 218.8013, kl_loss: 23.0799\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 310/900, test 0/100], loss: 209.4910, rmse_loss: 189.1255, kl_loss: 20.3654\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 320/900, test 0/100], loss: 256.6529, rmse_loss: 234.2227, kl_loss: 22.4302\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 330/900, test 0/100], loss: 233.2650, rmse_loss: 211.4639, kl_loss: 21.8011\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 340/900, test 0/100], loss: 176.0468, rmse_loss: 153.0176, kl_loss: 23.0291\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 350/900, test 0/100], loss: 265.3280, rmse_loss: 245.9136, kl_loss: 19.4144\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 360/900, test 0/100], loss: 270.5440, rmse_loss: 248.3710, kl_loss: 22.1730\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 370/900, test 0/100], loss: 231.1942, rmse_loss: 211.0481, kl_loss: 20.1461\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 380/900, test 0/100], loss: 243.5948, rmse_loss: 221.7116, kl_loss: 21.8832\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 390/900, test 0/100], loss: 174.4629, rmse_loss: 155.5594, kl_loss: 18.9036\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 400/900, test 0/100], loss: 199.9918, rmse_loss: 178.1384, kl_loss: 21.8534\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 410/900, test 0/100], loss: 193.0210, rmse_loss: 172.5592, kl_loss: 20.4619\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 420/900, test 0/100], loss: 212.1354, rmse_loss: 187.6530, kl_loss: 24.4823\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 430/900, test 0/100], loss: 185.3845, rmse_loss: 162.3022, kl_loss: 23.0823\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 440/900, test 0/100], loss: 147.8184, rmse_loss: 128.3403, kl_loss: 19.4781\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 450/900, test 0/100], loss: 152.5281, rmse_loss: 131.3960, kl_loss: 21.1321\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 460/900, test 0/100], loss: 205.8505, rmse_loss: 185.2554, kl_loss: 20.5951\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 470/900, test 0/100], loss: 160.3674, rmse_loss: 138.3360, kl_loss: 22.0314\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 480/900, test 0/100], loss: 181.3521, rmse_loss: 159.5353, kl_loss: 21.8168\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 490/900, test 0/100], loss: 157.1011, rmse_loss: 133.2126, kl_loss: 23.8885\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 500/900, test 0/100], loss: 195.0090, rmse_loss: 173.1092, kl_loss: 21.8999\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 510/900, test 0/100], loss: 149.2986, rmse_loss: 127.7261, kl_loss: 21.5724\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 520/900, test 0/100], loss: 176.3233, rmse_loss: 153.5483, kl_loss: 22.7750\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 530/900, test 0/100], loss: 218.3262, rmse_loss: 197.4538, kl_loss: 20.8724\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 540/900, test 0/100], loss: 165.4622, rmse_loss: 141.3876, kl_loss: 24.0746\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 550/900, test 0/100], loss: 121.8600, rmse_loss: 100.1625, kl_loss: 21.6974\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 560/900, test 0/100], loss: 159.4188, rmse_loss: 137.8364, kl_loss: 21.5823\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 570/900, test 0/100], loss: 195.0743, rmse_loss: 172.7527, kl_loss: 22.3216\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 580/900, test 0/100], loss: 116.0745, rmse_loss: 95.5492, kl_loss: 20.5252\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 590/900, test 0/100], loss: 151.8202, rmse_loss: 127.2115, kl_loss: 24.6087\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 600/900, test 0/100], loss: 170.8304, rmse_loss: 147.3729, kl_loss: 23.4575\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 610/900, test 0/100], loss: 122.7972, rmse_loss: 98.3192, kl_loss: 24.4780\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 620/900, test 0/100], loss: 115.6323, rmse_loss: 95.3588, kl_loss: 20.2735\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 630/900, test 0/100], loss: 145.2769, rmse_loss: 121.6265, kl_loss: 23.6504\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 640/900, test 0/100], loss: 143.4888, rmse_loss: 120.8769, kl_loss: 22.6119\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 650/900, test 0/100], loss: 132.7201, rmse_loss: 110.5547, kl_loss: 22.1654\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 660/900, test 0/100], loss: 114.0819, rmse_loss: 91.4034, kl_loss: 22.6785\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 670/900, test 0/100], loss: 166.1948, rmse_loss: 139.6274, kl_loss: 26.5674\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 680/900, test 0/100], loss: 104.5259, rmse_loss: 82.4417, kl_loss: 22.0842\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 690/900, test 0/100], loss: 149.4042, rmse_loss: 126.0433, kl_loss: 23.3610\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 700/900, test 0/100], loss: 95.3596, rmse_loss: 73.1976, kl_loss: 22.1620\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 710/900, test 0/100], loss: 156.0894, rmse_loss: 131.7974, kl_loss: 24.2920\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 720/900, test 0/100], loss: 158.3718, rmse_loss: 135.5771, kl_loss: 22.7948\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 730/900, test 0/100], loss: 125.9911, rmse_loss: 101.1118, kl_loss: 24.8793\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 740/900, test 0/100], loss: 186.3799, rmse_loss: 162.7637, kl_loss: 23.6162\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 750/900, test 0/100], loss: 149.8562, rmse_loss: 127.6651, kl_loss: 22.1912\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 760/900, test 0/100], loss: 134.5540, rmse_loss: 111.1745, kl_loss: 23.3795\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 770/900, test 0/100], loss: 148.4355, rmse_loss: 122.2264, kl_loss: 26.2092\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 780/900, test 0/100], loss: 139.7368, rmse_loss: 117.4375, kl_loss: 22.2993\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 790/900, test 0/100], loss: 114.7563, rmse_loss: 93.4662, kl_loss: 21.2901\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 800/900, test 0/100], loss: 109.1278, rmse_loss: 89.0733, kl_loss: 20.0545\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 810/900, test 0/100], loss: 150.4875, rmse_loss: 127.3470, kl_loss: 23.1405\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 820/900, test 0/100], loss: 119.6955, rmse_loss: 96.3191, kl_loss: 23.3764\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 830/900, test 0/100], loss: 104.6037, rmse_loss: 82.4868, kl_loss: 22.1169\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 840/900, test 0/100], loss: 106.4972, rmse_loss: 82.7043, kl_loss: 23.7929\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 850/900, test 0/100], loss: 119.1070, rmse_loss: 97.8109, kl_loss: 21.2961\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 860/900, test 0/100], loss: 115.9661, rmse_loss: 92.1076, kl_loss: 23.8585\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 870/900, test 0/100], loss: 104.0809, rmse_loss: 81.5676, kl_loss: 22.5133\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 880/900, test 0/100], loss: 128.8207, rmse_loss: 104.9122, kl_loss: 23.9086\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 890/900, test 0/100], loss: 108.2865, rmse_loss: 86.5867, kl_loss: 21.6997\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 900/900, test 0/100], loss: 96.7593, rmse_loss: 73.7177, kl_loss: 23.0416\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 2\n",
      "test: epoch 2\n",
      "[epoch 2/10][train 9000/900, test 10/100], loss: 121.9976, rmse_loss: 99.9410, kl_loss: 22.0566\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 9000/900, test 20/100], loss: 134.6967, rmse_loss: 111.5338, kl_loss: 23.1629\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 9000/900, test 30/100], loss: 102.3622, rmse_loss: 78.5060, kl_loss: 23.8562\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 9000/900, test 40/100], loss: 72.1369, rmse_loss: 51.3831, kl_loss: 20.7538\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 9000/900, test 50/100], loss: 99.5196, rmse_loss: 79.3554, kl_loss: 20.1642\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 9000/900, test 60/100], loss: 121.9479, rmse_loss: 100.7238, kl_loss: 21.2240\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 9000/900, test 70/100], loss: 122.8294, rmse_loss: 100.8622, kl_loss: 21.9672\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 9000/900, test 80/100], loss: 134.6211, rmse_loss: 110.9230, kl_loss: 23.6981\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 9000/900, test 90/100], loss: 121.0921, rmse_loss: 97.6360, kl_loss: 23.4561\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 2/10][train 9000/900, test 100/100], loss: 117.1475, rmse_loss: 93.8428, kl_loss: 23.3046\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 10/900, test 0/100], loss: 100.3427, rmse_loss: 78.0094, kl_loss: 22.3332\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 20/900, test 0/100], loss: 114.4074, rmse_loss: 93.0178, kl_loss: 21.3896\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 30/900, test 0/100], loss: 128.8977, rmse_loss: 104.6626, kl_loss: 24.2351\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 40/900, test 0/100], loss: 130.3940, rmse_loss: 109.1444, kl_loss: 21.2496\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 50/900, test 0/100], loss: 102.2997, rmse_loss: 79.3896, kl_loss: 22.9101\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 60/900, test 0/100], loss: 110.7088, rmse_loss: 86.6840, kl_loss: 24.0248\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 70/900, test 0/100], loss: 131.7099, rmse_loss: 107.1258, kl_loss: 24.5841\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 80/900, test 0/100], loss: 135.4897, rmse_loss: 112.3702, kl_loss: 23.1195\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 90/900, test 0/100], loss: 91.5820, rmse_loss: 68.6818, kl_loss: 22.9002\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 100/900, test 0/100], loss: 125.9986, rmse_loss: 103.1270, kl_loss: 22.8716\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 110/900, test 0/100], loss: 95.0207, rmse_loss: 71.1767, kl_loss: 23.8440\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 120/900, test 0/100], loss: 108.7743, rmse_loss: 85.4883, kl_loss: 23.2860\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 130/900, test 0/100], loss: 94.4264, rmse_loss: 71.7167, kl_loss: 22.7097\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 140/900, test 0/100], loss: 96.1254, rmse_loss: 74.2580, kl_loss: 21.8673\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 150/900, test 0/100], loss: 150.3553, rmse_loss: 127.6750, kl_loss: 22.6803\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 160/900, test 0/100], loss: 109.2636, rmse_loss: 84.7249, kl_loss: 24.5387\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 170/900, test 0/100], loss: 144.8843, rmse_loss: 121.8992, kl_loss: 22.9851\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 180/900, test 0/100], loss: 108.3210, rmse_loss: 84.4125, kl_loss: 23.9086\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 190/900, test 0/100], loss: 114.5969, rmse_loss: 92.7561, kl_loss: 21.8408\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 200/900, test 0/100], loss: 128.0344, rmse_loss: 104.7396, kl_loss: 23.2948\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 210/900, test 0/100], loss: 107.0275, rmse_loss: 85.3262, kl_loss: 21.7013\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 220/900, test 0/100], loss: 117.5402, rmse_loss: 94.6001, kl_loss: 22.9401\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 230/900, test 0/100], loss: 96.5255, rmse_loss: 73.4974, kl_loss: 23.0281\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 240/900, test 0/100], loss: 97.1816, rmse_loss: 74.5747, kl_loss: 22.6069\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 250/900, test 0/100], loss: 149.8109, rmse_loss: 125.9086, kl_loss: 23.9023\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 260/900, test 0/100], loss: 105.5670, rmse_loss: 84.3100, kl_loss: 21.2570\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 270/900, test 0/100], loss: 134.9340, rmse_loss: 110.5790, kl_loss: 24.3550\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 280/900, test 0/100], loss: 159.4318, rmse_loss: 134.3239, kl_loss: 25.1079\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 290/900, test 0/100], loss: 122.3471, rmse_loss: 97.3971, kl_loss: 24.9500\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 300/900, test 0/100], loss: 107.5606, rmse_loss: 85.4959, kl_loss: 22.0647\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 310/900, test 0/100], loss: 95.3760, rmse_loss: 72.0760, kl_loss: 23.2999\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 320/900, test 0/100], loss: 102.6588, rmse_loss: 78.7961, kl_loss: 23.8627\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 330/900, test 0/100], loss: 128.6057, rmse_loss: 104.5547, kl_loss: 24.0511\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 340/900, test 0/100], loss: 129.0941, rmse_loss: 104.8718, kl_loss: 24.2224\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 350/900, test 0/100], loss: 83.4944, rmse_loss: 62.1430, kl_loss: 21.3514\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 360/900, test 0/100], loss: 150.6069, rmse_loss: 128.3669, kl_loss: 22.2400\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 370/900, test 0/100], loss: 90.1721, rmse_loss: 67.7185, kl_loss: 22.4537\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 380/900, test 0/100], loss: 103.8735, rmse_loss: 80.3374, kl_loss: 23.5361\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 390/900, test 0/100], loss: 112.9197, rmse_loss: 89.3358, kl_loss: 23.5839\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 400/900, test 0/100], loss: 113.7844, rmse_loss: 88.9447, kl_loss: 24.8396\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 410/900, test 0/100], loss: 117.3681, rmse_loss: 96.4569, kl_loss: 20.9112\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 420/900, test 0/100], loss: 119.6147, rmse_loss: 95.2750, kl_loss: 24.3396\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 430/900, test 0/100], loss: 125.5564, rmse_loss: 101.2773, kl_loss: 24.2791\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 440/900, test 0/100], loss: 120.5767, rmse_loss: 95.1000, kl_loss: 25.4768\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 450/900, test 0/100], loss: 127.4796, rmse_loss: 102.3824, kl_loss: 25.0972\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 460/900, test 0/100], loss: 98.1208, rmse_loss: 75.6262, kl_loss: 22.4946\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 470/900, test 0/100], loss: 109.5220, rmse_loss: 87.3711, kl_loss: 22.1509\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 480/900, test 0/100], loss: 98.1382, rmse_loss: 75.2362, kl_loss: 22.9020\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 490/900, test 0/100], loss: 99.9974, rmse_loss: 77.5668, kl_loss: 22.4306\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 500/900, test 0/100], loss: 99.5773, rmse_loss: 77.7900, kl_loss: 21.7873\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 510/900, test 0/100], loss: 102.4693, rmse_loss: 79.6419, kl_loss: 22.8274\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 520/900, test 0/100], loss: 114.1872, rmse_loss: 90.1764, kl_loss: 24.0109\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 530/900, test 0/100], loss: 109.2953, rmse_loss: 85.8596, kl_loss: 23.4357\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 540/900, test 0/100], loss: 108.4217, rmse_loss: 82.1779, kl_loss: 26.2438\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 550/900, test 0/100], loss: 122.4080, rmse_loss: 98.5913, kl_loss: 23.8168\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 560/900, test 0/100], loss: 97.2663, rmse_loss: 74.4619, kl_loss: 22.8044\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 570/900, test 0/100], loss: 132.4032, rmse_loss: 106.0321, kl_loss: 26.3711\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 580/900, test 0/100], loss: 92.5634, rmse_loss: 69.6955, kl_loss: 22.8680\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 590/900, test 0/100], loss: 99.7947, rmse_loss: 75.6269, kl_loss: 24.1678\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 600/900, test 0/100], loss: 113.4294, rmse_loss: 89.4853, kl_loss: 23.9440\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 610/900, test 0/100], loss: 120.2011, rmse_loss: 97.5330, kl_loss: 22.6682\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 620/900, test 0/100], loss: 74.4444, rmse_loss: 51.8996, kl_loss: 22.5448\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 630/900, test 0/100], loss: 124.6761, rmse_loss: 102.0296, kl_loss: 22.6465\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 640/900, test 0/100], loss: 84.3588, rmse_loss: 62.6037, kl_loss: 21.7551\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 650/900, test 0/100], loss: 97.6769, rmse_loss: 73.3276, kl_loss: 24.3494\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 660/900, test 0/100], loss: 86.4716, rmse_loss: 64.8976, kl_loss: 21.5740\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 670/900, test 0/100], loss: 86.9956, rmse_loss: 63.3076, kl_loss: 23.6881\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 680/900, test 0/100], loss: 119.4527, rmse_loss: 94.7410, kl_loss: 24.7117\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 690/900, test 0/100], loss: 122.6708, rmse_loss: 98.9437, kl_loss: 23.7271\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 700/900, test 0/100], loss: 87.7434, rmse_loss: 64.2730, kl_loss: 23.4704\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 710/900, test 0/100], loss: 113.9990, rmse_loss: 89.9817, kl_loss: 24.0173\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 720/900, test 0/100], loss: 79.0543, rmse_loss: 56.4597, kl_loss: 22.5946\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 730/900, test 0/100], loss: 86.6358, rmse_loss: 62.7789, kl_loss: 23.8569\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 740/900, test 0/100], loss: 95.3734, rmse_loss: 71.5571, kl_loss: 23.8163\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 750/900, test 0/100], loss: 95.3868, rmse_loss: 72.8308, kl_loss: 22.5560\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 760/900, test 0/100], loss: 82.8247, rmse_loss: 60.7928, kl_loss: 22.0319\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 770/900, test 0/100], loss: 91.3711, rmse_loss: 69.0160, kl_loss: 22.3551\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 780/900, test 0/100], loss: 95.5263, rmse_loss: 70.3962, kl_loss: 25.1301\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 790/900, test 0/100], loss: 109.5491, rmse_loss: 88.1268, kl_loss: 21.4224\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 800/900, test 0/100], loss: 111.4317, rmse_loss: 87.8517, kl_loss: 23.5800\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 810/900, test 0/100], loss: 118.8157, rmse_loss: 94.9437, kl_loss: 23.8720\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 820/900, test 0/100], loss: 134.4286, rmse_loss: 108.7736, kl_loss: 25.6550\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 830/900, test 0/100], loss: 137.0288, rmse_loss: 113.2828, kl_loss: 23.7460\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 840/900, test 0/100], loss: 95.9886, rmse_loss: 73.1328, kl_loss: 22.8558\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 850/900, test 0/100], loss: 96.7675, rmse_loss: 73.7877, kl_loss: 22.9798\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 860/900, test 0/100], loss: 81.0736, rmse_loss: 57.4278, kl_loss: 23.6458\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 870/900, test 0/100], loss: 91.6919, rmse_loss: 68.2684, kl_loss: 23.4235\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 880/900, test 0/100], loss: 93.4919, rmse_loss: 70.4195, kl_loss: 23.0724\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 890/900, test 0/100], loss: 93.6355, rmse_loss: 71.8991, kl_loss: 21.7363\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 900/900, test 0/100], loss: 108.1358, rmse_loss: 83.9159, kl_loss: 24.2199\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 3\n",
      "test: epoch 3\n",
      "[epoch 3/10][train 9000/900, test 10/100], loss: 109.5577, rmse_loss: 86.1269, kl_loss: 23.4309\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 9000/900, test 20/100], loss: 115.0714, rmse_loss: 90.4962, kl_loss: 24.5752\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 9000/900, test 30/100], loss: 94.3627, rmse_loss: 70.0778, kl_loss: 24.2849\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 9000/900, test 40/100], loss: 74.3868, rmse_loss: 52.8391, kl_loss: 21.5477\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 9000/900, test 50/100], loss: 97.9070, rmse_loss: 76.0042, kl_loss: 21.9028\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 9000/900, test 60/100], loss: 108.4098, rmse_loss: 85.9252, kl_loss: 22.4846\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 9000/900, test 70/100], loss: 105.0192, rmse_loss: 81.9365, kl_loss: 23.0827\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 9000/900, test 80/100], loss: 113.5415, rmse_loss: 88.9781, kl_loss: 24.5634\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 9000/900, test 90/100], loss: 105.7828, rmse_loss: 81.0564, kl_loss: 24.7263\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 3/10][train 9000/900, test 100/100], loss: 101.8573, rmse_loss: 76.4589, kl_loss: 25.3984\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 10/900, test 0/100], loss: 105.7975, rmse_loss: 81.5409, kl_loss: 24.2567\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 20/900, test 0/100], loss: 75.8694, rmse_loss: 54.5643, kl_loss: 21.3051\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 30/900, test 0/100], loss: 86.0906, rmse_loss: 62.8011, kl_loss: 23.2895\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 40/900, test 0/100], loss: 102.0614, rmse_loss: 80.9196, kl_loss: 21.1418\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 50/900, test 0/100], loss: 111.7765, rmse_loss: 86.5174, kl_loss: 25.2591\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 60/900, test 0/100], loss: 91.3937, rmse_loss: 69.1751, kl_loss: 22.2186\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 70/900, test 0/100], loss: 134.0291, rmse_loss: 110.2555, kl_loss: 23.7737\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 80/900, test 0/100], loss: 89.8331, rmse_loss: 67.1341, kl_loss: 22.6990\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 90/900, test 0/100], loss: 110.4046, rmse_loss: 87.8528, kl_loss: 22.5518\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 100/900, test 0/100], loss: 90.6437, rmse_loss: 66.2447, kl_loss: 24.3989\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 110/900, test 0/100], loss: 91.9236, rmse_loss: 68.9859, kl_loss: 22.9377\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 120/900, test 0/100], loss: 109.9791, rmse_loss: 86.4869, kl_loss: 23.4922\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 130/900, test 0/100], loss: 79.9471, rmse_loss: 57.9904, kl_loss: 21.9567\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 140/900, test 0/100], loss: 96.9091, rmse_loss: 74.1293, kl_loss: 22.7798\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 150/900, test 0/100], loss: 110.6956, rmse_loss: 85.8899, kl_loss: 24.8057\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 160/900, test 0/100], loss: 91.6663, rmse_loss: 69.0279, kl_loss: 22.6384\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 170/900, test 0/100], loss: 93.8053, rmse_loss: 70.2911, kl_loss: 23.5142\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 180/900, test 0/100], loss: 92.6343, rmse_loss: 69.5444, kl_loss: 23.0899\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 190/900, test 0/100], loss: 94.2003, rmse_loss: 70.2359, kl_loss: 23.9644\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 200/900, test 0/100], loss: 91.0588, rmse_loss: 67.4641, kl_loss: 23.5946\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 210/900, test 0/100], loss: 106.1151, rmse_loss: 82.6948, kl_loss: 23.4203\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 220/900, test 0/100], loss: 93.0099, rmse_loss: 71.5610, kl_loss: 21.4490\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 230/900, test 0/100], loss: 118.0386, rmse_loss: 93.9421, kl_loss: 24.0966\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 240/900, test 0/100], loss: 89.2000, rmse_loss: 66.8263, kl_loss: 22.3736\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 250/900, test 0/100], loss: 133.6715, rmse_loss: 109.1454, kl_loss: 24.5261\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 260/900, test 0/100], loss: 87.0178, rmse_loss: 62.2729, kl_loss: 24.7450\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 270/900, test 0/100], loss: 93.2981, rmse_loss: 71.3915, kl_loss: 21.9065\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 280/900, test 0/100], loss: 59.4944, rmse_loss: 37.7794, kl_loss: 21.7150\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 290/900, test 0/100], loss: 70.8978, rmse_loss: 48.3394, kl_loss: 22.5584\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 300/900, test 0/100], loss: 77.8864, rmse_loss: 54.6868, kl_loss: 23.1996\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 310/900, test 0/100], loss: 84.8135, rmse_loss: 60.8211, kl_loss: 23.9924\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 320/900, test 0/100], loss: 83.4816, rmse_loss: 59.9484, kl_loss: 23.5332\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 330/900, test 0/100], loss: 102.7454, rmse_loss: 78.9917, kl_loss: 23.7537\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 340/900, test 0/100], loss: 97.9786, rmse_loss: 74.4062, kl_loss: 23.5723\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 350/900, test 0/100], loss: 89.6302, rmse_loss: 67.4983, kl_loss: 22.1320\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 360/900, test 0/100], loss: 127.2875, rmse_loss: 101.6673, kl_loss: 25.6202\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 370/900, test 0/100], loss: 80.6144, rmse_loss: 57.9987, kl_loss: 22.6157\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 380/900, test 0/100], loss: 81.0885, rmse_loss: 57.8339, kl_loss: 23.2546\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 390/900, test 0/100], loss: 100.3738, rmse_loss: 76.5990, kl_loss: 23.7749\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 400/900, test 0/100], loss: 90.0482, rmse_loss: 67.4713, kl_loss: 22.5770\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 410/900, test 0/100], loss: 163.0184, rmse_loss: 138.5425, kl_loss: 24.4759\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 420/900, test 0/100], loss: 109.8327, rmse_loss: 85.8406, kl_loss: 23.9921\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 430/900, test 0/100], loss: 107.2922, rmse_loss: 82.7541, kl_loss: 24.5381\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 440/900, test 0/100], loss: 117.6264, rmse_loss: 93.2879, kl_loss: 24.3384\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 450/900, test 0/100], loss: 83.5151, rmse_loss: 61.1678, kl_loss: 22.3473\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 460/900, test 0/100], loss: 76.8765, rmse_loss: 54.4297, kl_loss: 22.4467\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 470/900, test 0/100], loss: 96.7640, rmse_loss: 74.0082, kl_loss: 22.7558\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 480/900, test 0/100], loss: 88.3636, rmse_loss: 64.4302, kl_loss: 23.9334\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 490/900, test 0/100], loss: 97.8271, rmse_loss: 74.7167, kl_loss: 23.1105\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 500/900, test 0/100], loss: 104.0220, rmse_loss: 79.7962, kl_loss: 24.2259\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 510/900, test 0/100], loss: 100.5804, rmse_loss: 76.7067, kl_loss: 23.8736\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 520/900, test 0/100], loss: 88.1776, rmse_loss: 64.0892, kl_loss: 24.0884\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 530/900, test 0/100], loss: 86.1655, rmse_loss: 62.2763, kl_loss: 23.8891\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 540/900, test 0/100], loss: 89.6398, rmse_loss: 68.4875, kl_loss: 21.1523\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 550/900, test 0/100], loss: 84.9662, rmse_loss: 62.3267, kl_loss: 22.6396\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 560/900, test 0/100], loss: 114.1061, rmse_loss: 88.4816, kl_loss: 25.6245\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 570/900, test 0/100], loss: 83.9910, rmse_loss: 60.8713, kl_loss: 23.1197\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 580/900, test 0/100], loss: 90.0921, rmse_loss: 67.8070, kl_loss: 22.2851\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 590/900, test 0/100], loss: 118.6943, rmse_loss: 95.5668, kl_loss: 23.1276\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 600/900, test 0/100], loss: 79.1431, rmse_loss: 57.1323, kl_loss: 22.0108\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 610/900, test 0/100], loss: 74.8593, rmse_loss: 51.9070, kl_loss: 22.9524\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 620/900, test 0/100], loss: 101.5609, rmse_loss: 79.0636, kl_loss: 22.4974\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 630/900, test 0/100], loss: 84.6704, rmse_loss: 63.1505, kl_loss: 21.5199\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 640/900, test 0/100], loss: 67.4865, rmse_loss: 45.8031, kl_loss: 21.6834\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 650/900, test 0/100], loss: 97.2463, rmse_loss: 74.0431, kl_loss: 23.2032\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 660/900, test 0/100], loss: 149.1219, rmse_loss: 125.2061, kl_loss: 23.9158\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 670/900, test 0/100], loss: 97.1561, rmse_loss: 73.4456, kl_loss: 23.7105\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 680/900, test 0/100], loss: 104.1555, rmse_loss: 79.6696, kl_loss: 24.4860\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 690/900, test 0/100], loss: 111.7504, rmse_loss: 89.4278, kl_loss: 22.3225\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 700/900, test 0/100], loss: 114.5303, rmse_loss: 92.2322, kl_loss: 22.2981\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 710/900, test 0/100], loss: 69.8149, rmse_loss: 49.0413, kl_loss: 20.7736\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 720/900, test 0/100], loss: 132.6339, rmse_loss: 109.1311, kl_loss: 23.5027\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 730/900, test 0/100], loss: 112.8454, rmse_loss: 89.1536, kl_loss: 23.6918\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 740/900, test 0/100], loss: 87.1109, rmse_loss: 64.0074, kl_loss: 23.1035\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 750/900, test 0/100], loss: 89.1501, rmse_loss: 65.7561, kl_loss: 23.3941\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 760/900, test 0/100], loss: 98.9673, rmse_loss: 73.8199, kl_loss: 25.1473\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 770/900, test 0/100], loss: 90.3817, rmse_loss: 69.1599, kl_loss: 21.2218\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 780/900, test 0/100], loss: 90.1563, rmse_loss: 65.3863, kl_loss: 24.7700\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 790/900, test 0/100], loss: 77.8901, rmse_loss: 54.8626, kl_loss: 23.0275\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 800/900, test 0/100], loss: 92.0439, rmse_loss: 67.8853, kl_loss: 24.1586\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 810/900, test 0/100], loss: 83.9797, rmse_loss: 58.5314, kl_loss: 25.4483\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 820/900, test 0/100], loss: 96.9768, rmse_loss: 72.4556, kl_loss: 24.5212\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 830/900, test 0/100], loss: 118.1887, rmse_loss: 92.5988, kl_loss: 25.5900\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 840/900, test 0/100], loss: 116.9130, rmse_loss: 92.6037, kl_loss: 24.3093\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 850/900, test 0/100], loss: 94.7408, rmse_loss: 72.1203, kl_loss: 22.6204\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 860/900, test 0/100], loss: 72.1047, rmse_loss: 49.3224, kl_loss: 22.7823\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 870/900, test 0/100], loss: 113.8128, rmse_loss: 88.9112, kl_loss: 24.9016\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 880/900, test 0/100], loss: 77.6425, rmse_loss: 54.4331, kl_loss: 23.2093\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 890/900, test 0/100], loss: 79.5420, rmse_loss: 55.8224, kl_loss: 23.7196\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 900/900, test 0/100], loss: 81.3511, rmse_loss: 58.3009, kl_loss: 23.0501\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 4\n",
      "test: epoch 4\n",
      "[epoch 4/10][train 9000/900, test 10/100], loss: 101.1675, rmse_loss: 77.7867, kl_loss: 23.3808\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 9000/900, test 20/100], loss: 109.3708, rmse_loss: 85.2562, kl_loss: 24.1147\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 9000/900, test 30/100], loss: 86.6287, rmse_loss: 63.1193, kl_loss: 23.5094\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 9000/900, test 40/100], loss: 63.5657, rmse_loss: 41.9165, kl_loss: 21.6492\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 9000/900, test 50/100], loss: 92.4981, rmse_loss: 71.2024, kl_loss: 21.2957\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 9000/900, test 60/100], loss: 95.9363, rmse_loss: 73.9055, kl_loss: 22.0308\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 9000/900, test 70/100], loss: 94.7452, rmse_loss: 71.2874, kl_loss: 23.4578\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 9000/900, test 80/100], loss: 103.8697, rmse_loss: 79.8101, kl_loss: 24.0596\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 9000/900, test 90/100], loss: 95.3702, rmse_loss: 70.9627, kl_loss: 24.4076\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 4/10][train 9000/900, test 100/100], loss: 97.9837, rmse_loss: 72.7466, kl_loss: 25.2371\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 10/900, test 0/100], loss: 72.8994, rmse_loss: 49.4689, kl_loss: 23.4305\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 20/900, test 0/100], loss: 93.8556, rmse_loss: 68.1265, kl_loss: 25.7291\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 30/900, test 0/100], loss: 70.1284, rmse_loss: 46.6664, kl_loss: 23.4620\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 40/900, test 0/100], loss: 98.9019, rmse_loss: 74.2591, kl_loss: 24.6428\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 50/900, test 0/100], loss: 102.2427, rmse_loss: 77.2688, kl_loss: 24.9739\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 60/900, test 0/100], loss: 84.5058, rmse_loss: 61.6879, kl_loss: 22.8179\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 70/900, test 0/100], loss: 84.0185, rmse_loss: 60.7808, kl_loss: 23.2378\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 80/900, test 0/100], loss: 76.7258, rmse_loss: 54.7161, kl_loss: 22.0097\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 90/900, test 0/100], loss: 95.8740, rmse_loss: 73.1173, kl_loss: 22.7567\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 100/900, test 0/100], loss: 79.4488, rmse_loss: 55.7559, kl_loss: 23.6928\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 110/900, test 0/100], loss: 109.0419, rmse_loss: 85.5017, kl_loss: 23.5403\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 120/900, test 0/100], loss: 78.6771, rmse_loss: 56.5440, kl_loss: 22.1331\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 130/900, test 0/100], loss: 70.6200, rmse_loss: 47.7447, kl_loss: 22.8753\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 140/900, test 0/100], loss: 113.5053, rmse_loss: 89.3171, kl_loss: 24.1882\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 150/900, test 0/100], loss: 89.1746, rmse_loss: 66.0070, kl_loss: 23.1676\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 160/900, test 0/100], loss: 79.1114, rmse_loss: 57.2220, kl_loss: 21.8894\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 170/900, test 0/100], loss: 71.2917, rmse_loss: 47.9227, kl_loss: 23.3689\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 180/900, test 0/100], loss: 68.9290, rmse_loss: 46.8991, kl_loss: 22.0299\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 190/900, test 0/100], loss: 101.7503, rmse_loss: 76.6946, kl_loss: 25.0557\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 200/900, test 0/100], loss: 116.7438, rmse_loss: 92.1394, kl_loss: 24.6044\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 210/900, test 0/100], loss: 108.9372, rmse_loss: 84.4093, kl_loss: 24.5279\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 220/900, test 0/100], loss: 101.0069, rmse_loss: 76.6355, kl_loss: 24.3714\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 230/900, test 0/100], loss: 85.1956, rmse_loss: 62.3203, kl_loss: 22.8753\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 240/900, test 0/100], loss: 110.0894, rmse_loss: 85.6018, kl_loss: 24.4876\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 250/900, test 0/100], loss: 94.9850, rmse_loss: 71.7691, kl_loss: 23.2159\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 260/900, test 0/100], loss: 99.3210, rmse_loss: 76.5665, kl_loss: 22.7544\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 270/900, test 0/100], loss: 110.9550, rmse_loss: 86.0212, kl_loss: 24.9338\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 280/900, test 0/100], loss: 80.6410, rmse_loss: 56.4543, kl_loss: 24.1866\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 290/900, test 0/100], loss: 90.9469, rmse_loss: 68.1594, kl_loss: 22.7875\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 300/900, test 0/100], loss: 75.8947, rmse_loss: 51.9187, kl_loss: 23.9761\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 310/900, test 0/100], loss: 89.5223, rmse_loss: 64.7851, kl_loss: 24.7373\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 320/900, test 0/100], loss: 76.7313, rmse_loss: 52.9167, kl_loss: 23.8146\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 330/900, test 0/100], loss: 87.8582, rmse_loss: 64.2607, kl_loss: 23.5975\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 340/900, test 0/100], loss: 75.4464, rmse_loss: 52.4704, kl_loss: 22.9759\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 350/900, test 0/100], loss: 136.0771, rmse_loss: 113.1616, kl_loss: 22.9155\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 360/900, test 0/100], loss: 108.8574, rmse_loss: 84.1639, kl_loss: 24.6935\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 370/900, test 0/100], loss: 86.0871, rmse_loss: 63.1239, kl_loss: 22.9633\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 380/900, test 0/100], loss: 85.8017, rmse_loss: 64.5306, kl_loss: 21.2711\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 390/900, test 0/100], loss: 80.1382, rmse_loss: 58.2097, kl_loss: 21.9285\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 400/900, test 0/100], loss: 98.1582, rmse_loss: 74.8350, kl_loss: 23.3232\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 410/900, test 0/100], loss: 90.3954, rmse_loss: 68.6454, kl_loss: 21.7500\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 420/900, test 0/100], loss: 92.6350, rmse_loss: 67.8129, kl_loss: 24.8221\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 430/900, test 0/100], loss: 68.6658, rmse_loss: 46.7909, kl_loss: 21.8749\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 440/900, test 0/100], loss: 96.0096, rmse_loss: 70.9701, kl_loss: 25.0395\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 450/900, test 0/100], loss: 98.2372, rmse_loss: 73.9269, kl_loss: 24.3103\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 460/900, test 0/100], loss: 72.3187, rmse_loss: 49.5264, kl_loss: 22.7923\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 470/900, test 0/100], loss: 100.3880, rmse_loss: 76.3216, kl_loss: 24.0664\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 480/900, test 0/100], loss: 75.1859, rmse_loss: 52.2106, kl_loss: 22.9753\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 490/900, test 0/100], loss: 91.8407, rmse_loss: 68.0685, kl_loss: 23.7722\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 500/900, test 0/100], loss: 104.4770, rmse_loss: 80.3930, kl_loss: 24.0840\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 510/900, test 0/100], loss: 105.4977, rmse_loss: 81.1821, kl_loss: 24.3156\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 520/900, test 0/100], loss: 100.0882, rmse_loss: 76.7626, kl_loss: 23.3255\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 530/900, test 0/100], loss: 97.8533, rmse_loss: 73.2931, kl_loss: 24.5602\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 540/900, test 0/100], loss: 74.9748, rmse_loss: 52.4829, kl_loss: 22.4919\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 550/900, test 0/100], loss: 85.3675, rmse_loss: 61.8154, kl_loss: 23.5521\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 560/900, test 0/100], loss: 101.9543, rmse_loss: 77.8598, kl_loss: 24.0945\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 570/900, test 0/100], loss: 84.3754, rmse_loss: 61.2728, kl_loss: 23.1026\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 580/900, test 0/100], loss: 108.2921, rmse_loss: 82.6927, kl_loss: 25.5994\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 590/900, test 0/100], loss: 94.1177, rmse_loss: 71.2727, kl_loss: 22.8450\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 600/900, test 0/100], loss: 68.1743, rmse_loss: 45.4166, kl_loss: 22.7577\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 610/900, test 0/100], loss: 70.9831, rmse_loss: 48.2185, kl_loss: 22.7646\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 620/900, test 0/100], loss: 86.2987, rmse_loss: 61.8365, kl_loss: 24.4622\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 630/900, test 0/100], loss: 73.5896, rmse_loss: 51.1338, kl_loss: 22.4558\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 640/900, test 0/100], loss: 93.1341, rmse_loss: 71.3346, kl_loss: 21.7995\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 650/900, test 0/100], loss: 96.9371, rmse_loss: 72.8800, kl_loss: 24.0572\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 660/900, test 0/100], loss: 83.5428, rmse_loss: 60.2922, kl_loss: 23.2506\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 670/900, test 0/100], loss: 105.5715, rmse_loss: 81.6794, kl_loss: 23.8921\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 680/900, test 0/100], loss: 88.7306, rmse_loss: 66.3934, kl_loss: 22.3371\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 690/900, test 0/100], loss: 80.0833, rmse_loss: 56.4746, kl_loss: 23.6087\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 700/900, test 0/100], loss: 70.8768, rmse_loss: 48.3431, kl_loss: 22.5337\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 710/900, test 0/100], loss: 76.7085, rmse_loss: 55.0757, kl_loss: 21.6328\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 720/900, test 0/100], loss: 110.5905, rmse_loss: 84.4329, kl_loss: 26.1576\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 730/900, test 0/100], loss: 81.7429, rmse_loss: 59.2740, kl_loss: 22.4688\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 740/900, test 0/100], loss: 92.3675, rmse_loss: 68.5695, kl_loss: 23.7980\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 750/900, test 0/100], loss: 105.2797, rmse_loss: 81.2961, kl_loss: 23.9836\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 760/900, test 0/100], loss: 83.5627, rmse_loss: 58.9144, kl_loss: 24.6483\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 770/900, test 0/100], loss: 86.6646, rmse_loss: 61.6499, kl_loss: 25.0147\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 780/900, test 0/100], loss: 114.2643, rmse_loss: 90.1015, kl_loss: 24.1628\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 790/900, test 0/100], loss: 113.2741, rmse_loss: 87.9861, kl_loss: 25.2880\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 800/900, test 0/100], loss: 81.8484, rmse_loss: 58.0603, kl_loss: 23.7881\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 810/900, test 0/100], loss: 108.5288, rmse_loss: 84.5218, kl_loss: 24.0071\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 820/900, test 0/100], loss: 79.1425, rmse_loss: 57.2235, kl_loss: 21.9190\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 830/900, test 0/100], loss: 111.1490, rmse_loss: 87.2349, kl_loss: 23.9142\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 840/900, test 0/100], loss: 77.6360, rmse_loss: 53.7629, kl_loss: 23.8731\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 850/900, test 0/100], loss: 94.9965, rmse_loss: 70.1431, kl_loss: 24.8534\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 860/900, test 0/100], loss: 135.1108, rmse_loss: 107.9585, kl_loss: 27.1523\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 870/900, test 0/100], loss: 89.4470, rmse_loss: 66.4960, kl_loss: 22.9510\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 880/900, test 0/100], loss: 87.6236, rmse_loss: 62.5048, kl_loss: 25.1188\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 890/900, test 0/100], loss: 86.6388, rmse_loss: 62.7058, kl_loss: 23.9330\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 900/900, test 0/100], loss: 87.6043, rmse_loss: 62.9687, kl_loss: 24.6356\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 5\n",
      "test: epoch 5\n",
      "[epoch 5/10][train 9000/900, test 10/100], loss: 97.2023, rmse_loss: 72.4047, kl_loss: 24.7976\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 9000/900, test 20/100], loss: 103.1676, rmse_loss: 78.0150, kl_loss: 25.1527\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 9000/900, test 30/100], loss: 84.7220, rmse_loss: 60.5425, kl_loss: 24.1795\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 9000/900, test 40/100], loss: 62.9715, rmse_loss: 40.4313, kl_loss: 22.5402\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 9000/900, test 50/100], loss: 89.0527, rmse_loss: 66.7411, kl_loss: 22.3116\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 9000/900, test 60/100], loss: 99.2439, rmse_loss: 76.3184, kl_loss: 22.9255\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 9000/900, test 70/100], loss: 91.0755, rmse_loss: 66.8426, kl_loss: 24.2329\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 9000/900, test 80/100], loss: 105.9475, rmse_loss: 80.7846, kl_loss: 25.1629\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 9000/900, test 90/100], loss: 92.9469, rmse_loss: 67.4967, kl_loss: 25.4503\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 5/10][train 9000/900, test 100/100], loss: 91.9607, rmse_loss: 65.6402, kl_loss: 26.3205\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 10/900, test 0/100], loss: 84.6076, rmse_loss: 61.4352, kl_loss: 23.1724\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 20/900, test 0/100], loss: 106.8164, rmse_loss: 80.3425, kl_loss: 26.4739\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 30/900, test 0/100], loss: 106.8048, rmse_loss: 81.5896, kl_loss: 25.2152\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 40/900, test 0/100], loss: 75.1135, rmse_loss: 53.2336, kl_loss: 21.8799\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 50/900, test 0/100], loss: 76.3922, rmse_loss: 55.0830, kl_loss: 21.3091\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 60/900, test 0/100], loss: 109.1066, rmse_loss: 84.2159, kl_loss: 24.8907\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 70/900, test 0/100], loss: 85.5626, rmse_loss: 60.1721, kl_loss: 25.3906\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 80/900, test 0/100], loss: 79.3863, rmse_loss: 54.6295, kl_loss: 24.7568\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 90/900, test 0/100], loss: 90.9697, rmse_loss: 66.2279, kl_loss: 24.7419\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 100/900, test 0/100], loss: 65.3769, rmse_loss: 42.9136, kl_loss: 22.4633\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 110/900, test 0/100], loss: 134.7017, rmse_loss: 108.1086, kl_loss: 26.5931\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 120/900, test 0/100], loss: 82.8500, rmse_loss: 58.8311, kl_loss: 24.0189\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 130/900, test 0/100], loss: 107.6023, rmse_loss: 81.2617, kl_loss: 26.3405\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 140/900, test 0/100], loss: 83.6926, rmse_loss: 58.1030, kl_loss: 25.5896\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 150/900, test 0/100], loss: 71.0318, rmse_loss: 48.6506, kl_loss: 22.3812\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 160/900, test 0/100], loss: 96.4165, rmse_loss: 73.0766, kl_loss: 23.3400\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 170/900, test 0/100], loss: 113.8147, rmse_loss: 90.1815, kl_loss: 23.6332\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 180/900, test 0/100], loss: 78.5184, rmse_loss: 55.0395, kl_loss: 23.4789\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 190/900, test 0/100], loss: 72.5668, rmse_loss: 49.7073, kl_loss: 22.8594\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 200/900, test 0/100], loss: 83.7032, rmse_loss: 58.8013, kl_loss: 24.9019\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 210/900, test 0/100], loss: 79.2012, rmse_loss: 56.3601, kl_loss: 22.8411\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 220/900, test 0/100], loss: 73.1953, rmse_loss: 50.6623, kl_loss: 22.5330\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 230/900, test 0/100], loss: 114.6758, rmse_loss: 88.7959, kl_loss: 25.8799\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 240/900, test 0/100], loss: 87.1525, rmse_loss: 64.6376, kl_loss: 22.5150\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 250/900, test 0/100], loss: 132.0147, rmse_loss: 106.6537, kl_loss: 25.3610\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 260/900, test 0/100], loss: 75.1099, rmse_loss: 50.4532, kl_loss: 24.6567\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 270/900, test 0/100], loss: 83.4530, rmse_loss: 60.2525, kl_loss: 23.2004\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 280/900, test 0/100], loss: 89.2186, rmse_loss: 65.5200, kl_loss: 23.6986\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 290/900, test 0/100], loss: 88.5046, rmse_loss: 64.9040, kl_loss: 23.6006\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 300/900, test 0/100], loss: 82.9670, rmse_loss: 59.3669, kl_loss: 23.6001\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 310/900, test 0/100], loss: 97.4140, rmse_loss: 73.0702, kl_loss: 24.3438\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 320/900, test 0/100], loss: 116.5914, rmse_loss: 92.0784, kl_loss: 24.5131\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 330/900, test 0/100], loss: 105.6475, rmse_loss: 78.9700, kl_loss: 26.6775\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 340/900, test 0/100], loss: 103.5046, rmse_loss: 77.8860, kl_loss: 25.6186\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 350/900, test 0/100], loss: 86.8995, rmse_loss: 63.4279, kl_loss: 23.4716\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 360/900, test 0/100], loss: 101.2399, rmse_loss: 74.5566, kl_loss: 26.6832\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 370/900, test 0/100], loss: 97.3503, rmse_loss: 74.6351, kl_loss: 22.7152\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 380/900, test 0/100], loss: 106.9680, rmse_loss: 81.7105, kl_loss: 25.2575\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 390/900, test 0/100], loss: 83.4416, rmse_loss: 60.0294, kl_loss: 23.4122\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 400/900, test 0/100], loss: 74.0180, rmse_loss: 52.4010, kl_loss: 21.6171\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 410/900, test 0/100], loss: 86.4379, rmse_loss: 61.5793, kl_loss: 24.8587\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 420/900, test 0/100], loss: 85.4571, rmse_loss: 61.9621, kl_loss: 23.4949\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 430/900, test 0/100], loss: 87.9399, rmse_loss: 65.4585, kl_loss: 22.4814\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 440/900, test 0/100], loss: 74.2814, rmse_loss: 50.9079, kl_loss: 23.3736\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 450/900, test 0/100], loss: 86.5555, rmse_loss: 62.4418, kl_loss: 24.1137\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 460/900, test 0/100], loss: 88.9766, rmse_loss: 63.6928, kl_loss: 25.2839\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 470/900, test 0/100], loss: 79.8198, rmse_loss: 56.7434, kl_loss: 23.0764\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 480/900, test 0/100], loss: 70.5301, rmse_loss: 47.7712, kl_loss: 22.7589\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 490/900, test 0/100], loss: 94.7191, rmse_loss: 71.9538, kl_loss: 22.7653\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 500/900, test 0/100], loss: 79.3241, rmse_loss: 57.0892, kl_loss: 22.2349\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 510/900, test 0/100], loss: 79.8558, rmse_loss: 55.2143, kl_loss: 24.6416\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 520/900, test 0/100], loss: 80.1176, rmse_loss: 57.4762, kl_loss: 22.6414\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 530/900, test 0/100], loss: 99.7888, rmse_loss: 75.1022, kl_loss: 24.6867\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 540/900, test 0/100], loss: 88.4291, rmse_loss: 63.7442, kl_loss: 24.6850\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 550/900, test 0/100], loss: 70.2432, rmse_loss: 46.7577, kl_loss: 23.4854\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 560/900, test 0/100], loss: 102.2070, rmse_loss: 77.1572, kl_loss: 25.0499\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 570/900, test 0/100], loss: 78.6963, rmse_loss: 54.3547, kl_loss: 24.3417\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 580/900, test 0/100], loss: 71.8008, rmse_loss: 50.5547, kl_loss: 21.2461\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 590/900, test 0/100], loss: 85.2841, rmse_loss: 61.6676, kl_loss: 23.6166\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 600/900, test 0/100], loss: 73.5350, rmse_loss: 50.6169, kl_loss: 22.9181\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 610/900, test 0/100], loss: 96.9007, rmse_loss: 70.7918, kl_loss: 26.1089\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 620/900, test 0/100], loss: 82.4035, rmse_loss: 57.1846, kl_loss: 25.2188\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 630/900, test 0/100], loss: 67.5429, rmse_loss: 45.0956, kl_loss: 22.4472\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 640/900, test 0/100], loss: 88.1384, rmse_loss: 63.2844, kl_loss: 24.8541\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 650/900, test 0/100], loss: 129.1714, rmse_loss: 105.5465, kl_loss: 23.6249\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 660/900, test 0/100], loss: 91.3631, rmse_loss: 68.0329, kl_loss: 23.3303\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 670/900, test 0/100], loss: 101.1580, rmse_loss: 75.2940, kl_loss: 25.8640\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 680/900, test 0/100], loss: 102.8428, rmse_loss: 78.0778, kl_loss: 24.7650\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 690/900, test 0/100], loss: 87.6432, rmse_loss: 62.4974, kl_loss: 25.1458\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 700/900, test 0/100], loss: 85.0879, rmse_loss: 60.9457, kl_loss: 24.1422\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 710/900, test 0/100], loss: 92.7327, rmse_loss: 68.7697, kl_loss: 23.9630\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 720/900, test 0/100], loss: 82.7120, rmse_loss: 58.0077, kl_loss: 24.7043\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 730/900, test 0/100], loss: 91.1626, rmse_loss: 67.7858, kl_loss: 23.3768\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 740/900, test 0/100], loss: 109.7419, rmse_loss: 84.9887, kl_loss: 24.7531\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 750/900, test 0/100], loss: 78.5297, rmse_loss: 55.1985, kl_loss: 23.3312\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 760/900, test 0/100], loss: 102.6515, rmse_loss: 79.4624, kl_loss: 23.1890\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 770/900, test 0/100], loss: 113.4486, rmse_loss: 86.2378, kl_loss: 27.2109\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 780/900, test 0/100], loss: 78.1506, rmse_loss: 54.1566, kl_loss: 23.9940\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 790/900, test 0/100], loss: 95.3774, rmse_loss: 69.5595, kl_loss: 25.8179\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 800/900, test 0/100], loss: 158.5007, rmse_loss: 131.9358, kl_loss: 26.5649\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 810/900, test 0/100], loss: 79.2758, rmse_loss: 57.1185, kl_loss: 22.1573\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 820/900, test 0/100], loss: 93.2685, rmse_loss: 68.7688, kl_loss: 24.4997\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 830/900, test 0/100], loss: 69.9102, rmse_loss: 45.9342, kl_loss: 23.9760\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 840/900, test 0/100], loss: 64.3780, rmse_loss: 41.7645, kl_loss: 22.6135\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 850/900, test 0/100], loss: 75.6261, rmse_loss: 52.1260, kl_loss: 23.5001\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 860/900, test 0/100], loss: 77.1873, rmse_loss: 54.6049, kl_loss: 22.5824\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 870/900, test 0/100], loss: 82.2509, rmse_loss: 59.2246, kl_loss: 23.0263\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 880/900, test 0/100], loss: 95.9931, rmse_loss: 70.3463, kl_loss: 25.6468\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 890/900, test 0/100], loss: 85.6833, rmse_loss: 60.2159, kl_loss: 25.4674\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 900/900, test 0/100], loss: 129.1428, rmse_loss: 103.3140, kl_loss: 25.8288\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 6\n",
      "test: epoch 6\n",
      "[epoch 6/10][train 9000/900, test 10/100], loss: 87.1647, rmse_loss: 61.7334, kl_loss: 25.4314\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 9000/900, test 20/100], loss: 92.7808, rmse_loss: 66.9985, kl_loss: 25.7824\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 9000/900, test 30/100], loss: 77.1805, rmse_loss: 52.7872, kl_loss: 24.3933\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 9000/900, test 40/100], loss: 57.8217, rmse_loss: 35.3656, kl_loss: 22.4561\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 9000/900, test 50/100], loss: 82.8713, rmse_loss: 60.3489, kl_loss: 22.5225\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 9000/900, test 60/100], loss: 86.0286, rmse_loss: 62.6431, kl_loss: 23.3856\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 9000/900, test 70/100], loss: 83.7651, rmse_loss: 59.2663, kl_loss: 24.4988\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 9000/900, test 80/100], loss: 96.1956, rmse_loss: 70.9073, kl_loss: 25.2883\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 9000/900, test 90/100], loss: 83.9013, rmse_loss: 58.4087, kl_loss: 25.4926\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 6/10][train 9000/900, test 100/100], loss: 85.2897, rmse_loss: 58.9836, kl_loss: 26.3061\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 10/900, test 0/100], loss: 111.8013, rmse_loss: 87.5651, kl_loss: 24.2362\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 20/900, test 0/100], loss: 85.1286, rmse_loss: 60.0124, kl_loss: 25.1162\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 30/900, test 0/100], loss: 104.3366, rmse_loss: 78.5284, kl_loss: 25.8082\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 40/900, test 0/100], loss: 85.9057, rmse_loss: 63.0791, kl_loss: 22.8266\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 50/900, test 0/100], loss: 62.2976, rmse_loss: 40.2016, kl_loss: 22.0961\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 60/900, test 0/100], loss: 102.7960, rmse_loss: 78.7736, kl_loss: 24.0224\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 70/900, test 0/100], loss: 75.6625, rmse_loss: 51.5424, kl_loss: 24.1201\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 80/900, test 0/100], loss: 103.1341, rmse_loss: 78.7509, kl_loss: 24.3831\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 90/900, test 0/100], loss: 94.2066, rmse_loss: 68.4782, kl_loss: 25.7284\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 100/900, test 0/100], loss: 90.9147, rmse_loss: 65.7487, kl_loss: 25.1659\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 110/900, test 0/100], loss: 82.9123, rmse_loss: 58.7299, kl_loss: 24.1824\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 120/900, test 0/100], loss: 101.6113, rmse_loss: 75.3700, kl_loss: 26.2413\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 130/900, test 0/100], loss: 80.4534, rmse_loss: 57.5716, kl_loss: 22.8818\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 140/900, test 0/100], loss: 85.3109, rmse_loss: 63.4014, kl_loss: 21.9095\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 150/900, test 0/100], loss: 92.3272, rmse_loss: 68.1880, kl_loss: 24.1392\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 160/900, test 0/100], loss: 96.7361, rmse_loss: 72.2464, kl_loss: 24.4897\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 170/900, test 0/100], loss: 84.4100, rmse_loss: 61.0456, kl_loss: 23.3645\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 180/900, test 0/100], loss: 66.7807, rmse_loss: 43.4484, kl_loss: 23.3323\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 190/900, test 0/100], loss: 76.5127, rmse_loss: 52.2184, kl_loss: 24.2944\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 200/900, test 0/100], loss: 81.1351, rmse_loss: 56.2040, kl_loss: 24.9311\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 210/900, test 0/100], loss: 88.1520, rmse_loss: 63.4042, kl_loss: 24.7478\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 220/900, test 0/100], loss: 67.1646, rmse_loss: 43.5131, kl_loss: 23.6514\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 230/900, test 0/100], loss: 82.3916, rmse_loss: 58.9415, kl_loss: 23.4502\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 240/900, test 0/100], loss: 104.8195, rmse_loss: 79.0670, kl_loss: 25.7525\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 250/900, test 0/100], loss: 74.7894, rmse_loss: 50.4926, kl_loss: 24.2969\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 260/900, test 0/100], loss: 80.8150, rmse_loss: 57.1081, kl_loss: 23.7069\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 270/900, test 0/100], loss: 80.5113, rmse_loss: 56.9603, kl_loss: 23.5511\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 280/900, test 0/100], loss: 74.8073, rmse_loss: 51.1764, kl_loss: 23.6308\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 290/900, test 0/100], loss: 69.2817, rmse_loss: 45.9156, kl_loss: 23.3662\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 300/900, test 0/100], loss: 77.4903, rmse_loss: 53.3205, kl_loss: 24.1699\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 310/900, test 0/100], loss: 86.0376, rmse_loss: 60.3553, kl_loss: 25.6823\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 320/900, test 0/100], loss: 82.0881, rmse_loss: 57.9619, kl_loss: 24.1262\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 330/900, test 0/100], loss: 72.1289, rmse_loss: 49.0604, kl_loss: 23.0686\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 340/900, test 0/100], loss: 85.4887, rmse_loss: 60.4433, kl_loss: 25.0455\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 350/900, test 0/100], loss: 73.7734, rmse_loss: 51.0912, kl_loss: 22.6821\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 360/900, test 0/100], loss: 74.1090, rmse_loss: 50.7475, kl_loss: 23.3616\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 370/900, test 0/100], loss: 85.9524, rmse_loss: 60.8423, kl_loss: 25.1101\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 380/900, test 0/100], loss: 96.3429, rmse_loss: 71.8584, kl_loss: 24.4846\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 390/900, test 0/100], loss: 83.5645, rmse_loss: 57.6356, kl_loss: 25.9289\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 400/900, test 0/100], loss: 91.7813, rmse_loss: 68.1052, kl_loss: 23.6761\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 410/900, test 0/100], loss: 84.9010, rmse_loss: 59.3758, kl_loss: 25.5252\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 420/900, test 0/100], loss: 93.1510, rmse_loss: 67.6978, kl_loss: 25.4532\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 430/900, test 0/100], loss: 88.2977, rmse_loss: 63.0477, kl_loss: 25.2500\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 440/900, test 0/100], loss: 85.4732, rmse_loss: 60.8317, kl_loss: 24.6415\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 450/900, test 0/100], loss: 77.7921, rmse_loss: 54.1853, kl_loss: 23.6067\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 460/900, test 0/100], loss: 123.9762, rmse_loss: 98.1476, kl_loss: 25.8285\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 470/900, test 0/100], loss: 72.1305, rmse_loss: 48.4601, kl_loss: 23.6704\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 480/900, test 0/100], loss: 102.9938, rmse_loss: 76.9108, kl_loss: 26.0829\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 490/900, test 0/100], loss: 85.6875, rmse_loss: 61.2222, kl_loss: 24.4652\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 500/900, test 0/100], loss: 57.8844, rmse_loss: 35.0579, kl_loss: 22.8265\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 510/900, test 0/100], loss: 71.3419, rmse_loss: 48.4269, kl_loss: 22.9150\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 520/900, test 0/100], loss: 89.2679, rmse_loss: 65.3216, kl_loss: 23.9463\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 530/900, test 0/100], loss: 107.6781, rmse_loss: 83.1291, kl_loss: 24.5490\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 540/900, test 0/100], loss: 68.7792, rmse_loss: 45.2368, kl_loss: 23.5424\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 550/900, test 0/100], loss: 73.4760, rmse_loss: 50.0632, kl_loss: 23.4129\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 560/900, test 0/100], loss: 79.0378, rmse_loss: 54.3371, kl_loss: 24.7007\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 570/900, test 0/100], loss: 68.8193, rmse_loss: 44.7370, kl_loss: 24.0823\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 580/900, test 0/100], loss: 70.3686, rmse_loss: 45.6120, kl_loss: 24.7565\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 590/900, test 0/100], loss: 91.3258, rmse_loss: 66.2771, kl_loss: 25.0487\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 600/900, test 0/100], loss: 83.3491, rmse_loss: 58.7482, kl_loss: 24.6009\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 610/900, test 0/100], loss: 74.4948, rmse_loss: 50.2433, kl_loss: 24.2516\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 620/900, test 0/100], loss: 92.6728, rmse_loss: 68.8321, kl_loss: 23.8407\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 630/900, test 0/100], loss: 87.6824, rmse_loss: 64.9734, kl_loss: 22.7090\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 640/900, test 0/100], loss: 93.6006, rmse_loss: 69.5899, kl_loss: 24.0107\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 650/900, test 0/100], loss: 70.0524, rmse_loss: 48.1294, kl_loss: 21.9230\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 660/900, test 0/100], loss: 93.5142, rmse_loss: 68.1168, kl_loss: 25.3975\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 670/900, test 0/100], loss: 82.0560, rmse_loss: 57.5308, kl_loss: 24.5252\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 680/900, test 0/100], loss: 83.1657, rmse_loss: 58.9004, kl_loss: 24.2654\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 690/900, test 0/100], loss: 71.1176, rmse_loss: 47.4264, kl_loss: 23.6912\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 700/900, test 0/100], loss: 76.9760, rmse_loss: 51.8243, kl_loss: 25.1517\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 710/900, test 0/100], loss: 63.6754, rmse_loss: 41.2270, kl_loss: 22.4484\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 720/900, test 0/100], loss: 94.3139, rmse_loss: 69.0474, kl_loss: 25.2666\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 730/900, test 0/100], loss: 81.8240, rmse_loss: 58.1639, kl_loss: 23.6602\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 740/900, test 0/100], loss: 69.7195, rmse_loss: 47.5450, kl_loss: 22.1745\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 750/900, test 0/100], loss: 74.4935, rmse_loss: 49.8499, kl_loss: 24.6437\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 760/900, test 0/100], loss: 66.2604, rmse_loss: 41.3590, kl_loss: 24.9014\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 770/900, test 0/100], loss: 64.6453, rmse_loss: 42.3067, kl_loss: 22.3386\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 780/900, test 0/100], loss: 116.3078, rmse_loss: 90.0535, kl_loss: 26.2543\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 790/900, test 0/100], loss: 118.1752, rmse_loss: 92.5357, kl_loss: 25.6395\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 800/900, test 0/100], loss: 66.4076, rmse_loss: 44.4168, kl_loss: 21.9909\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 810/900, test 0/100], loss: 82.8365, rmse_loss: 56.8085, kl_loss: 26.0280\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 820/900, test 0/100], loss: 87.3328, rmse_loss: 63.3396, kl_loss: 23.9932\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 830/900, test 0/100], loss: 119.2681, rmse_loss: 93.3809, kl_loss: 25.8872\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 840/900, test 0/100], loss: 86.3337, rmse_loss: 61.6130, kl_loss: 24.7207\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 850/900, test 0/100], loss: 71.1238, rmse_loss: 48.9947, kl_loss: 22.1291\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 860/900, test 0/100], loss: 83.2159, rmse_loss: 59.2815, kl_loss: 23.9344\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 870/900, test 0/100], loss: 66.4822, rmse_loss: 42.0590, kl_loss: 24.4231\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 880/900, test 0/100], loss: 80.9628, rmse_loss: 57.6109, kl_loss: 23.3519\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 890/900, test 0/100], loss: 96.7009, rmse_loss: 73.6197, kl_loss: 23.0812\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 900/900, test 0/100], loss: 82.7864, rmse_loss: 58.3027, kl_loss: 24.4836\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 7\n",
      "test: epoch 7\n",
      "[epoch 7/10][train 9000/900, test 10/100], loss: 82.2657, rmse_loss: 56.8621, kl_loss: 25.4037\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 9000/900, test 20/100], loss: 92.3525, rmse_loss: 66.4964, kl_loss: 25.8561\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 9000/900, test 30/100], loss: 74.6283, rmse_loss: 50.1398, kl_loss: 24.4884\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 9000/900, test 40/100], loss: 55.1290, rmse_loss: 32.7286, kl_loss: 22.4004\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 9000/900, test 50/100], loss: 79.1768, rmse_loss: 57.0480, kl_loss: 22.1288\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 9000/900, test 60/100], loss: 81.6320, rmse_loss: 58.0022, kl_loss: 23.6298\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 9000/900, test 70/100], loss: 80.6350, rmse_loss: 55.7638, kl_loss: 24.8712\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 9000/900, test 80/100], loss: 90.6520, rmse_loss: 64.5282, kl_loss: 26.1238\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 9000/900, test 90/100], loss: 81.1698, rmse_loss: 55.5551, kl_loss: 25.6147\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 7/10][train 9000/900, test 100/100], loss: 82.2472, rmse_loss: 55.4371, kl_loss: 26.8101\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 10/900, test 0/100], loss: 74.7377, rmse_loss: 51.6700, kl_loss: 23.0677\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 20/900, test 0/100], loss: 84.5249, rmse_loss: 61.2089, kl_loss: 23.3160\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 30/900, test 0/100], loss: 72.7118, rmse_loss: 48.7694, kl_loss: 23.9424\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 40/900, test 0/100], loss: 83.6057, rmse_loss: 57.9835, kl_loss: 25.6222\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 50/900, test 0/100], loss: 81.4278, rmse_loss: 57.3414, kl_loss: 24.0864\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 60/900, test 0/100], loss: 87.8668, rmse_loss: 62.5949, kl_loss: 25.2719\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 70/900, test 0/100], loss: 89.9076, rmse_loss: 65.6780, kl_loss: 24.2295\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 80/900, test 0/100], loss: 76.7600, rmse_loss: 52.4068, kl_loss: 24.3532\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 90/900, test 0/100], loss: 97.6130, rmse_loss: 71.6855, kl_loss: 25.9275\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 100/900, test 0/100], loss: 106.7644, rmse_loss: 82.1340, kl_loss: 24.6304\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 110/900, test 0/100], loss: 81.9192, rmse_loss: 56.8523, kl_loss: 25.0669\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 120/900, test 0/100], loss: 92.4085, rmse_loss: 66.0312, kl_loss: 26.3773\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 130/900, test 0/100], loss: 124.5350, rmse_loss: 99.0190, kl_loss: 25.5159\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 140/900, test 0/100], loss: 73.7701, rmse_loss: 52.0014, kl_loss: 21.7687\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 150/900, test 0/100], loss: 80.9623, rmse_loss: 54.4406, kl_loss: 26.5217\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 160/900, test 0/100], loss: 70.5306, rmse_loss: 46.5006, kl_loss: 24.0300\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 170/900, test 0/100], loss: 68.1961, rmse_loss: 45.9682, kl_loss: 22.2279\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 180/900, test 0/100], loss: 72.4203, rmse_loss: 49.8377, kl_loss: 22.5827\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 190/900, test 0/100], loss: 93.0953, rmse_loss: 68.6457, kl_loss: 24.4496\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 200/900, test 0/100], loss: 80.3668, rmse_loss: 57.2327, kl_loss: 23.1341\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 210/900, test 0/100], loss: 94.2692, rmse_loss: 70.2219, kl_loss: 24.0474\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 220/900, test 0/100], loss: 96.1165, rmse_loss: 72.4545, kl_loss: 23.6620\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 230/900, test 0/100], loss: 82.2857, rmse_loss: 57.8773, kl_loss: 24.4085\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 240/900, test 0/100], loss: 78.6461, rmse_loss: 53.7358, kl_loss: 24.9104\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 250/900, test 0/100], loss: 77.3753, rmse_loss: 51.8901, kl_loss: 25.4852\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 260/900, test 0/100], loss: 91.5537, rmse_loss: 66.2006, kl_loss: 25.3530\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 270/900, test 0/100], loss: 87.8558, rmse_loss: 62.4105, kl_loss: 25.4453\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 280/900, test 0/100], loss: 77.2461, rmse_loss: 52.7814, kl_loss: 24.4647\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 290/900, test 0/100], loss: 80.5046, rmse_loss: 56.5878, kl_loss: 23.9168\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 300/900, test 0/100], loss: 106.2528, rmse_loss: 79.2866, kl_loss: 26.9663\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 310/900, test 0/100], loss: 93.8515, rmse_loss: 68.7044, kl_loss: 25.1471\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 320/900, test 0/100], loss: 81.8311, rmse_loss: 57.9980, kl_loss: 23.8331\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 330/900, test 0/100], loss: 78.9789, rmse_loss: 55.8515, kl_loss: 23.1274\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 340/900, test 0/100], loss: 84.7626, rmse_loss: 59.5242, kl_loss: 25.2384\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 350/900, test 0/100], loss: 75.2509, rmse_loss: 53.6225, kl_loss: 21.6284\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 360/900, test 0/100], loss: 86.4639, rmse_loss: 62.4632, kl_loss: 24.0007\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 370/900, test 0/100], loss: 76.2517, rmse_loss: 53.0674, kl_loss: 23.1843\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 380/900, test 0/100], loss: 61.6363, rmse_loss: 39.4560, kl_loss: 22.1803\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 390/900, test 0/100], loss: 88.2908, rmse_loss: 63.4083, kl_loss: 24.8825\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 400/900, test 0/100], loss: 76.8585, rmse_loss: 53.3605, kl_loss: 23.4980\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 410/900, test 0/100], loss: 83.4438, rmse_loss: 58.7103, kl_loss: 24.7335\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 420/900, test 0/100], loss: 93.5337, rmse_loss: 68.3705, kl_loss: 25.1633\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 430/900, test 0/100], loss: 77.6091, rmse_loss: 52.2423, kl_loss: 25.3668\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 440/900, test 0/100], loss: 109.2040, rmse_loss: 81.9379, kl_loss: 27.2661\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 450/900, test 0/100], loss: 93.4515, rmse_loss: 68.0203, kl_loss: 25.4312\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 460/900, test 0/100], loss: 100.4677, rmse_loss: 74.9736, kl_loss: 25.4942\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 470/900, test 0/100], loss: 100.7766, rmse_loss: 76.7816, kl_loss: 23.9950\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 480/900, test 0/100], loss: 84.1262, rmse_loss: 60.8855, kl_loss: 23.2407\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 490/900, test 0/100], loss: 103.2301, rmse_loss: 76.8847, kl_loss: 26.3454\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 500/900, test 0/100], loss: 78.7207, rmse_loss: 53.6277, kl_loss: 25.0930\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 510/900, test 0/100], loss: 73.3595, rmse_loss: 49.8803, kl_loss: 23.4792\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 520/900, test 0/100], loss: 79.4893, rmse_loss: 57.2484, kl_loss: 22.2409\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 530/900, test 0/100], loss: 81.5022, rmse_loss: 56.0093, kl_loss: 25.4929\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 540/900, test 0/100], loss: 84.7300, rmse_loss: 59.9429, kl_loss: 24.7872\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 550/900, test 0/100], loss: 91.7586, rmse_loss: 65.7607, kl_loss: 25.9980\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 560/900, test 0/100], loss: 72.4862, rmse_loss: 49.8309, kl_loss: 22.6553\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 570/900, test 0/100], loss: 74.4298, rmse_loss: 51.6606, kl_loss: 22.7692\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 580/900, test 0/100], loss: 92.2449, rmse_loss: 66.3732, kl_loss: 25.8717\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 590/900, test 0/100], loss: 90.4149, rmse_loss: 65.1722, kl_loss: 25.2427\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 600/900, test 0/100], loss: 105.8364, rmse_loss: 79.6437, kl_loss: 26.1927\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 610/900, test 0/100], loss: 92.1388, rmse_loss: 66.3644, kl_loss: 25.7745\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 620/900, test 0/100], loss: 72.0813, rmse_loss: 48.3798, kl_loss: 23.7014\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 630/900, test 0/100], loss: 70.3299, rmse_loss: 44.9443, kl_loss: 25.3856\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 640/900, test 0/100], loss: 82.5535, rmse_loss: 58.0916, kl_loss: 24.4618\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 650/900, test 0/100], loss: 99.8369, rmse_loss: 74.8928, kl_loss: 24.9441\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 660/900, test 0/100], loss: 95.0637, rmse_loss: 68.2303, kl_loss: 26.8333\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 670/900, test 0/100], loss: 66.9959, rmse_loss: 45.1413, kl_loss: 21.8546\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 680/900, test 0/100], loss: 100.7555, rmse_loss: 74.5890, kl_loss: 26.1665\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 690/900, test 0/100], loss: 93.2228, rmse_loss: 67.0715, kl_loss: 26.1513\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 700/900, test 0/100], loss: 62.1256, rmse_loss: 39.5374, kl_loss: 22.5882\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 710/900, test 0/100], loss: 71.5743, rmse_loss: 47.6354, kl_loss: 23.9389\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 720/900, test 0/100], loss: 81.7431, rmse_loss: 57.0061, kl_loss: 24.7371\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 730/900, test 0/100], loss: 67.3773, rmse_loss: 45.5498, kl_loss: 21.8275\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 740/900, test 0/100], loss: 82.9309, rmse_loss: 55.4018, kl_loss: 27.5291\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 750/900, test 0/100], loss: 84.2404, rmse_loss: 60.3052, kl_loss: 23.9352\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 760/900, test 0/100], loss: 72.3643, rmse_loss: 49.2741, kl_loss: 23.0902\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 770/900, test 0/100], loss: 75.7898, rmse_loss: 52.4333, kl_loss: 23.3566\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 780/900, test 0/100], loss: 71.1466, rmse_loss: 49.0496, kl_loss: 22.0970\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 790/900, test 0/100], loss: 79.6786, rmse_loss: 54.2021, kl_loss: 25.4766\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 800/900, test 0/100], loss: 64.9950, rmse_loss: 41.9313, kl_loss: 23.0637\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 810/900, test 0/100], loss: 77.8656, rmse_loss: 54.5331, kl_loss: 23.3326\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 820/900, test 0/100], loss: 84.8265, rmse_loss: 58.5619, kl_loss: 26.2645\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 830/900, test 0/100], loss: 95.2004, rmse_loss: 68.6249, kl_loss: 26.5755\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 840/900, test 0/100], loss: 68.6616, rmse_loss: 46.9671, kl_loss: 21.6945\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 850/900, test 0/100], loss: 90.1537, rmse_loss: 65.1449, kl_loss: 25.0087\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 860/900, test 0/100], loss: 72.8146, rmse_loss: 48.5224, kl_loss: 24.2922\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 870/900, test 0/100], loss: 95.0257, rmse_loss: 68.5096, kl_loss: 26.5161\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 880/900, test 0/100], loss: 80.1785, rmse_loss: 55.5599, kl_loss: 24.6186\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 890/900, test 0/100], loss: 75.9822, rmse_loss: 51.9138, kl_loss: 24.0684\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 900/900, test 0/100], loss: 82.9733, rmse_loss: 57.7817, kl_loss: 25.1917\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 8\n",
      "test: epoch 8\n",
      "[epoch 8/10][train 9000/900, test 10/100], loss: 94.4826, rmse_loss: 68.0078, kl_loss: 26.4748\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 9000/900, test 20/100], loss: 100.2238, rmse_loss: 72.8291, kl_loss: 27.3947\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 9000/900, test 30/100], loss: 77.1851, rmse_loss: 52.3840, kl_loss: 24.8012\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 9000/900, test 40/100], loss: 59.2642, rmse_loss: 36.7579, kl_loss: 22.5063\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 9000/900, test 50/100], loss: 89.3802, rmse_loss: 66.3538, kl_loss: 23.0264\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 9000/900, test 60/100], loss: 84.6810, rmse_loss: 60.2170, kl_loss: 24.4639\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 9000/900, test 70/100], loss: 86.6554, rmse_loss: 60.5931, kl_loss: 26.0622\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 9000/900, test 80/100], loss: 94.3106, rmse_loss: 67.6117, kl_loss: 26.6989\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 9000/900, test 90/100], loss: 90.8337, rmse_loss: 64.1789, kl_loss: 26.6547\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 8/10][train 9000/900, test 100/100], loss: 87.2888, rmse_loss: 59.3689, kl_loss: 27.9200\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 10/900, test 0/100], loss: 90.5923, rmse_loss: 66.9295, kl_loss: 23.6628\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 20/900, test 0/100], loss: 116.5824, rmse_loss: 90.8190, kl_loss: 25.7633\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 30/900, test 0/100], loss: 84.2137, rmse_loss: 57.9424, kl_loss: 26.2712\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 40/900, test 0/100], loss: 115.9771, rmse_loss: 90.0293, kl_loss: 25.9478\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 50/900, test 0/100], loss: 74.6155, rmse_loss: 52.7167, kl_loss: 21.8988\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 60/900, test 0/100], loss: 84.5404, rmse_loss: 58.7420, kl_loss: 25.7984\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 70/900, test 0/100], loss: 78.1190, rmse_loss: 55.2462, kl_loss: 22.8728\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 80/900, test 0/100], loss: 79.7396, rmse_loss: 55.7657, kl_loss: 23.9739\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 90/900, test 0/100], loss: 79.2900, rmse_loss: 56.1267, kl_loss: 23.1633\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 100/900, test 0/100], loss: 92.5899, rmse_loss: 66.2412, kl_loss: 26.3487\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 110/900, test 0/100], loss: 87.5758, rmse_loss: 63.2502, kl_loss: 24.3256\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 120/900, test 0/100], loss: 86.8618, rmse_loss: 62.9686, kl_loss: 23.8933\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 130/900, test 0/100], loss: 80.5298, rmse_loss: 56.4951, kl_loss: 24.0347\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 140/900, test 0/100], loss: 102.7845, rmse_loss: 73.9412, kl_loss: 28.8432\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 150/900, test 0/100], loss: 77.8885, rmse_loss: 53.7404, kl_loss: 24.1481\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 160/900, test 0/100], loss: 72.2694, rmse_loss: 48.5874, kl_loss: 23.6820\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 170/900, test 0/100], loss: 87.6170, rmse_loss: 63.2880, kl_loss: 24.3290\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 180/900, test 0/100], loss: 74.0425, rmse_loss: 51.3961, kl_loss: 22.6465\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 190/900, test 0/100], loss: 86.2957, rmse_loss: 61.5592, kl_loss: 24.7364\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 200/900, test 0/100], loss: 91.7217, rmse_loss: 67.0776, kl_loss: 24.6441\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 210/900, test 0/100], loss: 72.3554, rmse_loss: 48.3128, kl_loss: 24.0426\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 220/900, test 0/100], loss: 107.7478, rmse_loss: 85.8605, kl_loss: 21.8873\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 230/900, test 0/100], loss: 84.8153, rmse_loss: 57.1222, kl_loss: 27.6932\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 240/900, test 0/100], loss: 57.5789, rmse_loss: 36.4225, kl_loss: 21.1564\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 250/900, test 0/100], loss: 79.3280, rmse_loss: 54.7499, kl_loss: 24.5781\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 260/900, test 0/100], loss: 69.5342, rmse_loss: 44.9263, kl_loss: 24.6080\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 270/900, test 0/100], loss: 82.7061, rmse_loss: 59.4976, kl_loss: 23.2086\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 280/900, test 0/100], loss: 99.5121, rmse_loss: 74.4515, kl_loss: 25.0606\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 290/900, test 0/100], loss: 81.5154, rmse_loss: 54.6242, kl_loss: 26.8912\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 300/900, test 0/100], loss: 67.8490, rmse_loss: 45.3121, kl_loss: 22.5368\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 310/900, test 0/100], loss: 83.4488, rmse_loss: 58.2764, kl_loss: 25.1724\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 320/900, test 0/100], loss: 89.2273, rmse_loss: 62.1644, kl_loss: 27.0630\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 330/900, test 0/100], loss: 77.6126, rmse_loss: 53.2578, kl_loss: 24.3549\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 340/900, test 0/100], loss: 73.9241, rmse_loss: 51.0258, kl_loss: 22.8983\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 350/900, test 0/100], loss: 80.9551, rmse_loss: 54.7201, kl_loss: 26.2350\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 360/900, test 0/100], loss: 71.8496, rmse_loss: 47.7475, kl_loss: 24.1020\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 370/900, test 0/100], loss: 67.8177, rmse_loss: 44.1746, kl_loss: 23.6431\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 380/900, test 0/100], loss: 65.8006, rmse_loss: 42.2048, kl_loss: 23.5958\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 390/900, test 0/100], loss: 94.9680, rmse_loss: 69.8729, kl_loss: 25.0952\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 400/900, test 0/100], loss: 71.9974, rmse_loss: 47.6957, kl_loss: 24.3017\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 410/900, test 0/100], loss: 66.9173, rmse_loss: 44.1570, kl_loss: 22.7603\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 420/900, test 0/100], loss: 77.1703, rmse_loss: 52.4877, kl_loss: 24.6826\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 430/900, test 0/100], loss: 86.9194, rmse_loss: 60.6556, kl_loss: 26.2638\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 440/900, test 0/100], loss: 71.3847, rmse_loss: 47.8849, kl_loss: 23.4998\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 450/900, test 0/100], loss: 66.2609, rmse_loss: 41.3982, kl_loss: 24.8627\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 460/900, test 0/100], loss: 81.6831, rmse_loss: 56.6794, kl_loss: 25.0037\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 470/900, test 0/100], loss: 90.9372, rmse_loss: 64.1753, kl_loss: 26.7619\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 480/900, test 0/100], loss: 79.5364, rmse_loss: 54.7408, kl_loss: 24.7956\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 490/900, test 0/100], loss: 101.8051, rmse_loss: 76.4883, kl_loss: 25.3169\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 500/900, test 0/100], loss: 68.2046, rmse_loss: 45.4471, kl_loss: 22.7576\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 510/900, test 0/100], loss: 61.8525, rmse_loss: 39.4796, kl_loss: 22.3730\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 520/900, test 0/100], loss: 128.5851, rmse_loss: 103.4187, kl_loss: 25.1664\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 530/900, test 0/100], loss: 79.3931, rmse_loss: 55.1931, kl_loss: 24.2000\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 540/900, test 0/100], loss: 76.7179, rmse_loss: 52.0675, kl_loss: 24.6504\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 550/900, test 0/100], loss: 78.0113, rmse_loss: 52.9816, kl_loss: 25.0296\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 560/900, test 0/100], loss: 68.5759, rmse_loss: 45.1903, kl_loss: 23.3856\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 570/900, test 0/100], loss: 95.2646, rmse_loss: 68.7817, kl_loss: 26.4829\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 580/900, test 0/100], loss: 96.7182, rmse_loss: 70.9859, kl_loss: 25.7323\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 590/900, test 0/100], loss: 71.8336, rmse_loss: 47.4341, kl_loss: 24.3994\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 600/900, test 0/100], loss: 89.6606, rmse_loss: 65.6143, kl_loss: 24.0463\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 610/900, test 0/100], loss: 84.2344, rmse_loss: 57.5716, kl_loss: 26.6628\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 620/900, test 0/100], loss: 67.2286, rmse_loss: 45.8464, kl_loss: 21.3822\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 630/900, test 0/100], loss: 83.9192, rmse_loss: 59.3336, kl_loss: 24.5856\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 640/900, test 0/100], loss: 84.3720, rmse_loss: 56.6184, kl_loss: 27.7536\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 650/900, test 0/100], loss: 89.4000, rmse_loss: 63.2772, kl_loss: 26.1229\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 660/900, test 0/100], loss: 86.3583, rmse_loss: 59.8997, kl_loss: 26.4586\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 670/900, test 0/100], loss: 73.8459, rmse_loss: 50.2196, kl_loss: 23.6262\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 680/900, test 0/100], loss: 64.9501, rmse_loss: 42.3230, kl_loss: 22.6271\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 690/900, test 0/100], loss: 78.1432, rmse_loss: 53.1502, kl_loss: 24.9930\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 700/900, test 0/100], loss: 71.2735, rmse_loss: 48.0232, kl_loss: 23.2503\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 710/900, test 0/100], loss: 67.3030, rmse_loss: 43.8461, kl_loss: 23.4569\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 720/900, test 0/100], loss: 85.8406, rmse_loss: 57.6036, kl_loss: 28.2369\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 730/900, test 0/100], loss: 68.9111, rmse_loss: 47.2639, kl_loss: 21.6472\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 740/900, test 0/100], loss: 148.4950, rmse_loss: 122.4661, kl_loss: 26.0288\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 750/900, test 0/100], loss: 88.0724, rmse_loss: 61.5738, kl_loss: 26.4987\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 760/900, test 0/100], loss: 126.6714, rmse_loss: 96.7158, kl_loss: 29.9556\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 770/900, test 0/100], loss: 71.0820, rmse_loss: 46.6918, kl_loss: 24.3902\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 780/900, test 0/100], loss: 66.0088, rmse_loss: 43.6566, kl_loss: 22.3522\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 790/900, test 0/100], loss: 94.3867, rmse_loss: 68.7721, kl_loss: 25.6146\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 800/900, test 0/100], loss: 74.9711, rmse_loss: 51.4159, kl_loss: 23.5552\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 810/900, test 0/100], loss: 75.5736, rmse_loss: 50.6635, kl_loss: 24.9101\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 820/900, test 0/100], loss: 94.7801, rmse_loss: 68.6749, kl_loss: 26.1051\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 830/900, test 0/100], loss: 73.1413, rmse_loss: 48.5197, kl_loss: 24.6215\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 840/900, test 0/100], loss: 67.7848, rmse_loss: 45.2234, kl_loss: 22.5615\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 850/900, test 0/100], loss: 97.0732, rmse_loss: 68.4709, kl_loss: 28.6022\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 860/900, test 0/100], loss: 87.2279, rmse_loss: 61.0433, kl_loss: 26.1846\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 870/900, test 0/100], loss: 77.4870, rmse_loss: 53.1613, kl_loss: 24.3257\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 880/900, test 0/100], loss: 72.9352, rmse_loss: 46.9321, kl_loss: 26.0031\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 890/900, test 0/100], loss: 58.9981, rmse_loss: 35.6333, kl_loss: 23.3648\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 900/900, test 0/100], loss: 78.2859, rmse_loss: 53.7439, kl_loss: 24.5420\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 9\n",
      "test: epoch 9\n",
      "[epoch 9/10][train 9000/900, test 10/100], loss: 78.9575, rmse_loss: 52.9057, kl_loss: 26.0518\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 9000/900, test 20/100], loss: 85.9529, rmse_loss: 59.9411, kl_loss: 26.0118\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 9000/900, test 30/100], loss: 70.4783, rmse_loss: 46.4159, kl_loss: 24.0624\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 9000/900, test 40/100], loss: 52.5586, rmse_loss: 30.4904, kl_loss: 22.0682\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 9000/900, test 50/100], loss: 76.6234, rmse_loss: 54.7641, kl_loss: 21.8594\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 9000/900, test 60/100], loss: 76.4255, rmse_loss: 53.1198, kl_loss: 23.3057\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 9000/900, test 70/100], loss: 76.0229, rmse_loss: 51.1333, kl_loss: 24.8896\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 9000/900, test 80/100], loss: 84.2639, rmse_loss: 58.3638, kl_loss: 25.9001\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 9000/900, test 90/100], loss: 76.3227, rmse_loss: 50.9567, kl_loss: 25.3660\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 9/10][train 9000/900, test 100/100], loss: 75.2753, rmse_loss: 49.2915, kl_loss: 25.9838\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 10/900, test 0/100], loss: 103.1022, rmse_loss: 75.8684, kl_loss: 27.2339\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 20/900, test 0/100], loss: 73.6849, rmse_loss: 49.4333, kl_loss: 24.2516\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 30/900, test 0/100], loss: 63.8625, rmse_loss: 38.0991, kl_loss: 25.7635\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 40/900, test 0/100], loss: 78.3896, rmse_loss: 54.1506, kl_loss: 24.2390\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 50/900, test 0/100], loss: 86.7417, rmse_loss: 62.3306, kl_loss: 24.4111\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 60/900, test 0/100], loss: 74.6477, rmse_loss: 50.3502, kl_loss: 24.2975\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 70/900, test 0/100], loss: 75.4551, rmse_loss: 49.8391, kl_loss: 25.6160\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 80/900, test 0/100], loss: 86.6896, rmse_loss: 60.7613, kl_loss: 25.9283\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 90/900, test 0/100], loss: 65.1864, rmse_loss: 43.6031, kl_loss: 21.5833\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 100/900, test 0/100], loss: 82.4012, rmse_loss: 58.2203, kl_loss: 24.1810\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 110/900, test 0/100], loss: 70.3920, rmse_loss: 46.3146, kl_loss: 24.0773\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 120/900, test 0/100], loss: 90.0224, rmse_loss: 64.1301, kl_loss: 25.8924\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 130/900, test 0/100], loss: 85.2586, rmse_loss: 57.9159, kl_loss: 27.3427\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 140/900, test 0/100], loss: 86.3068, rmse_loss: 61.8948, kl_loss: 24.4120\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 150/900, test 0/100], loss: 59.7535, rmse_loss: 37.2634, kl_loss: 22.4901\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 160/900, test 0/100], loss: 75.5584, rmse_loss: 52.7428, kl_loss: 22.8155\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 170/900, test 0/100], loss: 69.9098, rmse_loss: 46.5945, kl_loss: 23.3154\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 180/900, test 0/100], loss: 104.0058, rmse_loss: 78.6606, kl_loss: 25.3452\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 190/900, test 0/100], loss: 63.5162, rmse_loss: 39.5724, kl_loss: 23.9438\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 200/900, test 0/100], loss: 91.5505, rmse_loss: 68.3324, kl_loss: 23.2181\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 210/900, test 0/100], loss: 89.4352, rmse_loss: 64.1300, kl_loss: 25.3052\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 220/900, test 0/100], loss: 102.7403, rmse_loss: 76.7992, kl_loss: 25.9411\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 230/900, test 0/100], loss: 86.3313, rmse_loss: 60.6074, kl_loss: 25.7239\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 240/900, test 0/100], loss: 69.3704, rmse_loss: 43.8420, kl_loss: 25.5285\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 250/900, test 0/100], loss: 97.4861, rmse_loss: 71.7226, kl_loss: 25.7635\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 260/900, test 0/100], loss: 86.1122, rmse_loss: 59.3074, kl_loss: 26.8048\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 270/900, test 0/100], loss: 79.7908, rmse_loss: 53.4052, kl_loss: 26.3856\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 280/900, test 0/100], loss: 80.4794, rmse_loss: 55.6691, kl_loss: 24.8103\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 290/900, test 0/100], loss: 68.4077, rmse_loss: 44.8956, kl_loss: 23.5120\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 300/900, test 0/100], loss: 88.8891, rmse_loss: 63.4754, kl_loss: 25.4137\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 310/900, test 0/100], loss: 87.4360, rmse_loss: 61.8771, kl_loss: 25.5590\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 320/900, test 0/100], loss: 71.8400, rmse_loss: 48.4375, kl_loss: 23.4025\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 330/900, test 0/100], loss: 71.8557, rmse_loss: 45.6712, kl_loss: 26.1845\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 340/900, test 0/100], loss: 67.9359, rmse_loss: 43.8276, kl_loss: 24.1083\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 350/900, test 0/100], loss: 71.5734, rmse_loss: 48.4542, kl_loss: 23.1192\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 360/900, test 0/100], loss: 71.1880, rmse_loss: 45.2749, kl_loss: 25.9131\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 370/900, test 0/100], loss: 72.6263, rmse_loss: 47.6611, kl_loss: 24.9652\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 380/900, test 0/100], loss: 76.5468, rmse_loss: 52.7142, kl_loss: 23.8326\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 390/900, test 0/100], loss: 90.0207, rmse_loss: 63.7760, kl_loss: 26.2447\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 400/900, test 0/100], loss: 74.4030, rmse_loss: 49.7207, kl_loss: 24.6823\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 410/900, test 0/100], loss: 69.2521, rmse_loss: 45.0018, kl_loss: 24.2503\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 420/900, test 0/100], loss: 76.2715, rmse_loss: 51.6886, kl_loss: 24.5829\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 430/900, test 0/100], loss: 103.0695, rmse_loss: 77.7076, kl_loss: 25.3620\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 440/900, test 0/100], loss: 68.4741, rmse_loss: 45.8684, kl_loss: 22.6057\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 450/900, test 0/100], loss: 78.3486, rmse_loss: 52.6792, kl_loss: 25.6694\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 460/900, test 0/100], loss: 79.0303, rmse_loss: 51.7267, kl_loss: 27.3037\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 470/900, test 0/100], loss: 66.3351, rmse_loss: 43.7660, kl_loss: 22.5691\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 480/900, test 0/100], loss: 73.3288, rmse_loss: 48.8566, kl_loss: 24.4722\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 490/900, test 0/100], loss: 86.5699, rmse_loss: 59.6549, kl_loss: 26.9150\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 500/900, test 0/100], loss: 76.0857, rmse_loss: 50.8033, kl_loss: 25.2823\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 510/900, test 0/100], loss: 63.2208, rmse_loss: 39.4776, kl_loss: 23.7432\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 520/900, test 0/100], loss: 84.2154, rmse_loss: 58.1798, kl_loss: 26.0356\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 530/900, test 0/100], loss: 61.5127, rmse_loss: 37.7299, kl_loss: 23.7828\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 540/900, test 0/100], loss: 78.5521, rmse_loss: 53.8624, kl_loss: 24.6897\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 550/900, test 0/100], loss: 77.3678, rmse_loss: 51.9174, kl_loss: 25.4504\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 560/900, test 0/100], loss: 87.4868, rmse_loss: 62.1976, kl_loss: 25.2892\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 570/900, test 0/100], loss: 100.4882, rmse_loss: 75.1649, kl_loss: 25.3233\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 580/900, test 0/100], loss: 66.2092, rmse_loss: 41.9900, kl_loss: 24.2193\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 590/900, test 0/100], loss: 89.2599, rmse_loss: 65.6173, kl_loss: 23.6426\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 600/900, test 0/100], loss: 70.8237, rmse_loss: 46.0273, kl_loss: 24.7964\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 610/900, test 0/100], loss: 86.8407, rmse_loss: 59.4262, kl_loss: 27.4145\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 620/900, test 0/100], loss: 76.9695, rmse_loss: 53.3366, kl_loss: 23.6329\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 630/900, test 0/100], loss: 82.6237, rmse_loss: 58.3291, kl_loss: 24.2946\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 640/900, test 0/100], loss: 91.7427, rmse_loss: 66.7073, kl_loss: 25.0354\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 650/900, test 0/100], loss: 79.7955, rmse_loss: 56.0578, kl_loss: 23.7377\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 660/900, test 0/100], loss: 86.4637, rmse_loss: 58.0545, kl_loss: 28.4092\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 670/900, test 0/100], loss: 95.9362, rmse_loss: 70.4958, kl_loss: 25.4404\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 680/900, test 0/100], loss: 79.8186, rmse_loss: 55.6255, kl_loss: 24.1931\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 690/900, test 0/100], loss: 88.1130, rmse_loss: 63.5140, kl_loss: 24.5990\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 700/900, test 0/100], loss: 75.9793, rmse_loss: 51.8507, kl_loss: 24.1287\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 710/900, test 0/100], loss: 82.2090, rmse_loss: 56.6564, kl_loss: 25.5526\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 720/900, test 0/100], loss: 68.2820, rmse_loss: 44.6864, kl_loss: 23.5956\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 730/900, test 0/100], loss: 71.2216, rmse_loss: 46.4995, kl_loss: 24.7221\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 740/900, test 0/100], loss: 74.1668, rmse_loss: 49.6455, kl_loss: 24.5213\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 750/900, test 0/100], loss: 62.8831, rmse_loss: 39.0205, kl_loss: 23.8626\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 760/900, test 0/100], loss: 72.0671, rmse_loss: 46.4095, kl_loss: 25.6576\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 770/900, test 0/100], loss: 78.3149, rmse_loss: 54.1088, kl_loss: 24.2061\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 780/900, test 0/100], loss: 74.9867, rmse_loss: 50.6825, kl_loss: 24.3042\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 790/900, test 0/100], loss: 107.5064, rmse_loss: 80.1229, kl_loss: 27.3835\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 800/900, test 0/100], loss: 64.5911, rmse_loss: 41.0919, kl_loss: 23.4992\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 810/900, test 0/100], loss: 76.8887, rmse_loss: 49.9995, kl_loss: 26.8892\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 820/900, test 0/100], loss: 82.4711, rmse_loss: 58.9575, kl_loss: 23.5136\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 830/900, test 0/100], loss: 88.2645, rmse_loss: 63.1088, kl_loss: 25.1557\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 840/900, test 0/100], loss: 95.0093, rmse_loss: 69.2507, kl_loss: 25.7586\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 850/900, test 0/100], loss: 71.6738, rmse_loss: 47.2605, kl_loss: 24.4133\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 860/900, test 0/100], loss: 76.5571, rmse_loss: 52.0016, kl_loss: 24.5555\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 870/900, test 0/100], loss: 88.5379, rmse_loss: 62.7897, kl_loss: 25.7482\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 880/900, test 0/100], loss: 75.0971, rmse_loss: 49.6159, kl_loss: 25.4812\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 890/900, test 0/100], loss: 104.2604, rmse_loss: 74.5333, kl_loss: 29.7271\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 900/900, test 0/100], loss: 65.0697, rmse_loss: 43.3356, kl_loss: 21.7341\n",
      "Use Memory: 364.60 MB\n",
      "save model: epoch 10\n",
      "test: epoch 10\n",
      "[epoch 10/10][train 9000/900, test 10/100], loss: 79.9145, rmse_loss: 53.5073, kl_loss: 26.4072\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 9000/900, test 20/100], loss: 89.6237, rmse_loss: 62.8974, kl_loss: 26.7263\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 9000/900, test 30/100], loss: 71.2870, rmse_loss: 47.0858, kl_loss: 24.2011\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 9000/900, test 40/100], loss: 57.2261, rmse_loss: 34.9346, kl_loss: 22.2915\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 9000/900, test 50/100], loss: 77.5163, rmse_loss: 55.0851, kl_loss: 22.4312\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 9000/900, test 60/100], loss: 80.8725, rmse_loss: 57.1396, kl_loss: 23.7329\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 9000/900, test 70/100], loss: 79.7230, rmse_loss: 54.1053, kl_loss: 25.6177\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 9000/900, test 80/100], loss: 85.5319, rmse_loss: 58.8149, kl_loss: 26.7169\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 9000/900, test 90/100], loss: 78.5971, rmse_loss: 52.0159, kl_loss: 26.5812\n",
      "Use Memory: 364.60 MB\n",
      "[epoch 10/10][train 9000/900, test 100/100], loss: 80.9531, rmse_loss: 53.5103, kl_loss: 27.4428\n",
      "Use Memory: 364.60 MB\n"
     ]
    }
   ],
   "source": [
    "if checkpoint_epoch + 1 == num_epochs:\n",
    "    print('already trained')\n",
    "else:\n",
    "    for epoch in range(checkpoint_epoch+1, num_epochs):\n",
    "        dataset_idx = np.array([id for id in range(9000)]).astype(int)\n",
    "        np.random.shuffle(dataset_idx)\n",
    "        for i in range(9000 // batch_size):\n",
    "            dataset = make_dataset_vae(batch_size, m, dataset_idx[i*batch_size:(i+1)*batch_size])\n",
    "            dataset = torch.tensor(dataset).float().to(device)\n",
    "\n",
    "            vae.train()\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = vae(dataset)\n",
    "            loss, r_loss, kl_loss = loss_function(recon_batch, dataset, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print('[epoch {}/{}][train {}/{}, test {}/{}], loss: {:.4f}, rmse_loss: {:.4f}, kl_loss: {:.4f}'.format(epoch + 1, num_epochs, i+1, 9000 // batch_size, 0, 1000 // batch_size, loss.item(), r_loss.item(), kl_loss.item()))\n",
    "                print(\"Use Memory: {:.2f} MB\".format(torch.cuda.memory_allocated() / 1024 / 1024))\n",
    "\n",
    "            # GPU memory: remove data\n",
    "            del dataset\n",
    "\n",
    "            if i != 9000 // batch_size - 1:\n",
    "                del loss\n",
    "                del r_loss\n",
    "                del kl_loss\n",
    "        \n",
    "        print(\"save model: epoch {}\".format(epoch + 1))\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vae.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'vae.pth')\n",
    "        \n",
    "        del loss\n",
    "        del r_loss\n",
    "        del kl_loss\n",
    "        \n",
    "        vae.eval()\n",
    "        print(\"test: epoch {}\".format(epoch + 1))\n",
    "        with torch.no_grad():\n",
    "            test_idx = np.array([i for i in range(9000, 10000)]).astype(int)\n",
    "            for i in range(1000 // batch_size):\n",
    "                dataset = make_dataset_vae(batch_size, m, test_idx[i*batch_size:(i+1)*batch_size])\n",
    "                dataset = torch.tensor(dataset).float().to(device)\n",
    "\n",
    "                recon_batch, mu, logvar = vae(dataset)\n",
    "                loss, r_loss, kl_loss = loss_function(recon_batch, dataset, mu, logvar)\n",
    "\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print('[epoch {}/{}][train {}/{}, test {}/{}], loss: {:.4f}, rmse_loss: {:.4f}, kl_loss: {:.4f}'.format(epoch + 1, num_epochs, 9000, 9000 // batch_size, i+1, 1000 // batch_size, loss.item(), r_loss.item(), kl_loss.item()))\n",
    "                    print(\"Use Memory: {:.2f} MB\".format(torch.cuda.memory_allocated() / 1024 / 1024))\n",
    "\n",
    "                # GPU memory: remove data\n",
    "                del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (mu): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    (logvar): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (l1): Linear(in_features=32, out_features=1024, bias=True)\n",
       "    (deconv1): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (deconv2): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (deconv3): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))\n",
       "    (deconv4): ConvTranspose2d(32, 3, kernel_size=(6, 6), stride=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "checkpoint = torch.load('vae.pth')\n",
    "vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "vae.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4242465150122549"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "rollouts = rollout.CarRacing_rollouts()\n",
    "\n",
    "states, _, _, _ = rollouts.load_rollout(90)\n",
    "t100 = states[100]\n",
    "np.mean(t100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = rollout.CarRacing_rollouts()\n",
    "\n",
    "states, _, _, _ = rollouts.load_rollout(10)\n",
    "t100 = states[150]\n",
    "t100_255 = t100 * 255.0\n",
    "pil_image = Image.fromarray(t100_255.astype(\"uint8\"))\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t100_vae = torch.tensor(t100).float().to(device)\n",
    "t100_vae = t100_vae.permute(2, 0, 1).unsqueeze(0)\n",
    "t100_vae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t100_vae = vae(t100_vae)[0].squeeze(0).permute(1, 2, 0).cpu().detach().numpy()\n",
    "t100_vae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_vae_255 = t100_vae * 255.0\n",
    "pil_image = Image.fromarray(t100_vae_255.astype(\"uint8\"))\n",
    "pil_image.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## im to z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.VAE import VAE\n",
    "import rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [22:43<00:00,  7.34it/s]\n"
     ]
    }
   ],
   "source": [
    "cr = rollout.CarRacing_rollouts()\n",
    "cr.rollout_to_z()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.MDN_RNN import MDNRNN, loss_func\n",
    "import rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_rnn(n, m, idx, use_reward=False, use_Done=False):\n",
    "    cr = rollout.CarRacing_rollouts()\n",
    "    Z = np.zeros((n, m, 32)) # (batch, seq, dim)\n",
    "    A = np.zeros((n, m, 3)) # (batch, seq, dim)\n",
    "    Reward = np.zeros((n, m, 1)) # (batch, seq, dim)\n",
    "    Done = np.zeros((n, m, 1)) # (batch, seq, dim)\n",
    "    for i, j in enumerate(idx):\n",
    "        z, _, _, action, reward, _ = cr.load_rollout_z(j)\n",
    "        Z[i] = z\n",
    "        A[i] = action\n",
    "        if use_reward:\n",
    "            Reward[i] = reward\n",
    "        if use_Done:\n",
    "            Done[i] = done\n",
    "\n",
    "    Z = np.moveaxis(Z, 1, 0)\n",
    "    A = np.moveaxis(A, 1, 0)\n",
    "    if use_reward:\n",
    "        Reward = np.moveaxis(Reward, 1, 0)\n",
    "    if use_Done:\n",
    "        Done = np.moveaxis(Done, 1, 0)\n",
    "    \n",
    "    return Z, A, Reward, Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, A, Reward, Done = make_dataset_rnn(3, 300, [0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "n = 10000\n",
    "m = 300\n",
    "file_list = os.listdir('./data_z/')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mdnrnn = MDNRNN().to(device)\n",
    "optimizer = torch.optim.Adam(mdnrnn.parameters(), lr=0.0001)\n",
    "checkpoint_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/900], Loss: 1.1539\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [20/900], Loss: 1.1320\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [30/900], Loss: 1.1717\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [40/900], Loss: 1.1503\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [50/900], Loss: 1.1206\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [60/900], Loss: 1.1484\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [70/900], Loss: 1.1572\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [80/900], Loss: 1.0820\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [90/900], Loss: 1.0449\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [100/900], Loss: 1.1136\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [110/900], Loss: 1.1271\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [120/900], Loss: 1.0941\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [130/900], Loss: 1.0632\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [140/900], Loss: 1.0584\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [150/900], Loss: 1.0711\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [160/900], Loss: 1.0564\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [170/900], Loss: 1.0977\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [180/900], Loss: 1.0437\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [190/900], Loss: 1.0816\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [200/900], Loss: 1.0581\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [210/900], Loss: 1.0414\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [220/900], Loss: 1.0878\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [230/900], Loss: 1.0354\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [240/900], Loss: 1.1190\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [250/900], Loss: 1.1551\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [260/900], Loss: 1.0213\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [270/900], Loss: 1.0510\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [280/900], Loss: 1.0515\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [290/900], Loss: 1.0502\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [300/900], Loss: 1.0832\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [310/900], Loss: 1.0304\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [320/900], Loss: 1.0114\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [330/900], Loss: 1.0033\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [340/900], Loss: 0.9948\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [350/900], Loss: 1.0873\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [360/900], Loss: 1.0030\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [370/900], Loss: 0.9974\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [380/900], Loss: 1.0406\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [390/900], Loss: 1.0100\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [400/900], Loss: 1.0201\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [410/900], Loss: 1.0086\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [420/900], Loss: 0.9818\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [430/900], Loss: 1.0420\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [440/900], Loss: 1.0302\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [450/900], Loss: 1.0451\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [460/900], Loss: 0.9825\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [470/900], Loss: 0.9805\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [480/900], Loss: 0.9890\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [490/900], Loss: 0.9736\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [500/900], Loss: 0.9721\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [510/900], Loss: 0.9854\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [520/900], Loss: 0.9586\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [530/900], Loss: 0.9356\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [540/900], Loss: 0.9479\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [550/900], Loss: 0.9893\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [560/900], Loss: 0.9427\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [570/900], Loss: 0.9770\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [580/900], Loss: 0.9235\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [590/900], Loss: 0.9495\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [600/900], Loss: 0.9843\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [610/900], Loss: 0.8680\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [620/900], Loss: 0.9258\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [630/900], Loss: 0.9285\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [640/900], Loss: 0.9269\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [650/900], Loss: 0.9743\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [660/900], Loss: 0.8919\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [670/900], Loss: 0.8894\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [680/900], Loss: 0.8749\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [690/900], Loss: 0.9274\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [700/900], Loss: 0.9012\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [710/900], Loss: 0.9009\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [720/900], Loss: 0.8446\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [730/900], Loss: 0.8473\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [740/900], Loss: 0.8519\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [750/900], Loss: 0.8644\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [760/900], Loss: 0.8876\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [770/900], Loss: 0.8267\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [780/900], Loss: 0.8330\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [790/900], Loss: 0.8737\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [800/900], Loss: 0.8249\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [810/900], Loss: 0.7821\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [820/900], Loss: 0.8006\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [830/900], Loss: 0.7956\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [840/900], Loss: 0.8016\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [850/900], Loss: 0.8134\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [860/900], Loss: 0.8322\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [870/900], Loss: 0.8012\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [880/900], Loss: 0.8119\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [890/900], Loss: 0.7847\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [900/900], Loss: 0.7774\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 1\n",
      "test: epoch 1\n",
      "Epoch [1/10], Step [10/100], Loss: 0.7806\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [20/100], Loss: 0.7811\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [30/100], Loss: 0.7644\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [40/100], Loss: 0.7195\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [50/100], Loss: 0.7410\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [60/100], Loss: 0.7604\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [70/100], Loss: 0.7632\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [80/100], Loss: 0.8009\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [90/100], Loss: 0.7750\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [1/10], Step [100/100], Loss: 0.7926\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [10/900], Loss: 0.7801\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [20/900], Loss: 0.7112\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [30/900], Loss: 0.7329\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [40/900], Loss: 0.7248\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [50/900], Loss: 0.7397\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [60/900], Loss: 0.7051\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [70/900], Loss: 0.7450\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [80/900], Loss: 0.7285\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [90/900], Loss: 0.7321\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [100/900], Loss: 0.7176\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [110/900], Loss: 0.6952\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [120/900], Loss: 0.6932\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [130/900], Loss: 0.7409\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [140/900], Loss: 0.7330\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [150/900], Loss: 0.6881\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [160/900], Loss: 0.6551\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [170/900], Loss: 0.6682\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [180/900], Loss: 0.6483\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [190/900], Loss: 0.6380\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [200/900], Loss: 0.6539\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [210/900], Loss: 0.6256\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [220/900], Loss: 0.6854\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [230/900], Loss: 0.6310\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [240/900], Loss: 0.6446\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [250/900], Loss: 0.6022\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [260/900], Loss: 0.6132\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [270/900], Loss: 0.6361\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [280/900], Loss: 0.5938\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [290/900], Loss: 0.5679\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [300/900], Loss: 0.6143\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [310/900], Loss: 0.5489\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [320/900], Loss: 0.5555\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [330/900], Loss: 0.6127\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [340/900], Loss: 0.6251\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [350/900], Loss: 0.5871\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [360/900], Loss: 0.5936\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [370/900], Loss: 0.6168\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [380/900], Loss: 0.6045\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [390/900], Loss: 0.6053\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [400/900], Loss: 0.5455\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [410/900], Loss: 0.6126\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [420/900], Loss: 0.5632\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [430/900], Loss: 0.5995\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [440/900], Loss: 0.5630\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [450/900], Loss: 0.5355\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [460/900], Loss: 0.5497\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [470/900], Loss: 0.5434\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [480/900], Loss: 0.5715\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [490/900], Loss: 0.6114\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [500/900], Loss: 0.5859\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [510/900], Loss: 0.5782\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [520/900], Loss: 0.6170\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [530/900], Loss: 0.5470\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [540/900], Loss: 0.5334\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [550/900], Loss: 0.5579\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [560/900], Loss: 0.5450\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [570/900], Loss: 0.6002\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [580/900], Loss: 0.5777\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [590/900], Loss: 0.5236\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [600/900], Loss: 0.5399\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [610/900], Loss: 0.5853\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [620/900], Loss: 0.5750\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [630/900], Loss: 0.5408\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [640/900], Loss: 0.5667\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [650/900], Loss: 0.5959\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [660/900], Loss: 0.5333\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [670/900], Loss: 0.5625\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [680/900], Loss: 0.5352\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [690/900], Loss: 0.5486\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [700/900], Loss: 0.5306\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [710/900], Loss: 0.5472\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [720/900], Loss: 0.5459\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [730/900], Loss: 0.5393\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [740/900], Loss: 0.5927\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [750/900], Loss: 0.5710\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [760/900], Loss: 0.5654\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [770/900], Loss: 0.5148\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [780/900], Loss: 0.5848\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [790/900], Loss: 0.5770\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [800/900], Loss: 0.5227\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [810/900], Loss: 0.5256\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [820/900], Loss: 0.5309\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [830/900], Loss: 0.5474\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [840/900], Loss: 0.5376\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [850/900], Loss: 0.5402\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [860/900], Loss: 0.5736\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [870/900], Loss: 0.5315\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [880/900], Loss: 0.5836\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [890/900], Loss: 0.5566\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [900/900], Loss: 0.5839\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 2\n",
      "test: epoch 2\n",
      "Epoch [2/10], Step [10/100], Loss: 0.5441\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [20/100], Loss: 0.5749\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [30/100], Loss: 0.5459\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [40/100], Loss: 0.5416\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [50/100], Loss: 0.5525\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [60/100], Loss: 0.5683\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [70/100], Loss: 0.5109\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [80/100], Loss: 0.5384\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [90/100], Loss: 0.5064\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [2/10], Step [100/100], Loss: 0.5479\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [10/900], Loss: 0.5276\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [20/900], Loss: 0.5461\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [30/900], Loss: 0.5508\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [40/900], Loss: 0.5454\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [50/900], Loss: 0.5516\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [60/900], Loss: 0.5678\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [70/900], Loss: 0.5461\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [80/900], Loss: 0.5168\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [90/900], Loss: 0.5628\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [100/900], Loss: 0.5226\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [110/900], Loss: 0.5271\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [120/900], Loss: 0.5656\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [130/900], Loss: 0.5536\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [140/900], Loss: 0.5425\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [150/900], Loss: 0.5728\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [160/900], Loss: 0.5615\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [170/900], Loss: 0.5605\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [180/900], Loss: 0.5578\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [190/900], Loss: 0.5662\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [200/900], Loss: 0.5277\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [210/900], Loss: 0.5587\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [220/900], Loss: 0.5198\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [230/900], Loss: 0.5654\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [240/900], Loss: 0.5818\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [250/900], Loss: 0.5789\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [260/900], Loss: 0.5382\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [270/900], Loss: 0.5160\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [280/900], Loss: 0.5363\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [290/900], Loss: 0.5326\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [300/900], Loss: 0.5309\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [310/900], Loss: 0.5229\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [320/900], Loss: 0.5143\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [330/900], Loss: 0.5215\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [340/900], Loss: 0.4943\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [350/900], Loss: 0.5704\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [360/900], Loss: 0.5142\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [370/900], Loss: 0.5199\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [380/900], Loss: 0.5114\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [390/900], Loss: 0.5290\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [400/900], Loss: 0.5827\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [410/900], Loss: 0.5235\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [420/900], Loss: 0.5381\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [430/900], Loss: 0.5006\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [440/900], Loss: 0.5141\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [450/900], Loss: 0.5438\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [460/900], Loss: 0.5403\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [470/900], Loss: 0.5231\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [480/900], Loss: 0.5222\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [490/900], Loss: 0.5089\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [500/900], Loss: 0.5222\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [510/900], Loss: 0.4886\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [520/900], Loss: 0.5371\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [530/900], Loss: 0.5303\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [540/900], Loss: 0.5272\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [550/900], Loss: 0.5216\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [560/900], Loss: 0.4904\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [570/900], Loss: 0.5258\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [580/900], Loss: 0.5150\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [590/900], Loss: 0.5490\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [600/900], Loss: 0.5364\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [610/900], Loss: 0.5496\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [620/900], Loss: 0.5153\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [630/900], Loss: 0.4980\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [640/900], Loss: 0.4922\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [650/900], Loss: 0.5833\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [660/900], Loss: 0.5174\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [670/900], Loss: 0.5157\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [680/900], Loss: 0.5291\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [690/900], Loss: 0.5102\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [700/900], Loss: 0.5401\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [710/900], Loss: 0.5352\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [720/900], Loss: 0.4906\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [730/900], Loss: 0.5056\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [740/900], Loss: 0.5157\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [750/900], Loss: 0.5490\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [760/900], Loss: 0.5336\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [770/900], Loss: 0.5398\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [780/900], Loss: 0.5109\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [790/900], Loss: 0.5311\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [800/900], Loss: 0.5378\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [810/900], Loss: 0.5506\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [820/900], Loss: 0.5370\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [830/900], Loss: 0.5205\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [840/900], Loss: 0.5029\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [850/900], Loss: 0.5023\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [860/900], Loss: 0.5458\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [870/900], Loss: 0.5476\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [880/900], Loss: 0.4987\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [890/900], Loss: 0.5152\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [900/900], Loss: 0.5051\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 3\n",
      "test: epoch 3\n",
      "Epoch [3/10], Step [10/100], Loss: 0.5214\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [20/100], Loss: 0.5512\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [30/100], Loss: 0.5265\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [40/100], Loss: 0.5148\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [50/100], Loss: 0.5279\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [60/100], Loss: 0.5402\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [70/100], Loss: 0.4786\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [80/100], Loss: 0.5007\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [90/100], Loss: 0.4779\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [3/10], Step [100/100], Loss: 0.5255\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [10/900], Loss: 0.5250\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [20/900], Loss: 0.5361\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [30/900], Loss: 0.5026\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [40/900], Loss: 0.4987\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [50/900], Loss: 0.5115\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [60/900], Loss: 0.5197\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [70/900], Loss: 0.5130\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [80/900], Loss: 0.5182\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [90/900], Loss: 0.5004\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [100/900], Loss: 0.5469\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [110/900], Loss: 0.5357\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [120/900], Loss: 0.5231\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [130/900], Loss: 0.5398\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [140/900], Loss: 0.5118\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [150/900], Loss: 0.5024\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [160/900], Loss: 0.5272\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [170/900], Loss: 0.5073\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [180/900], Loss: 0.5233\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [190/900], Loss: 0.5219\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [200/900], Loss: 0.5170\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [210/900], Loss: 0.4907\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [220/900], Loss: 0.4997\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [230/900], Loss: 0.5065\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [240/900], Loss: 0.4716\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [250/900], Loss: 0.5073\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [260/900], Loss: 0.5108\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [270/900], Loss: 0.5134\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [280/900], Loss: 0.5037\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [290/900], Loss: 0.5043\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [300/900], Loss: 0.5297\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [310/900], Loss: 0.5402\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [320/900], Loss: 0.4996\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [330/900], Loss: 0.5027\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [340/900], Loss: 0.4799\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [350/900], Loss: 0.5034\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [360/900], Loss: 0.5259\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [370/900], Loss: 0.4927\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [380/900], Loss: 0.5140\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [390/900], Loss: 0.5234\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [400/900], Loss: 0.5509\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [410/900], Loss: 0.5106\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [420/900], Loss: 0.5261\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [430/900], Loss: 0.5258\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [440/900], Loss: 0.5359\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [450/900], Loss: 0.5011\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [460/900], Loss: 0.4871\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [470/900], Loss: 0.5287\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [480/900], Loss: 0.5013\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [490/900], Loss: 0.5447\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [500/900], Loss: 0.5138\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [510/900], Loss: 0.5148\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [520/900], Loss: 0.5052\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [530/900], Loss: 0.5284\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [540/900], Loss: 0.5087\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [550/900], Loss: 0.4996\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [560/900], Loss: 0.5410\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [570/900], Loss: 0.5027\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [580/900], Loss: 0.4957\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [590/900], Loss: 0.5286\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [600/900], Loss: 0.5500\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [610/900], Loss: 0.5467\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [620/900], Loss: 0.4952\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [630/900], Loss: 0.5059\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [640/900], Loss: 0.4901\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [650/900], Loss: 0.5417\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [660/900], Loss: 0.4839\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [670/900], Loss: 0.4953\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [680/900], Loss: 0.5220\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [690/900], Loss: 0.5016\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [700/900], Loss: 0.5109\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [710/900], Loss: 0.5090\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [720/900], Loss: 0.5302\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [730/900], Loss: 0.4953\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [740/900], Loss: 0.4841\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [750/900], Loss: 0.5049\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [760/900], Loss: 0.4890\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [770/900], Loss: 0.5239\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [780/900], Loss: 0.5172\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [790/900], Loss: 0.4953\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [800/900], Loss: 0.5297\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [810/900], Loss: 0.5438\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [820/900], Loss: 0.4742\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [830/900], Loss: 0.4761\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [840/900], Loss: 0.5467\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [850/900], Loss: 0.5310\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [860/900], Loss: 0.4910\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [870/900], Loss: 0.5040\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [880/900], Loss: 0.5142\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [890/900], Loss: 0.4902\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [900/900], Loss: 0.4936\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 4\n",
      "test: epoch 4\n",
      "Epoch [4/10], Step [10/100], Loss: 0.5087\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [20/100], Loss: 0.5395\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [30/100], Loss: 0.5158\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [40/100], Loss: 0.5005\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [50/100], Loss: 0.5168\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [60/100], Loss: 0.5290\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [70/100], Loss: 0.4624\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [80/100], Loss: 0.4841\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [90/100], Loss: 0.4612\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [4/10], Step [100/100], Loss: 0.5099\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [10/900], Loss: 0.4940\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [20/900], Loss: 0.5290\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [30/900], Loss: 0.5117\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [40/900], Loss: 0.4869\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [50/900], Loss: 0.5407\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [60/900], Loss: 0.4804\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [70/900], Loss: 0.5563\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [80/900], Loss: 0.5142\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [90/900], Loss: 0.4898\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [100/900], Loss: 0.5021\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [110/900], Loss: 0.5307\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [120/900], Loss: 0.5043\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [130/900], Loss: 0.5250\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [140/900], Loss: 0.5212\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [150/900], Loss: 0.4947\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [160/900], Loss: 0.5050\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [170/900], Loss: 0.5587\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [180/900], Loss: 0.4950\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [190/900], Loss: 0.5164\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [200/900], Loss: 0.4939\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [210/900], Loss: 0.5055\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [220/900], Loss: 0.5040\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [230/900], Loss: 0.4753\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [240/900], Loss: 0.4877\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [250/900], Loss: 0.4548\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [260/900], Loss: 0.4879\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [270/900], Loss: 0.5100\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [280/900], Loss: 0.4869\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [290/900], Loss: 0.4863\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [300/900], Loss: 0.5259\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [310/900], Loss: 0.4990\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [320/900], Loss: 0.5307\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [330/900], Loss: 0.5177\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [340/900], Loss: 0.5327\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [350/900], Loss: 0.5303\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [360/900], Loss: 0.4791\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [370/900], Loss: 0.4702\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [380/900], Loss: 0.5014\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [390/900], Loss: 0.5071\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [400/900], Loss: 0.4795\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [410/900], Loss: 0.4864\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [420/900], Loss: 0.5244\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [430/900], Loss: 0.5002\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [440/900], Loss: 0.4964\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [450/900], Loss: 0.4932\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [460/900], Loss: 0.5175\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [470/900], Loss: 0.4774\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [480/900], Loss: 0.5004\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [490/900], Loss: 0.5103\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [500/900], Loss: 0.5099\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [510/900], Loss: 0.4988\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [520/900], Loss: 0.5442\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [530/900], Loss: 0.5271\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [540/900], Loss: 0.5172\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [550/900], Loss: 0.4888\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [560/900], Loss: 0.5162\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [570/900], Loss: 0.5130\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [580/900], Loss: 0.4814\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [590/900], Loss: 0.5161\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [600/900], Loss: 0.4799\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [610/900], Loss: 0.4780\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [620/900], Loss: 0.4958\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [630/900], Loss: 0.5326\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [640/900], Loss: 0.4973\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [650/900], Loss: 0.4724\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [660/900], Loss: 0.5109\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [670/900], Loss: 0.4965\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [680/900], Loss: 0.4959\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [690/900], Loss: 0.5144\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [700/900], Loss: 0.4924\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [710/900], Loss: 0.4690\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [720/900], Loss: 0.5002\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [730/900], Loss: 0.4898\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [740/900], Loss: 0.4778\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [750/900], Loss: 0.4799\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [760/900], Loss: 0.4875\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [770/900], Loss: 0.4801\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [780/900], Loss: 0.5198\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [790/900], Loss: 0.4873\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [800/900], Loss: 0.5216\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [810/900], Loss: 0.4834\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [820/900], Loss: 0.5010\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [830/900], Loss: 0.4879\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [840/900], Loss: 0.4827\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [850/900], Loss: 0.4679\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [860/900], Loss: 0.4851\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [870/900], Loss: 0.4840\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [880/900], Loss: 0.5060\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [890/900], Loss: 0.4980\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [900/900], Loss: 0.5269\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 5\n",
      "test: epoch 5\n",
      "Epoch [5/10], Step [10/100], Loss: 0.4986\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [20/100], Loss: 0.5253\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [30/100], Loss: 0.5043\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [40/100], Loss: 0.4918\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [50/100], Loss: 0.5082\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [60/100], Loss: 0.5140\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [70/100], Loss: 0.4510\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [80/100], Loss: 0.4724\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [90/100], Loss: 0.4489\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [5/10], Step [100/100], Loss: 0.4917\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [10/900], Loss: 0.5099\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [20/900], Loss: 0.4982\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [30/900], Loss: 0.4889\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [40/900], Loss: 0.4807\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [50/900], Loss: 0.5044\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [60/900], Loss: 0.5057\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [70/900], Loss: 0.4636\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [80/900], Loss: 0.4916\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [90/900], Loss: 0.4828\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [100/900], Loss: 0.5064\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [110/900], Loss: 0.4784\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [120/900], Loss: 0.4871\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [130/900], Loss: 0.4954\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [140/900], Loss: 0.5111\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [150/900], Loss: 0.4822\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [160/900], Loss: 0.4808\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [170/900], Loss: 0.4826\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [180/900], Loss: 0.4726\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [190/900], Loss: 0.5315\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [200/900], Loss: 0.4689\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [210/900], Loss: 0.4783\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [220/900], Loss: 0.4749\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [230/900], Loss: 0.5301\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [240/900], Loss: 0.4943\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [250/900], Loss: 0.4923\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [260/900], Loss: 0.5035\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [270/900], Loss: 0.5201\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [280/900], Loss: 0.4652\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [290/900], Loss: 0.4888\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [300/900], Loss: 0.4940\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [310/900], Loss: 0.4700\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [320/900], Loss: 0.5139\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [330/900], Loss: 0.4812\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [340/900], Loss: 0.5088\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [350/900], Loss: 0.5068\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [360/900], Loss: 0.5023\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [370/900], Loss: 0.4843\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [380/900], Loss: 0.4763\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [390/900], Loss: 0.4863\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [400/900], Loss: 0.4885\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [410/900], Loss: 0.4803\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [420/900], Loss: 0.4752\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [430/900], Loss: 0.4957\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [440/900], Loss: 0.4791\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [450/900], Loss: 0.5284\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [460/900], Loss: 0.4951\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [470/900], Loss: 0.5225\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [480/900], Loss: 0.5056\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [490/900], Loss: 0.4810\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [500/900], Loss: 0.4938\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [510/900], Loss: 0.5147\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [520/900], Loss: 0.4988\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [530/900], Loss: 0.4757\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [540/900], Loss: 0.5199\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [550/900], Loss: 0.4965\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [560/900], Loss: 0.4665\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [570/900], Loss: 0.4679\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [580/900], Loss: 0.4969\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [590/900], Loss: 0.4931\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [600/900], Loss: 0.4947\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [610/900], Loss: 0.4826\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [620/900], Loss: 0.4819\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [630/900], Loss: 0.5101\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [640/900], Loss: 0.4740\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [650/900], Loss: 0.4957\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [660/900], Loss: 0.4691\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [670/900], Loss: 0.5156\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [680/900], Loss: 0.4920\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [690/900], Loss: 0.5259\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [700/900], Loss: 0.4960\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [710/900], Loss: 0.4581\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [720/900], Loss: 0.5043\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [730/900], Loss: 0.5140\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [740/900], Loss: 0.5075\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [750/900], Loss: 0.4913\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [760/900], Loss: 0.4909\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [770/900], Loss: 0.4997\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [780/900], Loss: 0.5361\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [790/900], Loss: 0.5102\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [800/900], Loss: 0.4737\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [810/900], Loss: 0.4723\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [820/900], Loss: 0.4737\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [830/900], Loss: 0.5038\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [840/900], Loss: 0.4665\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [850/900], Loss: 0.4683\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [860/900], Loss: 0.4900\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [870/900], Loss: 0.5533\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [880/900], Loss: 0.4663\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [890/900], Loss: 0.4721\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [900/900], Loss: 0.5221\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 6\n",
      "test: epoch 6\n",
      "Epoch [6/10], Step [10/100], Loss: 0.4877\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [20/100], Loss: 0.5126\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [30/100], Loss: 0.4954\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [40/100], Loss: 0.4832\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [50/100], Loss: 0.4993\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [60/100], Loss: 0.5063\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [70/100], Loss: 0.4418\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [80/100], Loss: 0.4595\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [90/100], Loss: 0.4371\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [6/10], Step [100/100], Loss: 0.4785\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [10/900], Loss: 0.4872\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [20/900], Loss: 0.4864\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [30/900], Loss: 0.4773\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [40/900], Loss: 0.4746\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [50/900], Loss: 0.4761\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [60/900], Loss: 0.4830\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [70/900], Loss: 0.4752\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [80/900], Loss: 0.4866\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [90/900], Loss: 0.4508\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [100/900], Loss: 0.4957\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [110/900], Loss: 0.4964\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [120/900], Loss: 0.5116\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [130/900], Loss: 0.4493\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [140/900], Loss: 0.5080\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [150/900], Loss: 0.4710\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [160/900], Loss: 0.4898\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [170/900], Loss: 0.5093\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [180/900], Loss: 0.4965\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [190/900], Loss: 0.5040\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [200/900], Loss: 0.4674\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [210/900], Loss: 0.5062\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [220/900], Loss: 0.4665\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [230/900], Loss: 0.4877\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [240/900], Loss: 0.4457\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [250/900], Loss: 0.4923\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [260/900], Loss: 0.4821\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [270/900], Loss: 0.4700\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [280/900], Loss: 0.4940\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [290/900], Loss: 0.4629\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [300/900], Loss: 0.5052\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [310/900], Loss: 0.4518\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [320/900], Loss: 0.5089\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [330/900], Loss: 0.4888\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [340/900], Loss: 0.4691\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [350/900], Loss: 0.4970\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [360/900], Loss: 0.4930\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [370/900], Loss: 0.4905\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [380/900], Loss: 0.4716\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [390/900], Loss: 0.4814\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [400/900], Loss: 0.4750\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [410/900], Loss: 0.4665\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [420/900], Loss: 0.4603\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [430/900], Loss: 0.5164\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [440/900], Loss: 0.4647\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [450/900], Loss: 0.4867\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [460/900], Loss: 0.4808\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [470/900], Loss: 0.4886\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [480/900], Loss: 0.4694\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [490/900], Loss: 0.5047\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [500/900], Loss: 0.4532\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [510/900], Loss: 0.4707\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [520/900], Loss: 0.4805\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [530/900], Loss: 0.4808\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [540/900], Loss: 0.4751\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [550/900], Loss: 0.4964\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [560/900], Loss: 0.4633\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [570/900], Loss: 0.5027\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [580/900], Loss: 0.4831\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [590/900], Loss: 0.4765\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [600/900], Loss: 0.4660\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [610/900], Loss: 0.4877\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [620/900], Loss: 0.4812\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [630/900], Loss: 0.4854\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [640/900], Loss: 0.5059\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [650/900], Loss: 0.4982\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [660/900], Loss: 0.4851\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [670/900], Loss: 0.4864\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [680/900], Loss: 0.4568\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [690/900], Loss: 0.4888\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [700/900], Loss: 0.4805\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [710/900], Loss: 0.4916\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [720/900], Loss: 0.4853\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [730/900], Loss: 0.4638\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [740/900], Loss: 0.4744\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [750/900], Loss: 0.4638\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [760/900], Loss: 0.4824\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [770/900], Loss: 0.4642\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [780/900], Loss: 0.4681\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [790/900], Loss: 0.4418\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [800/900], Loss: 0.4945\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [810/900], Loss: 0.4504\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [820/900], Loss: 0.5010\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [830/900], Loss: 0.4850\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [840/900], Loss: 0.4935\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [850/900], Loss: 0.4774\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [860/900], Loss: 0.4801\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [870/900], Loss: 0.4511\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [880/900], Loss: 0.4761\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [890/900], Loss: 0.4534\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [900/900], Loss: 0.4860\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 7\n",
      "test: epoch 7\n",
      "Epoch [7/10], Step [10/100], Loss: 0.4762\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [20/100], Loss: 0.5004\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [30/100], Loss: 0.4895\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [40/100], Loss: 0.4738\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [50/100], Loss: 0.4928\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [60/100], Loss: 0.4975\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [70/100], Loss: 0.4363\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [80/100], Loss: 0.4503\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [90/100], Loss: 0.4271\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [7/10], Step [100/100], Loss: 0.4682\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [10/900], Loss: 0.4707\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [20/900], Loss: 0.4932\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [30/900], Loss: 0.4525\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [40/900], Loss: 0.4450\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [50/900], Loss: 0.4562\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [60/900], Loss: 0.4874\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [70/900], Loss: 0.4627\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [80/900], Loss: 0.4842\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [90/900], Loss: 0.4578\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [100/900], Loss: 0.4437\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [110/900], Loss: 0.4841\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [120/900], Loss: 0.4631\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [130/900], Loss: 0.5103\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [140/900], Loss: 0.4658\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [150/900], Loss: 0.4696\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [160/900], Loss: 0.4966\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [170/900], Loss: 0.4658\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [180/900], Loss: 0.4702\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [190/900], Loss: 0.5290\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [200/900], Loss: 0.4668\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [210/900], Loss: 0.5295\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [220/900], Loss: 0.4835\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [230/900], Loss: 0.4717\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [240/900], Loss: 0.4795\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [250/900], Loss: 0.4826\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [260/900], Loss: 0.4920\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [270/900], Loss: 0.4616\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [280/900], Loss: 0.4933\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [290/900], Loss: 0.4686\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [300/900], Loss: 0.4811\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [310/900], Loss: 0.4792\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [320/900], Loss: 0.4776\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [330/900], Loss: 0.4754\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [340/900], Loss: 0.4594\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [350/900], Loss: 0.4447\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [360/900], Loss: 0.4916\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [370/900], Loss: 0.4487\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [380/900], Loss: 0.4786\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [390/900], Loss: 0.4901\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [400/900], Loss: 0.4707\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [410/900], Loss: 0.4615\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [420/900], Loss: 0.4677\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [430/900], Loss: 0.4834\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [440/900], Loss: 0.4677\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [450/900], Loss: 0.4895\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [460/900], Loss: 0.4820\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [470/900], Loss: 0.4640\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [480/900], Loss: 0.4832\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [490/900], Loss: 0.4717\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [500/900], Loss: 0.4771\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [510/900], Loss: 0.4847\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [520/900], Loss: 0.5001\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [530/900], Loss: 0.5051\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [540/900], Loss: 0.4737\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [550/900], Loss: 0.4746\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [560/900], Loss: 0.4621\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [570/900], Loss: 0.4844\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [580/900], Loss: 0.4762\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [590/900], Loss: 0.4901\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [600/900], Loss: 0.4670\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [610/900], Loss: 0.4866\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [620/900], Loss: 0.4852\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [630/900], Loss: 0.4967\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [640/900], Loss: 0.4408\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [650/900], Loss: 0.4800\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [660/900], Loss: 0.4946\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [670/900], Loss: 0.4852\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [680/900], Loss: 0.4996\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [690/900], Loss: 0.4511\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [700/900], Loss: 0.4491\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [710/900], Loss: 0.4701\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [720/900], Loss: 0.4581\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [730/900], Loss: 0.4570\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [740/900], Loss: 0.4642\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [750/900], Loss: 0.4767\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [760/900], Loss: 0.4584\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [770/900], Loss: 0.4441\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [780/900], Loss: 0.4418\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [790/900], Loss: 0.4688\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [800/900], Loss: 0.4569\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [810/900], Loss: 0.4838\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [820/900], Loss: 0.4571\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [830/900], Loss: 0.4682\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [840/900], Loss: 0.4622\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [850/900], Loss: 0.4683\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [860/900], Loss: 0.4621\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [870/900], Loss: 0.4767\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [880/900], Loss: 0.4468\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [890/900], Loss: 0.4476\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [900/900], Loss: 0.4736\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 8\n",
      "test: epoch 8\n",
      "Epoch [8/10], Step [10/100], Loss: 0.4634\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [20/100], Loss: 0.4900\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [30/100], Loss: 0.4847\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [40/100], Loss: 0.4661\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [50/100], Loss: 0.4865\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [60/100], Loss: 0.4913\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [70/100], Loss: 0.4297\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [80/100], Loss: 0.4418\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [90/100], Loss: 0.4209\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [8/10], Step [100/100], Loss: 0.4582\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [10/900], Loss: 0.4727\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [20/900], Loss: 0.5019\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [30/900], Loss: 0.4750\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [40/900], Loss: 0.5223\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [50/900], Loss: 0.4276\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [60/900], Loss: 0.4893\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [70/900], Loss: 0.4991\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [80/900], Loss: 0.4630\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [90/900], Loss: 0.4593\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [100/900], Loss: 0.4617\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [110/900], Loss: 0.4335\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [120/900], Loss: 0.4784\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [130/900], Loss: 0.4704\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [140/900], Loss: 0.4583\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [150/900], Loss: 0.4567\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [160/900], Loss: 0.4815\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [170/900], Loss: 0.4918\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [180/900], Loss: 0.4688\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [190/900], Loss: 0.4959\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [200/900], Loss: 0.4528\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [210/900], Loss: 0.4308\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [220/900], Loss: 0.4570\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [230/900], Loss: 0.4782\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [240/900], Loss: 0.4503\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [250/900], Loss: 0.4638\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [260/900], Loss: 0.4737\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [270/900], Loss: 0.4889\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [280/900], Loss: 0.4780\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [290/900], Loss: 0.4713\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [300/900], Loss: 0.4499\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [310/900], Loss: 0.4712\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [320/900], Loss: 0.4609\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [330/900], Loss: 0.4264\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [340/900], Loss: 0.4868\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [350/900], Loss: 0.4509\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [360/900], Loss: 0.4990\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [370/900], Loss: 0.4745\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [380/900], Loss: 0.4818\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [390/900], Loss: 0.4707\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [400/900], Loss: 0.4923\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [410/900], Loss: 0.4669\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [420/900], Loss: 0.4457\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [430/900], Loss: 0.4827\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [440/900], Loss: 0.4601\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [450/900], Loss: 0.4296\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [460/900], Loss: 0.4892\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [470/900], Loss: 0.4828\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [480/900], Loss: 0.4623\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [490/900], Loss: 0.4292\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [500/900], Loss: 0.4665\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [510/900], Loss: 0.4613\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [520/900], Loss: 0.4906\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [530/900], Loss: 0.4635\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [540/900], Loss: 0.4681\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [550/900], Loss: 0.4893\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [560/900], Loss: 0.4659\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [570/900], Loss: 0.4458\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [580/900], Loss: 0.4413\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [590/900], Loss: 0.4890\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [600/900], Loss: 0.4965\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [610/900], Loss: 0.4542\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [620/900], Loss: 0.4574\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [630/900], Loss: 0.4687\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [640/900], Loss: 0.4602\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [650/900], Loss: 0.4651\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [660/900], Loss: 0.4758\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [670/900], Loss: 0.4590\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [680/900], Loss: 0.4719\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [690/900], Loss: 0.4784\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [700/900], Loss: 0.4740\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [710/900], Loss: 0.4542\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [720/900], Loss: 0.4517\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [730/900], Loss: 0.4328\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [740/900], Loss: 0.4437\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [750/900], Loss: 0.4482\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [760/900], Loss: 0.4812\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [770/900], Loss: 0.4476\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [780/900], Loss: 0.4458\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [790/900], Loss: 0.4958\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [800/900], Loss: 0.4481\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [810/900], Loss: 0.4952\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [820/900], Loss: 0.4743\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [830/900], Loss: 0.4832\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [840/900], Loss: 0.4527\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [850/900], Loss: 0.4232\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [860/900], Loss: 0.4419\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [870/900], Loss: 0.4687\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [880/900], Loss: 0.4932\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [890/900], Loss: 0.4648\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [900/900], Loss: 0.4646\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 9\n",
      "test: epoch 9\n",
      "Epoch [9/10], Step [10/100], Loss: 0.4545\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [20/100], Loss: 0.4805\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [30/100], Loss: 0.4807\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [40/100], Loss: 0.4630\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [50/100], Loss: 0.4828\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [60/100], Loss: 0.4872\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [70/100], Loss: 0.4273\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [80/100], Loss: 0.4344\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [90/100], Loss: 0.4152\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [9/10], Step [100/100], Loss: 0.4520\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [10/900], Loss: 0.4740\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [20/900], Loss: 0.4480\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [30/900], Loss: 0.4822\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [40/900], Loss: 0.4703\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [50/900], Loss: 0.4975\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [60/900], Loss: 0.4460\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [70/900], Loss: 0.5012\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [80/900], Loss: 0.4734\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [90/900], Loss: 0.4305\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [100/900], Loss: 0.5388\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [110/900], Loss: 0.4697\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [120/900], Loss: 0.4724\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [130/900], Loss: 0.4883\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [140/900], Loss: 0.4776\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [150/900], Loss: 0.4708\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [160/900], Loss: 0.4907\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [170/900], Loss: 0.4522\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [180/900], Loss: 0.4693\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [190/900], Loss: 0.4541\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [200/900], Loss: 0.4711\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [210/900], Loss: 0.4811\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [220/900], Loss: 0.4792\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [230/900], Loss: 0.4763\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [240/900], Loss: 0.4988\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [250/900], Loss: 0.4542\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [260/900], Loss: 0.4863\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [270/900], Loss: 0.4531\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [280/900], Loss: 0.4727\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [290/900], Loss: 0.4696\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [300/900], Loss: 0.4532\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [310/900], Loss: 0.4974\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [320/900], Loss: 0.4710\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [330/900], Loss: 0.4417\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [340/900], Loss: 0.4748\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [350/900], Loss: 0.4527\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [360/900], Loss: 0.4300\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [370/900], Loss: 0.4521\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [380/900], Loss: 0.4495\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [390/900], Loss: 0.4898\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [400/900], Loss: 0.4605\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [410/900], Loss: 0.4623\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [420/900], Loss: 0.4629\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [430/900], Loss: 0.4709\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [440/900], Loss: 0.4831\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [450/900], Loss: 0.4865\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [460/900], Loss: 0.4445\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [470/900], Loss: 0.4633\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [480/900], Loss: 0.4412\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [490/900], Loss: 0.4554\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [500/900], Loss: 0.4612\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [510/900], Loss: 0.4363\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [520/900], Loss: 0.4512\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [530/900], Loss: 0.4724\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [540/900], Loss: 0.4242\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [550/900], Loss: 0.4844\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [560/900], Loss: 0.4797\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [570/900], Loss: 0.4467\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [580/900], Loss: 0.4443\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [590/900], Loss: 0.4965\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [600/900], Loss: 0.4401\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [610/900], Loss: 0.4402\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [620/900], Loss: 0.4799\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [630/900], Loss: 0.4488\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [640/900], Loss: 0.4165\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [650/900], Loss: 0.4858\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [660/900], Loss: 0.4500\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [670/900], Loss: 0.4562\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [680/900], Loss: 0.4659\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [690/900], Loss: 0.4650\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [700/900], Loss: 0.4596\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [710/900], Loss: 0.4577\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [720/900], Loss: 0.4765\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [730/900], Loss: 0.4467\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [740/900], Loss: 0.4989\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [750/900], Loss: 0.4441\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [760/900], Loss: 0.4670\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [770/900], Loss: 0.4675\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [780/900], Loss: 0.4965\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [790/900], Loss: 0.4595\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [800/900], Loss: 0.4422\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [810/900], Loss: 0.4785\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [820/900], Loss: 0.4969\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [830/900], Loss: 0.4592\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [840/900], Loss: 0.4302\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [850/900], Loss: 0.4361\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [860/900], Loss: 0.4419\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [870/900], Loss: 0.4871\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [880/900], Loss: 0.4448\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [890/900], Loss: 0.4637\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [900/900], Loss: 0.4775\n",
      "Use Memory: 29.76 MB\n",
      "save model: epoch 10\n",
      "test: epoch 10\n",
      "Epoch [10/10], Step [10/100], Loss: 0.4512\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [20/100], Loss: 0.4741\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [30/100], Loss: 0.4791\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [40/100], Loss: 0.4604\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [50/100], Loss: 0.4806\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [60/100], Loss: 0.4830\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [70/100], Loss: 0.4235\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [80/100], Loss: 0.4290\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [90/100], Loss: 0.4110\n",
      "Use Memory: 29.76 MB\n",
      "Epoch [10/10], Step [100/100], Loss: 0.4480\n",
      "Use Memory: 29.76 MB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "if checkpoint_epoch + 1 == num_epochs:\n",
    "    print('already trained')\n",
    "else:\n",
    "    for epoch in range(checkpoint_epoch+1, num_epochs):\n",
    "        dataset_idx = np.array([id for id in range(9000)]).astype(int)\n",
    "        np.random.shuffle(dataset_idx)\n",
    "        for i in range(9000 // batch_size):\n",
    "            Z, A, _, _ = make_dataset_rnn(batch_size, m, dataset_idx[i*batch_size:(i+1)*batch_size], use_reward=False, use_Done=False)\n",
    "            Z = torch.tensor(Z).float().to(device)\n",
    "            A = torch.tensor(A).float().to(device)\n",
    "            H0 = torch.zeros(1, 299, 256).float().to(device)\n",
    "            C0 = torch.zeros(1, 299, 256).float().to(device)\n",
    "\n",
    "            in_Z = Z[:-1]\n",
    "            out_Z = Z[1:]\n",
    "            A = A[:-1]\n",
    "\n",
    "            mdnrnn.train()\n",
    "            optimizer.zero_grad()\n",
    "            # mdn_out, hidden, cell = mdnrnn(A, in_Z, H0, C0)\n",
    "            pi, mu, logsigma, hidden, cell = mdnrnn(A, in_Z, H0, C0)\n",
    "            loss = loss_func(out_Z, pi, mu, logsigma)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(epoch+1, num_epochs, i+1, 9000 // batch_size, loss.item()))\n",
    "                print(\"Use Memory: {:.2f} MB\".format(torch.cuda.memory_allocated() / 1024 / 1024))\n",
    "\n",
    "            # GPU memory: remove data\n",
    "            del Z, A, H0, C0, in_Z, out_Z, pi, mu, logsigma, hidden, cell\n",
    "\n",
    "            if i != 9000 // batch_size - 1:\n",
    "                del loss\n",
    "        \n",
    "        print(\"save model: epoch {}\".format(epoch + 1))\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': mdnrnn.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'mdnrnn.pth')\n",
    "        \n",
    "        del loss\n",
    "\n",
    "        mdnrnn.eval()\n",
    "        print(\"test: epoch {}\".format(epoch + 1))\n",
    "        with torch.no_grad():\n",
    "            test_idx = np.array([i for i in range(9000, 10000)]).astype(int)\n",
    "            for i in range(1000 // batch_size):\n",
    "                Z, A, _, _ = make_dataset_rnn(batch_size, m, test_idx[i*batch_size:(i+1)*batch_size], use_reward=False, use_Done=False)\n",
    "                Z = torch.tensor(Z).float().to(device)\n",
    "                A = torch.tensor(A).float().to(device)\n",
    "                H0 = torch.zeros(1, 299, 256).float().to(device)\n",
    "                C0 = torch.zeros(1, 299, 256).float().to(device)\n",
    "\n",
    "                in_Z = Z[:-1]\n",
    "                out_Z = Z[1:]\n",
    "                A = A[:-1]\n",
    "\n",
    "                pi, mu, logsigma, hidden, cell = mdnrnn(A, in_Z, H0, C0)\n",
    "                loss = loss_func(out_Z, pi, mu, logsigma)\n",
    "\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\".format(epoch+1, num_epochs, i+1, 1000 // batch_size, loss.item()))\n",
    "                    print(\"Use Memory: {:.2f} MB\".format(torch.cuda.memory_allocated() / 1024 / 1024))\n",
    "\n",
    "                # GPU memory: remove data\n",
    "                del Z, A, H0, C0, in_Z, out_Z, pi, mu, logsigma, hidden, cell, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(35, 256, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"LSTM\"\"\"\n",
    "    def __init__(self, z_size=32, a_size=3, hidden_size=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.z_size = z_size\n",
    "        self.a_size = a_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(z_size + a_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, z, a, hidden, cell):\n",
    "        input = torch.cat((z, a), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        return output, hidden, cell\n",
    "\n",
    "lstm = LSTM().to(device)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    \"\"\"Mixtures Density Network\"\"\"\n",
    "    def __init__(self, z_size=32, hidden_size=256, num_layers=1, num_mixtures=5, use_reward=False, use_Done=False):\n",
    "        super().__init__()\n",
    "        self.z_size = z_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.use_reward = use_reward\n",
    "        self.use_Done = use_Done\n",
    "\n",
    "        if use_reward and use_Done:\n",
    "            self.output_size = 3 * num_mixtures * z_size + 2\n",
    "        elif use_reward or use_Done:\n",
    "            self.output_size = 3 * num_mixtures * z_size + 1\n",
    "        else:\n",
    "            self.output_size = 3 * num_mixtures * z_size\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "        # if self.use_reward and self.use_Done:\n",
    "        #     pi, mu, logsigma, reward, done = self.get_mixture(output)\n",
    "        #     return pi, mu, logsigma, reward, done\n",
    "        # elif self.use_reward or self.use_Done:\n",
    "        #     pi, mu, logsigma, reward_done = self.get_mixture(output)\n",
    "        #     return pi, mu, logsigma, reward_done\n",
    "        # else:\n",
    "        #     pi, mu, logsigma = self.get_mixture(output)\n",
    "        #     return pi, mu, logsigma\n",
    "    \n",
    "    def get_mixture(self, output):\n",
    "        pi = output[:, :, :self.num_mixtures]\n",
    "        mu = output[:, :, self.num_mixtures:2*self.num_mixtures]\n",
    "        logsigma = output[:, :, 2*self.num_mixtures:3*self.num_mixtures]\n",
    "\n",
    "        if self.use_reward and self.use_Done:\n",
    "            reward = output[:, :, 3*self.num_mixtures]\n",
    "            done = output[:, :, 3*self.num_mixtures+1]\n",
    "            return pi, mu, logsigma, reward, done\n",
    "        elif self.use_reward or self.use_Done:\n",
    "            reward_done = output[:, :, 3*self.num_mixtures]\n",
    "            return pi, mu, logsigma, reward_done\n",
    "        else:\n",
    "            return pi, mu, logsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDNRNN(nn.Module):\n",
    "    \"\"\"MDN-RNN\"\"\"\n",
    "    def __init__(self, z_size=32, a_size=3, hidden_size=256, num_layers=1, num_mixtures=5, use_reward=False, use_Done=False):\n",
    "        super().__init__()\n",
    "        self.z_size = z_size\n",
    "        self.a_size = a_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_mixtures = num_mixtures\n",
    "        self.use_reward = use_reward\n",
    "        self.use_Done = use_Done\n",
    "\n",
    "        self.lstm = LSTM(z_size, a_size, hidden_size, num_layers)\n",
    "        self.mdn = MDN(z_size, hidden_size, num_layers, num_mixtures, use_reward, use_Done)\n",
    "    \n",
    "    def forward(self, z, a, hidden, cell):\n",
    "        output, hidden, cell = self.lstm(z, a, hidden, cell)\n",
    "        # pi, mu, logsigma = self.mdn(output)\n",
    "        output = self.mdn(output)\n",
    "        # return pi, mu, logsigma, hidden, cell\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MDNRNN                                   [1, 1, 480]               --\n",
       "├─LSTM: 1-1                              [1, 1, 256]               --\n",
       "│    └─LSTM: 2-1                         [1, 1, 256]               300,032\n",
       "├─MDN: 1-2                               [1, 1, 480]               --\n",
       "│    └─Linear: 2-2                       [1, 1, 480]               123,360\n",
       "==========================================================================================\n",
       "Total params: 423,392\n",
       "Trainable params: 423,392\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 1.69\n",
       "Estimated Total Size (MB): 1.70\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(MDNRNN().to(device), [(1, 1, 32), (1, 1, 3), (1, 1, 256), (1, 1, 256)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([299, 10, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def loss_func(y_true, pi, mu, logsigma):\n",
    "    \"\"\"MDN Loss Function\n",
    "    \n",
    "    y_true: (seq_len, batch_size, z_size)\n",
    "    pi: (seq_len, batch_size, num_mixtures)\n",
    "    mu: (seq_len, batch_size, num_mixtures * z_size)\n",
    "    \"\"\"\n",
    "    pi = pi - torch.max(pi, dim=2, keepdim=True)[0]\n",
    "\n",
    "    log_softmax_pi = torch.log_softmax(pi, dim=2)\n",
    "    log_gauss = -0.5 * (2 * logsigma + math.log(2 * math.pi) + torch.pow(y_true - mu, 2) / torch.exp(2 * logsigma))\n",
    "\n",
    "    loss = torch.sum(torch.exp(log_softmax_pi + log_gauss), dim=2, keepdim=True)\n",
    "    # log0 対策\n",
    "    loss = torch.maximum(loss, torch.tensor(1e-10).to(device))\n",
    "    loss = -torch.log(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conoller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller:\n",
    "    \"\"\"Controller\"\"\"\n",
    "    def __init__(self, z_size, a_size, h_size, params):\n",
    "        self.z_size = z_size\n",
    "        self.a_size = a_size\n",
    "        self.h_size = h_size\n",
    "\n",
    "        self.input_size = z_size + h_size\n",
    "\n",
    "        self.bias = np.array(params[:self.a_size])\n",
    "        self.weight = np.array(params[self.a_size:]).reshape(self.input_size, self.a_size)\n",
    "\n",
    "    def forward(self, z, h):\n",
    "        input = np.concatenate((z, h), axis=0)\n",
    "        a = np.tanh(np.dot(self.weight.T, input) + self.bias)[0]\n",
    "        a[1] = (a[1] + 1.0) / 2.0\n",
    "        a[2] = np.minimum(np.maximam(a[2], 0.0), 1.0)\n",
    "        return a\n",
    "    \n",
    "def get_random_params(z_size, a_size, h_size, sigma):\n",
    "    \"\"\"ランダムなパラメータを生成\"\"\"\n",
    "    params_size = (z_size + h_size) * a_size + a_size\n",
    "    return np.random.standard_cauchy(params_size) * sigma\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_size = 32\n",
    "a_size = 3\n",
    "h_size = 256\n",
    "\n",
    "params_size = (z_size + h_size) * a_size + a_size\n",
    "\n",
    "es = cma.CMAEvolutionStrategy(params_size * [0], 0.1, {'popsize': 64})\n",
    "rewards_through_gens = []\n",
    "generation = 1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch_world_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
